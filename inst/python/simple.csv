"","pkgname","views","vigs","vnc"
"1","DESeq2","Sequencing:RNASeq:ChIPSeq:GeneExpression","--- title: ""Analyzing RNA-seq data with DESeq2"" author: ""Michael I. Love, Simon Anders, and Wolfgang Huber"" date: ""`r format(Sys.Date(), '%m/%d/%Y')`"" abstract: >   A basic task in the analysis of count data from RNA-seq is the   detection of differentially expressed genes. The count data are   presented as a table which reports, for each sample, the number of   sequence fragments that have been assigned to each gene. Analogous   data also arise for other assay types, including comparative ChIP-Seq,   HiC, shRNA screening, and mass spectrometry.  An important analysis   question is the quantification and statistical inference of systematic   changes between conditions, as compared to within-condition   variability. The package DESeq2 provides methods to test for   differential expression by use of negative binomial generalized linear   models; the estimates of dispersion and logarithmic fold changes   incorporate data-driven prior distributions. This vignette explains the   use of the package and demonstrates typical workflows.   [An RNA-seq workflow](http://www.bioconductor.org/help/workflows/rnaseqGene/)   on the Bioconductor website covers similar material to this vignette   but at a slower pace, including the generation of count matrices from   FASTQ files.   DESeq2 package version: `r packageVersion(""DESeq2"")` output:   rmarkdown::html_document:     highlight: pygments     toc: true     fig_width: 5 bibliography: library.bib vignette: >   %\VignetteIndexEntry{Analyzing RNA-seq data with DESeq2}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8}   %\usepackage[utf8]{inputenc} ---  ```{r setup, echo=FALSE, results=""hide""} knitr::opts_chunk$set(tidy = FALSE,                       cache = FALSE,                       dev = ""png"",                       message = FALSE, error = FALSE, warning = TRUE) ```	  # Standard workflow  **Note:** if you use DESeq2 in published research, please cite:  > Love, M.I., Huber, W., Anders, S. (2014) > Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. > *Genome Biology*, **15**:550. > [10.1186/s13059-014-0550-8](http://dx.doi.org/10.1186/s13059-014-0550-8)  Other Bioconductor packages with similar aims are [edgeR](http://bioconductor.org/packages/edgeR), [limma](http://bioconductor.org/packages/limma), [DSS](http://bioconductor.org/packages/DSS), [EBSeq](http://bioconductor.org/packages/EBSeq), and  [baySeq](http://bioconductor.org/packages/baySeq).  ## Quick start  Here we show the most basic steps for a differential expression analysis. There are a variety of steps upstream of DESeq2 that result in the generation of counts or estimated counts for each sample, which we will discuss in the sections below. This code chunk assumes that you have a count matrix called `cts` and a table of sample information called `coldata`.  The `design` indicates how to model the samples, here, that we want to measure the effect of the condition, controlling for batch differences. The two factor variables `batch` and `condition` should  be columns of `coldata`.   ```{r quickStart, eval=FALSE} dds <- DESeqDataSetFromMatrix(countData = cts,                               colData = coldata,                               design= ~ batch + condition) dds <- DESeq(dds) resultsNames(dds) # lists the coefficients res <- results(dds, name=""condition_trt_vs_untrt"") # or to shrink log fold changes association with condition: res <- lfcShrink(dds, coef=""condition_trt_vs_untrt"", type=""apeglm"") ```  The following starting functions will be explained below:  * If you have performed transcript quantification    (with *Salmon*, *kallisto*, *RSEM*, etc.)    you could import the data with *tximport*, which produces a list,   and then you can use `DESeqDataSetFromTximport()`. * If you imported quantification data with *tximeta*, which produces a   *SummarizedExperiment* with additional metadata, you can then use   `DESeqDataSet()`. * If you have *htseq-count* files, you can use    `DESeqDataSetFromHTSeq()`.  ## How to get help for DESeq2  Any and all DESeq2 questions should be posted to the  **Bioconductor support site**, which serves as a searchable knowledge base of questions and answers:  <https://support.bioconductor.org>  Posting a question and tagging with ""DESeq2"" will automatically send an alert to the package authors to respond on the support site.  See the first question in the list of [Frequently Asked Questions](#FAQ) (FAQ) for information about how to construct an informative post.   You should **not** email your question to the package authors, as we will just reply that the question should be posted to the  **Bioconductor support site**.  ## Acknowledgments  Constantin Ahlmann-Eltze has contributed core code for increasing the computational performance of *DESeq2* and building an interface to his *glmGamPoi* package.  We have benefited in the development of *DESeq2* from the help and feedback of many individuals, including but not limited to:   The Bionconductor Core Team, Alejandro Reyes, Andrzej Oles, Aleksandra Pekowska, Felix Klein, Nikolaos Ignatiadis (IHW), Anqi Zhu (apeglm), Joseph Ibrahim (apeglm), Vince Carey, Owen Solberg, Ruping Sun, Devon Ryan,  Steve Lianoglou, Jessica Larson, Christina Chaivorapol, Pan Du, Richard Bourgon, Willem Talloen,  Elin Videvall, Hanneke van Deutekom, Todd Burwell,  Jesse Rowley, Igor Dolgalev, Stephen Turner, Ryan C Thompson, Tyr Wiesner-Hanks, Konrad Rudolph, David Robinson, Mingxiang Teng, Mathias Lesche, Sonali Arora, Jordan Ramilowski, Ian Dworkin, Bjorn Gruning, Ryan McMinds, Paul Gordon, Leonardo Collado Torres, Enrico Ferrero, Peter Langfelder, Gavin Kelly, Rob Patro, Charlotte Soneson, Koen Van den Berge, Fanny Perraudeau, Davide Risso, Stephan Engelbrecht, Nicolas Alcala, Jeremy Simon, Travis Ptacek, Rory Kirchner, R. Butler, Ben Keith, Dan Liang, Nil Aygün, Rory Nolan, Michael Schubert, Hugo Tavares, Eric Davis, Wancen Mu, Zhang Cheng, Frederik Ziebell, Luca Menestrina, Hendrik Weisse, I-Hsuan Lin, Rasmus Henningsson.  ## Funding  DESeq2 and its developers have been partially supported by funding from the European Union’s 7th Framework Programme via Project RADIANT, NIH NHGRI R01-HG009937, and by a CZI EOSS award.  ## Input data  ### Why un-normalized counts?  As input, the DESeq2 package expects count data as obtained, e.g., from RNA-seq or another high-throughput sequencing experiment, in the form of a matrix of integer values. The value in the *i*-th row and the *j*-th column of the matrix tells how many reads can be assigned to gene *i* in sample *j*. Analogously, for other types of assays, the rows of the matrix might correspond e.g. to binding regions (with ChIP-Seq) or peptide sequences (with quantitative mass spectrometry). We will list method for obtaining count matrices in sections below.  The values in the matrix should be un-normalized counts or estimated counts of sequencing reads (for single-end RNA-seq) or fragments (for paired-end RNA-seq).  The [RNA-seq workflow](http://www.bioconductor.org/help/workflows/rnaseqGene/) describes multiple techniques for preparing such count matrices.  It is important to provide count matrices as input for DESeq2's statistical model [@Love2014] to hold, as only the count values allow assessing the measurement precision correctly. The DESeq2 model internally corrects for library size, so transformed or normalized values such as counts scaled by library size should not be used as input.  ### The DESeqDataSet  The object class used by the DESeq2 package to store the read counts  and the intermediate estimated quantities during statistical analysis is the *DESeqDataSet*, which will usually be represented in the code here as an object `dds`.  A technical detail is that the *DESeqDataSet* class extends the *RangedSummarizedExperiment* class of the  [SummarizedExperiment](http://bioconductor.org/packages/SummarizedExperiment) package.  The ""Ranged"" part refers to the fact that the rows of the assay data  (here, the counts) can be associated with genomic ranges (the exons of genes). This association facilitates downstream exploration of results, making use of other Bioconductor packages' range-based functionality (e.g. find the closest ChIP-seq peaks to the differentially expressed genes).  A *DESeqDataSet* object must have an associated *design formula*. The design formula expresses the variables which will be used in modeling. The formula should be a tilde (~) followed by the variables with plus signs between them (it will be coerced into an *formula* if it is not already). The design can be changed later,  however then all differential analysis steps should be repeated,  as the design formula is used to estimate the dispersions and  to estimate the log2 fold changes of the model.   *Note*: In order to benefit from the default settings of the package, you should put the variable of interest at the end of the formula and make sure the control level is the first level.  We will now show 4 ways of constructing a *DESeqDataSet*, depending on what pipeline was used upstream of DESeq2 to generated counts or estimated counts:  1) From [transcript abundance files and tximport](#tximport) 2) From a [count matrix](#countmat) 3) From [htseq-count files](#htseq) 4) From a [SummarizedExperiment](#se) object  <a name=""tximport""/>  ### Transcript abundance files and *tximport* / *tximeta*  Our recommended pipeline for *DESeq2* is to use fast transcript  abundance quantifiers upstream of DESeq2, and then to create gene-level count matrices for use with DESeq2  by importing the quantification data using [tximport](http://bioconductor.org/packages/tximport) [@Soneson2015]. This workflow allows users to import transcript abundance estimates from a variety of external software, including the following methods:  * [Salmon](http://combine-lab.github.io/salmon/)   [@Patro2017Salmon] * [Sailfish](http://www.cs.cmu.edu/~ckingsf/software/sailfish/)   [@Patro2014Sailfish] * [kallisto](https://pachterlab.github.io/kallisto/about.html)   [@Bray2016Near] * [RSEM](http://deweylab.github.io/RSEM/)   [@Li2011RSEM]  Some advantages of using the above methods for transcript abundance estimation are:  (i) this approach corrects for potential changes in gene length across samples  (e.g. from differential isoform usage) [@Trapnell2013Differential], (ii) some of these methods (*Salmon*, *Sailfish*, *kallisto*)  are substantially faster and require less memory and disk usage compared to alignment-based methods that require creation and storage of BAM files, and (iii) it is possible to avoid discarding those fragments that can align to multiple genes with homologous sequence, thus increasing sensitivity [@Robert2015Errors].  Full details on the motivation and methods for importing transcript level abundance and count estimates, summarizing to gene-level count matrices  and producing an offset which corrects for potential changes in average transcript length across samples are described in [@Soneson2015]. Note that the tximport-to-DESeq2 approach uses *estimated* gene counts from the transcript abundance quantifiers, but not *normalized* counts.   A tutorial on how to use the *Salmon* software for quantifying transcript abundance can be found [here](https://combine-lab.github.io/salmon/getting_started/). We recommend using the `--gcBias`  [flag](http://salmon.readthedocs.io/en/latest/salmon.html#gcbias) which estimates a correction factor for systematic biases commonly present in RNA-seq data [@Love2016Modeling; @Patro2017Salmon],  unless you are certain that your data do not contain such bias.  Here, we demonstrate how to import transcript abundances and construct a gene-level *DESeqDataSet* object from *Salmon* `quant.sf` files, which are stored in the [tximportData](http://bioconductor.org/packages/tximportData) package. You do not need the `tximportData` package for your analysis, it is only used here for demonstration.  Note that, instead of locating `dir` using *system.file*, a user would typically just provide a path, e.g. `/path/to/quant/files`. For a typical use, the `condition` information should already be present as a column of the sample table `samples`, while here we construct artificial condition labels for demonstration.  ```{r txiSetup} library(""tximport"") library(""readr"") library(""tximportData"") dir <- system.file(""extdata"", package=""tximportData"") samples <- read.table(file.path(dir,""samples.txt""), header=TRUE) samples$condition <- factor(rep(c(""A"",""B""),each=3)) rownames(samples) <- samples$run samples[,c(""pop"",""center"",""run"",""condition"")] ```  Next we specify the path to the files using the appropriate columns of `samples`, and we read in a table that links transcripts to genes for this dataset.  ```{r txiFiles} files <- file.path(dir,""salmon"", samples$run, ""quant.sf.gz"") names(files) <- samples$run tx2gene <- read_csv(file.path(dir, ""tx2gene.gencode.v27.csv"")) ```  We import the necessary quantification data for DESeq2 using the *tximport* function.  For further details on use of *tximport*, including the construction of the `tx2gene` table for linking transcripts to genes in your dataset, please refer to the  [tximport](http://bioconductor.org/packages/tximport) package vignette.  ```{r tximport, results=""hide""} txi <- tximport(files, type=""salmon"", tx2gene=tx2gene) ```  Finally, we can construct a *DESeqDataSet* from the `txi` object and sample information in `samples`.  ```{r txi2dds, results=""hide""} library(""DESeq2"") ddsTxi <- DESeqDataSetFromTximport(txi,                                    colData = samples,                                    design = ~ condition) ```  The `ddsTxi` object here can then be used as `dds` in the following analysis steps.  ### Tximeta for import with automatic metadata  Another Bioconductor package,  [tximeta](https://bioconductor.org/packages/tximeta) [@Love2020], extends *tximport*, offering the same functionality, plus the additional benefit of automatic addition of annotation metadata for commonly used transcriptomes (GENCODE, Ensembl, RefSeq for human and mouse). See the [tximeta](https://bioconductor.org/packages/tximeta) package vignette for more details. *tximeta* produces a *SummarizedExperiment* that can be loaded easily into *DESeq2* using the `DESeqDataSet` function, with an example in the *tximeta* package vignette, and below:  ```{r} coldata <- samples coldata$files <- files coldata$names <- coldata$run ```  ```{r echo=FALSE} library(""tximeta"") se <- tximeta(coldata, skipMeta=TRUE) ddsTxi2 <- DESeqDataSet(se, design = ~condition) ```  ```{r eval=FALSE} library(""tximeta"") se <- tximeta(coldata) ddsTxi <- DESeqDataSet(se, design = ~ condition) ```  The `ddsTxi` object here can then be used as `dds` in the following analysis steps. If *tximeta* recognized the reference transcriptome as one of those with a pre-computed hashed checksum, the `rowRanges` of the `dds` object will be pre-populated. Again, see the *tximeta* vignette for full details.  <a name=""countmat""/>  ### Count matrix input  Alternatively, the function *DESeqDataSetFromMatrix* can be used if you already have a matrix of read counts prepared from another source. Another method for quickly producing count matrices  from alignment files is the *featureCounts* function [@Liao2013feature] in the [Rsubread](http://bioconductor.org/packages/Rsubread) package. To use *DESeqDataSetFromMatrix*, the user should provide  the counts matrix, the information about the samples (the columns of the  count matrix) as a *DataFrame* or *data.frame*, and the design formula.  To demonstrate the use of *DESeqDataSetFromMatrix*,  we will read in count data from the [pasilla](http://bioconductor.org/packages/pasilla) package.  We read in a count matrix, which we will name `cts`,  and the sample information table, which we will name `coldata`.  Further below we describe how to extract these objects from, e.g. *featureCounts* output.   ```{r loadPasilla} library(""pasilla"") pasCts <- system.file(""extdata"",                       ""pasilla_gene_counts.tsv"",                       package=""pasilla"", mustWork=TRUE) pasAnno <- system.file(""extdata"",                        ""pasilla_sample_annotation.csv"",                        package=""pasilla"", mustWork=TRUE) cts <- as.matrix(read.csv(pasCts,sep=""\t"",row.names=""gene_id"")) coldata <- read.csv(pasAnno, row.names=1) coldata <- coldata[,c(""condition"",""type"")] coldata$condition <- factor(coldata$condition) coldata$type <- factor(coldata$type) ```  We examine the count matrix and column data to see if they are consistent in terms of sample order.  ```{r showPasilla} head(cts,2) coldata ```  Note that these are not in the same order with respect to samples!   It is absolutely critical that the columns of the count matrix and the rows of the column data (information about samples) are in the same order.  DESeq2 will not make guesses as to which column of the count matrix belongs to which row of the column data, these must be provided to DESeq2 already in consistent order.  As they are not in the correct order as given, we need to re-arrange one or the other so that they are consistent in terms of sample order (if we do not, later functions would produce an error). We additionally need to chop off the `""fb""` of the row names of `coldata`, so the naming is consistent.  ```{r reorderPasila} rownames(coldata) <- sub(""fb"", """", rownames(coldata)) all(rownames(coldata) %in% colnames(cts)) all(rownames(coldata) == colnames(cts)) cts <- cts[, rownames(coldata)] all(rownames(coldata) == colnames(cts)) ```  If you have used the *featureCounts* function [@Liao2013feature] in the  [Rsubread](http://bioconductor.org/packages/Rsubread) package,  the matrix of read counts can be directly  provided from the `""counts""` element in the list output. The count matrix and column data can typically be read into R  from flat files using base R functions such as *read.csv* or *read.delim*. For *htseq-count* files, see the dedicated input function below.   With the count matrix, `cts`, and the sample information, `coldata`, we can construct a *DESeqDataSet*:  ```{r matrixInput} library(""DESeq2"") dds <- DESeqDataSetFromMatrix(countData = cts,                               colData = coldata,                               design = ~ condition) dds ```  If you have additional feature data, it can be added to the *DESeqDataSet* by adding to the metadata columns of a newly constructed object. (Here we add redundant data just for demonstration, as the gene names are already the rownames of the `dds`.)  ```{r addFeatureData} featureData <- data.frame(gene=rownames(cts)) mcols(dds) <- DataFrame(mcols(dds), featureData) mcols(dds) ```  <a name=""htseq""/>  ### *htseq-count* input  You can use the function *DESeqDataSetFromHTSeqCount* if you have used *htseq-count* from the  [HTSeq](http://www-huber.embl.de/users/anders/HTSeq)  python package [@Anders:2014:htseq]. For an example of using the python scripts, see the [pasilla](http://bioconductor.org/packages/pasilla) data package. First you will want to specify a variable which points to the directory in which the *htseq-count* output files are located.   ```{r htseqDirI, eval=FALSE} directory <- ""/path/to/your/files/"" ```  However, for demonstration purposes only, the following line of code points to the directory for the demo *htseq-count* output files packages for the [pasilla](http://bioconductor.org/packages/pasilla) package.  ```{r htseqDirII} directory <- system.file(""extdata"", package=""pasilla"",                          mustWork=TRUE) ```  We specify which files to read in using *list.files*, and select those files which contain the string `""treated""` using *grep*. The *sub* function is used to  chop up the sample filename to obtain the condition status, or  you might alternatively read in a phenotypic table  using *read.table*.  ```{r htseqInput} sampleFiles <- grep(""treated"",list.files(directory),value=TRUE) sampleCondition <- sub(""(.*treated).*"",""\\1"",sampleFiles) sampleTable <- data.frame(sampleName = sampleFiles,                           fileName = sampleFiles,                           condition = sampleCondition) sampleTable$condition <- factor(sampleTable$condition) ```  Then we build the *DESeqDataSet* using the following function:  ```{r hsteqDds} library(""DESeq2"") ddsHTSeq <- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable,                                        directory = directory,                                        design= ~ condition) ddsHTSeq ```  <a name=""se""/>  ### *SummarizedExperiment* input  If one has already created or obtained a *SummarizedExperiment*, it can be easily input into DESeq2 as follows. First we load the package containing the `airway` dataset.  ```{r loadSumExp} library(""airway"") data(""airway"") se <- airway ``` The constructor function below shows the generation of a *DESeqDataSet* from a *RangedSummarizedExperiment* `se`.  ```{r sumExpInput} library(""DESeq2"") ddsSE <- DESeqDataSet(se, design = ~ cell + dex) ddsSE ```  ### Pre-filtering  While it is not necessary to pre-filter low count genes before running the DESeq2 functions, there are two reasons which make pre-filtering useful: by removing rows in which there are very few reads, we reduce the memory size of the `dds` data object, and we increase the speed of count modeling within DESeq2. It can also improve visualizations, as features with no information for differential expression are not plotted in dispersion plots or MA-plots.  Here we perform pre-filtering to keep only rows that have a count of at least 10 for a minimal number of samples. The count of 10 is a reasonable choice for bulk RNA-seq. A recommendation for the minimal number of samples is to specify the smallest group size, e.g. here there are 3 treated samples. If there are not discrete groups, one can use the minimal number of samples where non-zero counts would be considered interesting. One can also omit this step entirely and just rely on the independent filtering procedures available in `results()`, either *IHW* or *genefilter*. See [independent filtering](#indfilt) section.  ```{r prefilter} smallestGroupSize <- 3 keep <- rowSums(counts(dds) >= 10) >= smallestGroupSize dds <- dds[keep,] ```  <a name=""factorlevels""/>  ### Note on factor levels   By default, R will choose a *reference level* for factors based on alphabetical order. Then, if you never tell the DESeq2 functions which level you want to compare against (e.g. which level represents the control group), the comparisons will be based on the alphabetical order of the levels. There are two solutions: you can either explicitly tell *results* which comparison to make using the `contrast` argument (this will be shown later), or you can explicitly set the factors levels. In order to see the change of reference levels reflected in the results names, you need to either run `DESeq` or `nbinomWaldTest`/`nbinomLRT` after the re-leveling operation. Setting the factor levels can be done in two ways, either using factor:  ```{r factorlvl} dds$condition <- factor(dds$condition, levels = c(""untreated"",""treated"")) ```   ...or using *relevel*, just specifying the reference level:  ```{r relevel} dds$condition <- relevel(dds$condition, ref = ""untreated"") ```   If you need to subset the columns of a *DESeqDataSet*, i.e., when removing certain samples from the analysis, it is possible that all the samples for one or more levels of a variable in the design formula would be removed. In this case, the *droplevels* function can be used to remove those levels which do not have samples in the current *DESeqDataSet*:  ```{r droplevels} dds$condition <- droplevels(dds$condition) ```   ### Collapsing technical replicates  DESeq2 provides a function *collapseReplicates* which can assist in combining the counts from technical replicates into single columns of the count matrix. The term *technical replicate*  implies multiple sequencing runs of the same library.  You should not collapse biological replicates using this function. See the manual page for an example of the use of *collapseReplicates*.  ### About the pasilla dataset  We continue with the [pasilla](http://bioconductor.org/packages/pasilla) data constructed from the count matrix method above. This data set is from an experiment on *Drosophila melanogaster* cell cultures and investigated the effect of RNAi knock-down of the splicing factor *pasilla* [@Brooks2010].  The detailed transcript of the production of the [pasilla](http://bioconductor.org/packages/pasilla) data is provided in the vignette of the  data package [pasilla](http://bioconductor.org/packages/pasilla).  <a name=""de""/>  ## Differential expression analysis   The standard differential expression analysis steps are wrapped into a single function, *DESeq*. The estimation steps performed by this function are described [below](#theory), in the manual page for `?DESeq` and in the Methods section of the DESeq2 publication [@Love2014].   Results tables are generated using the function *results*, which extracts a results table with log2 fold changes, *p* values and adjusted *p* values. With no additional arguments to *results*, the log2 fold change and Wald test *p* value will be for the **last variable** in the design formula, and if this is a factor, the comparison will be the **last level** of this variable over the **reference level**  (see previous [note on factor levels](#factorlevels)).  However, the order of the variables of the design do not matter so long as the user specifies the comparison to build a results table for, using the `name` or `contrast` arguments of *results*.  Details about the comparison are printed to the console, directly above the results table. The text, `condition treated vs untreated`, tells you that the estimates are of the logarithmic fold change log2(treated/untreated).  ```{r deseq} dds <- DESeq(dds) res <- results(dds) res ```   Note that we could have specified the coefficient or contrast we want to build a results table for, using either of the following equivalent commands:  ```{r eval=FALSE} res <- results(dds, name=""condition_treated_vs_untreated"") res <- results(dds, contrast=c(""condition"",""treated"",""untreated"")) ```  One exception to the equivalence of these two commands, is that, using `contrast` will additionally set to 0 the estimated LFC in a comparison of two groups, where all of the counts in the two groups are equal to 0 (while other groups have positive counts). As this may be a desired feature to have the LFC in these cases set to 0, one can use `contrast` to build these results tables. More information about extracting specific coefficients from a fitted *DESeqDataSet* object can be found in the help page `?results`. The use of the `contrast` argument is also further discussed [below](#contrasts).  <a name=""lfcShrink""/>  ### Log fold change shrinkage for visualization and ranking  Shrinkage of effect size (LFC estimates) is useful for visualization and ranking of genes. To shrink the LFC, we pass the `dds` object to the function `lfcShrink`. Below we specify to use the *apeglm* method for effect size shrinkage [@Zhu2018], which improves on the previous estimator.  We provide the `dds` object and the name or number of the coefficient we want to shrink, where the number refers to the order of the coefficient as it appears in `resultsNames(dds)`.  ```{r lfcShrink} resultsNames(dds) resLFC <- lfcShrink(dds, coef=""condition_treated_vs_untreated"", type=""apeglm"") resLFC ```  Shrinkage estimation is discussed more in a [later section](#altshrink).  <a name=""parallel""/>  ### Speed-up and parallelization thoughts  The above steps should take less than 30 seconds for most analyses. For experiments with complex designs and many samples (e.g. dozens of coefficients, ~100s of samples), one may want  to have faster computation than provided by the default run of `DESeq`. We have two recommendations:  1) By using the argument `fitType=""glmGamPoi""`, one can leverage the faster NB GLM engine written by Constantin Ahlmann-Eltze. Note that glmGamPoi's interface in DESeq2 requires use of `test=""LRT""` and specification of a `reduced` design.  2) One can take advantage of parallelized computation. Parallelizing `DESeq`, `results`, and `lfcShrink` can be easily accomplished by loading the BiocParallel package, and then setting the following arguments: `parallel=TRUE` and `BPPARAM=MulticoreParam(4)`, for example, splitting the job over 4 cores. However, some words of advice on parallelization: first, it is recommend to filter genes where all samples have low counts, to avoid sending data unnecessarily to child processes, when those genes have low power and will be independently filtered anyway; secondly, there is often diminishing returns for adding more cores due to overhead of sending data to child processes, therefore I recommend first starting with small number of additional cores. Note that obtaining `results` for coefficients or contrasts listed in `resultsNames(dds)` is fast and will not need parallelization. As an alternative to `BPPARAM`, one can `register` cores at the beginning of an analysis, and then just specify `parallel=TRUE` to the functions when called.  ```{r parallel, eval=FALSE} library(""BiocParallel"") register(MulticoreParam(4)) ```  ### p-values and adjusted p-values  We can order our results table by the smallest *p* value:  ```{r resOrder} resOrdered <- res[order(res$pvalue),] ```  We can summarize some basic tallies using the *summary* function.  ```{r sumRes} summary(res) ```   How many adjusted p-values were less than 0.1?  ```{r sumRes01} sum(res$padj < 0.1, na.rm=TRUE) ```   The *results* function contains a number of arguments to customize the results table which is generated. You can read about these arguments by looking up `?results`. Note that the *results* function automatically performs independent filtering based on the mean of normalized counts for each gene, optimizing the number o",123181
"2","edgeR","GeneExpression:Transcription:AlternativeSplicing:Coverage","\documentclass{article} \usepackage{pdfpages} %\VignetteIndexEntry{edgeR User's Guide} %\VignetteEncoding{UTF-8}  \begin{document} \includepdf[pages=-, fitpaper=true]{pdf/edgeRUsersGuide.pdf} \end{document}",206
"3","edgeR","GeneExpression:Transcription:AlternativeSplicing:Coverage","--- title: A brief introduction to edgeR date: ""10 October 2012 (last revised 21 June 2023)"" output:   BiocStyle::html_document:     toc: FALSE     number_sections: FALSE vignette: >   %\VignetteIndexEntry{A brief introduction to edgeR}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8} ---  ```{r, include = FALSE} knitr::opts_chunk$set(   prompt = TRUE,   comment = NA ) ```  # What is it?  edgeR is a package for differential analyses of read count data from sequencing technologies such as RNA-seq, ChIP-seq, ATAC-seq, BS-seq and CUT&RUN. It has particularly strong capabilities for expression analyses of RNA-seq data, including gene expression, transcript expression and tests for differential splicing.  edgeR implements novel statistical methods based on the negative binomial distribution as a model for count variability, including empirical Bayes methods, exact tests, and generalized linear models. The package is especially suitable for analysing designed experiments with multiple experimental factors but possibly small numbers of replicates. It has unique abilities to model transcript specific variation even in small samples, a capability essential for prioritizing genes or transcripts that have consistent effects across replicates.  # How to get help  The edgeR User's Guide is available by ```{r, eval=FALSE, echo=TRUE} library(edgeR) edgeRUsersGuide() ``` or alternatively from the [edgeR landing page](https://bioconductor.org/packages/edgeR).  Documentation for specific functions is available through the usual R help system, e.g., `?glmFit`. Further questions about the package should be directed to the [Bioconductor support site](https://support.bioconductor.org).  # Further reading  Chen Y, Chen L, Lun ATL, Baldoni PL, Smyth GK (2024). edgeR 4.0: powerful differential analysis of sequencing data with expanded functionality and improved support for small counts and larger datasets. *bioRxiv* doi: [10.1101/2024.01.21.576131](https://doi.org/10.1101/2024.01.21.576131).  Chen, Y, Pal, B, Visvader, JE, Smyth, GK (2017). Differential methylation analysis of reduced representation bisulfite sequencing experiments using edgeR. *F1000Research* 6, 2055. [doi:10.12688/f1000research.13196.2](https://doi.org/10.12688/f1000research.13196.2)  Chen Y, Lun ATL, Smyth GK (2016). From reads to genes to pathways: differential expression analysis of RNA-Seq experiments using Rsubread and the edgeR quasi-likelihood pipeline. *F1000Research* 5, 1438. [doi:10.12688/f1000research.8987.2](https://doi.org/10.12688/f1000research.8987.2)  McCarthy, DJ, Chen, Y, Smyth, GK (2012). Differential expression analysis of multifactor RNA-Seq experiments with respect to biological variation. *Nucleic Acids Research* 40, 4288-4297. [doi:10.1093/nar/gks042](https://doi.org/10.1093/nar/gks042)  Robinson, MD, McCarthy, DJ, Smyth, GK (2010). edgeR: a Bioconductor package for differential expression analysis of digital gene expression data. *Bioinformatics* 26, 139-140. [doi:10.1093/bioinformatics/btp616](https://doi.org/10.1093/bioinformatics/btp616)",3080
"4","tximeta","Annotation:GenomeAnnotation:DataImport:Preprocessing","--- title: ""Tximeta: transcript quantification import with automatic metadata"" author: ""Michael I. Love, Charlotte Soneson, Peter F. Hickey, Rob Patro"" date: ""`r format(Sys.time(), '%m/%d/%Y')`"" output:    rmarkdown::html_document:     highlight: tango     toc: true     toc_float: true abstract: >   Tximeta performs numerous annotation and metadata gathering tasks on   behalf of users during the import of transcript quantifications from   *Salmon* or *alevin* into R/Bioconductor. Metadata and transcript   ranges are added automatically, facilitating genomic analyses and   assisting in computational reproducibility. bibliography: library.bib vignette: |   %\VignetteIndexEntry{Transcript quantification import with automatic metadata}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8} ---  # Introduction  The `tximeta` package [@Love2020] extends the `tximport` package [@Soneson2015] for import of transcript-level quantification data into R/Bioconductor. It automatically adds annotation metadata when the RNA-seq data has been quantified with *Salmon* [@Patro2017] or for scRNA-seq data quantified with *alevin* [@Srivastava2019]. To our knowledge, `tximeta` is the only package for RNA-seq data import that can automatically identify and attach transcriptome metadata based on the unique sequence of the reference transcripts. For more details on these packages -- including the motivation for `tximeta` and description of similar work -- consult the **References** below.  **Note:** `tximeta` requires that the **entire output directory** of *Salmon* / *alevin* is present and unmodified in order to identify the provenance of the reference transcripts. In general, it's a good idea to not modify or re-arrange the output directory of bioinformatic software as other downstream software rely on and assume a consistent directory structure. For sharing multiple samples, one can use, for example, `tar -czf` to bundle up a set of Salmon output directories, or to bundle one alevin output directory. For tips on using `tximeta` with other quantifiers see the  [other quantifiers](#other_quantifiers) section below.  <center> <img width=600 src=""diagram.png""/> </center>  # Analysis starts with sample table  The first step using `tximeta` is to read in the sample table, which will become the *column data*, `colData`, of the final object, a *SummarizedExperiment*. The sample table should contain all the information we need to identify the *Salmon* quantification directories. For *alevin* quantification, one should point to the `quants_mat.gz` file that contains the counts for all of the cells.  Here we will use a *Salmon* quantification file in the *tximportData* package to demonstrate the usage of `tximeta`. We do not have a sample table, so we construct one in R. It is recommended to keep a sample table as a CSV or TSV file while working on an RNA-seq project with multiple samples.  ```{r} dir <- system.file(""extdata/salmon_dm"", package=""tximportData"") files <- file.path(dir, ""SRR1197474"", ""quant.sf"")  file.exists(files) coldata <- data.frame(files, names=""SRR1197474"", condition=""A"", stringsAsFactors=FALSE) coldata ```  `tximeta` expects at least two columns in `coldata`:   1. `files` - a pointer to the `quant.sf` files 2. `names` - the unique names that should be used to identify samples  # Running tximeta  Normally, we would just run `tximeta` like so:  ```{r eval=FALSE} library(tximeta) se <- tximeta(coldata) ```  However, to avoid downloading remote GTF files during this vignette, we will point to a GTF file saved locally (in the *tximportData* package). We link the transcriptome of the *Salmon* index to its locally saved GTF. The standard recommended usage of `tximeta` would be the code chunk above, or to specify a remote GTF source, not a local one. **This following code is therefore not recommended for a typically workflow, but is particular to the vignette code.**  ```{r} indexDir <- file.path(dir, ""Dm.BDGP6.22.98_salmon-0.14.1"") fastaFTP <- c(""ftp://ftp.ensembl.org/pub/release-98/fasta/drosophila_melanogaster/cdna/Drosophila_melanogaster.BDGP6.22.cdna.all.fa.gz"",               ""ftp://ftp.ensembl.org/pub/release-98/fasta/drosophila_melanogaster/ncrna/Drosophila_melanogaster.BDGP6.22.ncrna.fa.gz"") gtfPath <- file.path(dir,""Drosophila_melanogaster.BDGP6.22.98.gtf.gz"") suppressPackageStartupMessages(library(tximeta)) makeLinkedTxome(indexDir=indexDir,                 source=""Ensembl"",                 organism=""Drosophila melanogaster"",                 release=""98"",                 genome=""BDGP6.22"",                 fasta=fastaFTP,                 gtf=gtfPath,                 write=FALSE) ```  ```{r} library(tximeta) se <- tximeta(coldata) ```  # What happened?   `tximeta` recognized the hashed checksum of the transcriptome that the files were quantified against, it accessed the GTF file of the transcriptome source, found and attached the transcript ranges, and added the appropriate transcriptome and genome metadata.  A remote GTF is only downloaded once, and a local or remote GTF is only parsed to build a *TxDb* or *EnsDb* once: if `tximeta` recognizes that it has seen this *Salmon* index before, it will use a cached version of the metadata and transcript ranges.  Note the warning above that 5 of the transcripts are missing from the GTF file and so are dropped from the final output. This is a problem coming from the annotation source, and not easily avoided by `tximeta`.   # TxDb, EnsDb, and AnnotationHub  `tximeta` makes use of Bioconductor packages for storing transcript databases as *TxDb* or *EnsDb* objects, which both are  connected by default to `sqlite` backends. For GENCODE and RefSeq GTF files, `tximeta` uses the *GenomicFeatures* package [@granges] to parse the GTF and build a *TxDb*. For Ensembl GTF files, `tximeta` will first attempt to obtain the correct *EnsDb* object using *AnnotationHub*. The *ensembldb* package [@ensembldb] contains classes and methods for extracting relevant data from Ensembl files. If the *EnsDb* has already been made available on AnnotationHub, `tximeta` will download the database directly, which saves the user time parsing the GTF into a database (to avoid this, set `useHub=FALSE`). If the relevant *EnsDb* is not available on AnnotationHub, `tximeta` will build an *EnsDb* using *ensembldb* after downloading the GTF file. Again, the download/construction of a transcript database occurs only once, and upon subsequent usage of *tximeta* functions, the cached version will be used.  # Pre-computed checksums  We plan to support a wide variety of sources and organisms for transcriptomes with pre-computed checksums, though for now the software focuses on predominantly human and mouse transcriptomes  (see **Next steps** below for details).  The following checksums are supported in this version of `tximeta`:  ```{r echo=FALSE} dir2 <- system.file(""extdata"", package=""tximeta"") tab <- read.csv(file.path(dir2, ""hashtable.csv""),                 stringsAsFactors=FALSE) release.range <- function(tab, source, organism) {   tab.idx <- tab$organism == organism & tab$source == source   rels <- tab$release[tab.idx]   if (organism == ""Mus musculus"" & source == ""GENCODE"") {     paste0(""M"", range(as.numeric(sub(""M"","""",rels))))   } else if (source == ""RefSeq"") {     paste0(""p"", range(as.numeric(sub("".*p"","""",rels))))   } else {     range(as.numeric(rels))   } } dat <- data.frame(   source=rep(c(""GENCODE"",""Ensembl"",""RefSeq""),c(2,3,2)),   organism=c(""Homo sapiens"",""Mus musculus"",              ""Drosophila melanogaster"")[c(1:2,1:3,1:2)] ) rng <- t(sapply(seq_len(nrow(dat)), function(i)   release.range(tab, dat[i,1], dat[i,2]))) dat$releases <- paste0(rng[,1], ""-"", rng[,2]) knitr::kable(dat) ```  For Ensembl transcriptomes, we support the combined protein coding (cDNA) and non-coding (ncRNA) sequences, as well as the protein coding alone (although the former approach combining coding and non-coding transcripts is recommended for more accurate quantification).  `tximeta` also has functions to support *linked transcriptomes*, where one or more sources for transcript sequences have been combined or filtered. See the **Linked transcriptome** section below for a demonstration. (The *makeLinkedTxome* function was used above to avoid downloading the GTF during the vignette building process.)  # SummarizedExperiment output  We have our coldata from before. Note that we've removed `files`.  ```{r} suppressPackageStartupMessages(library(SummarizedExperiment)) colData(se) ```  Here we show the three matrices that were imported.   ```{r} assayNames(se) ```  If there were inferential replicates (Gibbs samples or bootstrap samples), these would be imported as additional assays named `""infRep1""`, `""infRep2""`, ...  `tximeta` has imported the correct ranges for the transcripts:  ```{r} rowRanges(se) ```  We have appropriate genome information, which prevents us from making  bioinformatic mistakes:  ```{r} seqinfo(se) ```  # Retrieve the transcript database  The `se` object has associated metadata that allows `tximeta` to link to locally stored cached databases and other Bioconductor objects. In further sections, we will show examples functions that leverage this databases for adding exon information, summarize transcript-level data to the gene level, or add identifiers. However, first we mention that the user can easily access the cached database with the following helper function. In this case, `tximeta` has an associated *EnsDb* object that we can retrieve and use in our R session:  ```{r} edb <- retrieveDb(se) class(edb) ```  The database returned by `retrieveDb` is either a *TxDb* in the case of GENCODE or RefSeq GTF annotation file, or an *EnsDb* in the case of an Ensembl GTF annotation file. For further use of these two database objects, consult the *GenomicFeatures* vignettes and the *ensembldb* vignettes, respectively (both Bioconductor packages).  # Add exons per transcript  Because the SummarizedExperiment maintains all the metadata of its creation, it also keeps a pointer to the necessary database for pulling out additional information, as demonstrated in the following sections.   If necessary, the *tximeta* package can pull down the remote source to build a TxDb, but given that we've already built a TxDb once, it simply loads the cached version. In order to remove the cached TxDb and regenerate, one can remove the relevant entry from the `tximeta` file cache that resides at the location given by `getTximetaBFC()`.  The `se` object created by `tximeta`, has the start, end, and strand information for each transcript. Here, we swap out the transcript *GRanges* for exons-by-transcript *GRangesList* (it is a list of *GRanges*, where each element of the list gives the exons for a particular transcript).  ```{r} se.exons <- addExons(se) rowRanges(se.exons)[[1]] ```  As with the transcript ranges, the exon ranges will be generated once and cached locally. As it takes a non-negligible amount of time to generate the exon-by-transcript *GRangesList*, this local caching offers substantial time savings for repeated usage of `addExons` with the same transcriptome.  We have implemented `addExons` to work only on the transcript-level *SummarizedExperiment* object. We provide some motivation for this choice in `?addExons`. Briefly, if it is desired to know the exons associated with a particular gene, we feel that it makes more sense to pull out the relevant set of exons-by-transcript for the transcripts for this gene, rather than losing the hierarchical structure (exons to transcripts to genes) that would occur with a *GRangesList* of exons grouped per gene.  # Easy summarization to gene-level  Likewise, the *tximeta* package can make use of the cached TxDb database for the purpose of summarizing transcript-level quantifications and bias corrections to the gene-level. After summarization, the `rowRanges` reflect the start and end position of the gene, which in Bioconductor are defined by the left-most and right-most genomic coordinates of all the transcripts. As with the transcript and exons, the gene ranges are cached locally for repeated usage. The transcript IDs are stored as a *CharacterList* column `tx_ids`.  ```{r} gse <- summarizeToGene(se) rowRanges(gse) ```  # Add different identifiers  We would like to add support to easily map transcript or gene identifiers from one annotation to another. This is just a prototype function, but we show how we can easily add alternate IDs given that we know the organism and the source of the transcriptome. (This function currently only works for GENCODE and Ensembl gene or transcript IDs but could be extended to work for arbitrary sources.)  ```{r} library(org.Dm.eg.db) gse <- addIds(gse, ""REFSEQ"", gene=TRUE) mcols(gse) ```  # Differential expression analysis  The following code chunk demonstrates how to build a *DESeqDataSet* and begin a differential expression analysis.   ```{r} suppressPackageStartupMessages(library(DESeq2)) # here there is a single sample so we use ~1. # expect a warning that there is only a single sample... suppressWarnings({dds <- DESeqDataSet(gse, ~1)}) # ... see DESeq2 vignette ```  We have a convenient wrapper function that will build a *DGEList* object for use with *edgeR*.  ```{r} suppressPackageStartupMessages(library(edgeR)) y <- makeDGEList(gse) # ... see edgeR User's Guide for further steps ```  The following code chunk demonstrates the code inside of the above wrapper function, and produces the same output.  ```{r} cts <- assays(gse)[[""counts""]] normMat <- assays(gse)[[""length""]] normMat <- normMat / exp(rowMeans(log(normMat))) o <- log(calcNormFactors(cts/normMat)) + log(colSums(cts/normMat)) y <- DGEList(cts) y <- scaleOffset(y, t(t(log(normMat)) + o)) # ... see edgeR User's Guide for further steps ```  The following code chunk demonstrates how one could use the *Swish* method in the fishpond Bioconductor package. Here we use the transcript-level object `se`. This dataset only has a single sample and no inferential replicates, but the analysis would begin with such code. See the Swish vignette in the fishpond package for a complete example:   ```{r eval=FALSE} y <- se # rename the object to 'y' library(fishpond) # if inferential replicates existed in the data, # analysis would begin with: # # y <- scaleInfReps(y) # ... see Swish vignette in the fishpond package ```  For *limma* with *voom* transformation we recommend, as in the *tximport* vignette to generate counts-from-abundance instead of providing an offset for average transcript length.  ```{r} gse <- summarizeToGene(se, countsFromAbundance=""lengthScaledTPM"") library(limma) y <- DGEList(assays(gse)[[""counts""]]) # see limma User's Guide for further steps ```  Above we generated counts-from-abundance when calling `summarizeToGene`. The counts-from-abundance status is then stored in the metadata:  ```{r} metadata(gse)$countsFromAbundance  ```  # Additional metadata  The following information is attached to the *SummarizedExperiment* by `tximeta`:   ```{r} names(metadata(se)) str(metadata(se)[[""quantInfo""]]) str(metadata(se)[[""txomeInfo""]]) metadata(se)[[""tximetaInfo""]] metadata(se)[[""txdbInfo""]] ```  # Errors connecting to a database  `tximeta` makes use of *BiocFileCache* to store transcript and other databases, so saving the relevant databases in a centralized location used by other Bioconductor packages as well. It is possible that an error can occur in connecting to these databases, either if the files were accidentally removed from the file system, or if there was an error generating or writing the database to the cache location. In each of these cases, it is easy to remove the entry in the *BiocFileCache* so that `tximeta` will know to regenerate the transcript database or any other missing database.  If you have used the default cache location, then you can obtain access to your BiocFileCache with:  ```{r} library(BiocFileCache) bfc <- BiocFileCache() ```  Otherwise, you can recall your particular `tximeta` cache location with `getTximetaBFC()`.  You can then inspect the entries in your BiocFileCache using `bfcinfo` and remove the entry associated with the missing database with `bfcremove`.  See the BiocFileCache vignette for more details on finding and removing entries from a BiocFileCache.  Note that there may be many entries in the BiocFileCache location, including `.sqlite` database files and serialized `.rds` files. You should only remove the entry associated with the missing database, e.g. if R gave an error when trying to connect to the TxDb associated with GENCODE v99 human transcripts, you should look for the `rid` of the entry associated with the human v99 GTF from GENCODE.  # What if checksum isn't known?  `tximeta` automatically imports relevant metadata when the transcriptome matches a known source -- *known* in the sense that it is in the set of pre-computed hashed checksums in `tximeta` (GENCODE, Ensembl, and RefSeq for human and mouse). `tximeta` also facilitates the linking of transcriptomes used in building the *Salmon* index with relevant public sources, in the case that these are not part of this pre-computed set known to `tximeta`. The linking of the transcriptome source with the quantification files is important in the case that the transcript sequence no longer matches a known source (uniquely combined or filtered FASTA files), or if the source is not known to `tximeta`. Combinations of coding and non-coding human, mouse, and fruit fly *Ensembl* transcripts should be automatically recognized by `tximeta` and does not require making a *linkedTxome*. As the package is further developed, we plan to roll out support for all common transcriptomes, from all sources.  Below we demonstrate how to make a *linkedTxome* and how to share and load a *linkedTxome*.  We point to a *Salmon* quantification file which was quantified against a transcriptome that included the coding and non-coding *Drosophila melanogaster* transcripts, as well as an artificial transcript of 960 bp (for demonstration purposes only).  ```{r} file <- file.path(dir, ""SRR1197474.plus"", ""quant.sf"") file.exists(file) coldata <- data.frame(files=file, names=""SRR1197474"", sample=""1"",                       stringsAsFactors=FALSE) ```  Trying to import the files gives a message that `tximeta` couldn't find a matching transcriptome, so it returns an non-ranged *SummarizedExperiment*.   ```{r} se <- tximeta(coldata) ```  # Linked transcriptomes  If the transcriptome used to generate the *Salmon* index does not match any transcriptomes from known sources (e.g. from combining or filtering known transcriptome files), there is not much that can be done to automatically populate the metadata during quantification import. However, we can facilitate the following two cases:   1) the transcriptome was created locally and has been linked to its public source(s)  2) the transcriptome was produced by another group, and they have produced and shared a file that links the transcriptome to public source(s)  `tximeta` offers functionality to assist reproducible analysis in both of these cases.  To make this quantification reproducible, we make a `linkedTxome` which records key information about the sources of the transcript FASTA files, and the location of the relevant GTF file. It also records the checksum of the transcriptome that was computed by *Salmon* during the `index` step.  **Multiple GTF/GFF files:** `linkedTxome` and `tximeta` do not currently support multiple GTF/GFF files, which is a more complicated case than multiple FASTA, which is supported. Currently, we recommend that users should add or combine GTF/GFF files themselves to create a single GTF/GFF file that contains all features used in quantification, and then upload such a file to *Zenodo*, which can then be linked as shown below. Feel free to contact the developers on the Bioconductor support site or GitHub Issue page for further details or feature requests.  By default, `linkedTxome` will write out a JSON file which can be shared with others, linking the checksum of the index with the other metadata, including FASTA and GTF sources. By default, it will write out to a file with the same name as the `indexDir`, but with a `.json` extension added. This can be prevented with `write=FALSE`, and the file location can be changed with `jsonFile`.  First we specify the path where the *Salmon* index is located.   Typically you would not use `system.file` and `file.path` to locate this directory, but simply define `indexDir` to be the path of the *Salmon* directory on your machine. Here we use `system.file` and `file.path` because we have included parts of a *Salmon* index directory in the *tximeta* package itself for demonstration of functionality in this vignette.  ```{r} indexDir <- file.path(dir, ""Dm.BDGP6.22.98.plus_salmon-0.14.1"") ```  Now we provide the location of the FASTA files and the GTF file for this transcriptome.   **Note:** the basename for the GTF file is used as a unique identifier for the cached versions of the *TxDb* and the transcript ranges, which are stored on the user's behalf via *BiocFileCache*. This is not an issue, as GENCODE, Ensembl, and RefSeq all provide GTF files which are uniquely identified by their filename, e.g. `Drosophila_melanogaster.BDGP6.22.98.gtf.gz`.  The recommended usage of `tximeta` would be to specify a remote GTF source, as seen in the commented-out line below:   ```{r} fastaFTP <- c(""ftp://ftp.ensembl.org/pub/release-98/fasta/drosophila_melanogaster/cdna/Drosophila_melanogaster.BDGP6.22.cdna.all.fa.gz"",               ""ftp://ftp.ensembl.org/pub/release-98/fasta/drosophila_melanogaster/ncrna/Drosophila_melanogaster.BDGP6.22.ncrna.fa.gz"",               ""extra_transcript.fa.gz"") #gtfFTP <- ""ftp://path/to/custom/Drosophila_melanogaster.BDGP6.22.98.plus.gtf.gz"" ```  Instead of the above commented-out FTP location for the GTF file, we specify a location within an R package. This step is just to avoid downloading from a remote FTP during vignette building. This use of `file.path` to point to a file in an R package is specific to this vignette and should not be used in a typical workflow. The following GTF file is a modified version of the release 98 from Ensembl, which includes description of a one transcript, one exon artificial gene which was inserted into the transcriptome (for demonstration purposes only).   ```{r} gtfPath <- file.path(dir,""Drosophila_melanogaster.BDGP6.22.98.plus.gtf.gz"") ```  Finally, we create a *linkedTxome*.  In this vignette, we point to a temporary directory for the JSON file, but a more typical workflow would write the JSON file to the same location as the *Salmon* index by not specifying `jsonFile`.  `makeLinkedTxome` performs two operation: (1) it creates a new entry in an internal table that links the transcriptome used in the *Salmon* index to its sources, and (2) it creates a JSON file such that this *linkedTxome* can be shared.  ```{r} tmp <- tempdir() jsonFile <- file.path(tmp, paste0(basename(indexDir), "".json"")) makeLinkedTxome(indexDir=indexDir,                 source=""Ensembl"", organism=""Drosophila melanogaster"",                 release=""98"", genome=""BDGP6.22"",                 fasta=fastaFTP, gtf=gtfPath,                 jsonFile=jsonFile) ```  After running `makeLinkedTxome`, the connection between this *Salmon* index (and its checksum) with the sources is saved for persistent usage. Note that because we added a single transcript of 960bp to the FASTA file used for quantification, `tximeta` could tell that this was not quantified against release 98 of the Ensembl transcripts for *Drosophila melanogaster*. Only when the correct set of transcripts were specified does `tximeta` recognize and import the correct metadata.  With use of `tximeta` and a *linkedTxome*, the software figures out if the remote GTF has been accessed and compiled into a *TxDb* before, and on future calls, it will simply load the pre-computed metadata and transcript ranges.  Note the warning that 5 of the transcripts are missing from the GTF file and so are dropped from the final output. This is a problem coming from the annotation source, and not easily avoided by `tximeta`.   ```{r} se <- tximeta(coldata) ```  We can see that the appropriate metadata and transcript ranges are attached.  ```{r} rowRanges(se) seqinfo(se) ```  # Clear *linkedTxomes*  The following code removes the entire table with information about the *linkedTxomes*. This is just for demonstration, so that we can show how to load a JSON file below.  **Note:** Running this code will clear any information about *linkedTxomes*. Don't run this unless you really want to clear this table!  ```{r} library(BiocFileCache) if (interactive()) {   bfcloc <- getTximetaBFC() } else {   bfcloc <- tempdir() } bfc <- BiocFileCache(bfcloc) bfcinfo(bfc) bfcremove(bfc, bfcquery(bfc, ""linkedTxomeTbl"")$rid) bfcinfo(bfc) ```  # Loading *linkedTxome* JSON files  If a collaborator or the Suppmentary Files for a publication shares a `linkedTxome` JSON file, we can likewise use `tximeta` to automatically assemble the relevant metadata and transcript ranges. This implies that the other person has used `tximeta` with the function `makeLinkedTxome` demonstrated above, pointing to their *Salmon* index and to the FASTA and GTF source(s).  We point to the JSON file and use `loadLinkedTxome` and then the relevant metadata is saved for persistent usage. In this case, we saved the JSON file in a temporary directory.  ```{r} jsonFile <- file.path(tmp, paste0(basename(indexDir), "".json"")) loadLinkedTxome(jsonFile) ```  Again, using `tximeta` figures out whether it needs to access the remote GTF or not, and assembles the appropriate object on the user's behalf.  ```{r} se <- tximeta(coldata) ```  # Clear *linkedTxomes* again  Finally, we clear the *linkedTxomes* table again so that the above examples will work. This is just for the vignette code and not part of a typical workflow.  **Note:** Running this code will clear any information about *linkedTxomes*. Don't run this unless you really want to clear this table!  ```{r} if (interactive()) {   bfcloc <- getTximetaBFC() } else {   bfcloc <- tempdir() } bfc <- BiocFileCache(bfcloc) bfcinfo(bfc) bfcremove(bfc, bfcquery(bfc, ""linkedTxomeTbl"")$rid) bfcinfo(bfc) ```  # Other quantifiers  `tximeta` can import the output from any quantifiers that are supported by `tximport`, and if these are not *Salmon*, *alevin*, or *Sailfish* output, it will simply return a non-ranged *SummarizedExperiment* by default.  An alternative solution is to wrap other quantifiers in workflows that include metadata information JSON files along with each quantification file. One can place these files in `aux_info/meta_info.json` or any relative location specified by `customMetaInfo`, for example `customMetaInfo=""meta_info.json""`. This JSON file is located relative to the quantification file and should contain a tag `index_seq_hash` with an associated value of the SHA-256 hash of the reference transcripts. For computing the hash value of the reference transcripts, see the [FastaDigest](https://github.com/COMBINE-lab/FastaDigest) python package. The hash value used by *Salmon* is the SHA-256 hash value of the reference sequences stripped of the header lines, and concatenated together with the empty string (so only cDNA sequences combined without any new line characters). *FastaDigest* can be installed with `pip install fasta_digest`.  # Automated analysis with ARMOR  This vignette described the use of `tximeta` to import quantification data into R/Bioconductor with automatic detection and addition of metadata. The *SummarizedExperiment* produced by `tximeta` can then be provided to downstream statistical analysis packages as described above. The *tximeta* package does not contain any functionality for automated differential analysis.   The [ARMOR](https://github.com/csoneson/ARMOR) workflow does automate  a variety of differential analyses, and make use of `tximeta` for creation of a *SummarizedExperiment* with attached annotation metadata. ARMOR stands for  ``An Automated Reproducible MOdular Workflow for Preprocessing and Differential Analysis of RNA-seq Data'' and is described in more detail in the article by @Orjuelag2019.  # Acknowledgments  The development of *tximeta* has benefited from suggestions from these and other individuals in the community:  * Vincent Carey * Lori Shepherd * Martin Morgan * Koen Van den Berge * Johannes Rainer * James Ashmore * Ben Johnson * Tim Triche * Kristoffer Vitting-Seerup  # Next steps  **Integration with GA4GH / refget API**  * We are collaborating and in communication with GA4GH working groups   to build out functionality to perform lookup on collections of   transcripts, i.e. transcriptomes, provided by Ensembl. This will   greatly expand the applicability of `tximeta`. The current version   of `tximeta` relies on hashing of common transcriptomes, which are   stored within the package's `extdata` directory in a CSV file, or   on the use of *linkedTxome* for any additional transcriptomes.    However, with the GA4GH / `refget` API in place, `tximeta` will   be able to identify and access transcriptome metadata for vastly   more reference transcriptomes (collections of transcripts).  **Facilitate plots and summaries**      * Basic plots across samples: abundances, mapping rates, rich bias model parameters * Time summaries: when quantified? when imported?  **Extended functionality**  * Facilitate functional annotation, either with vignettes/workflow or   with additional functionality. E.g.:    housekeeping genes, arbitrary gene sets, genes expressed in GTEx tissues * `liftOver` is clunky and doesn't integrate with   *GenomeInfoDb*. It requires user input and there's a chance to   mis-annotate. Ideally this should all be automated.  # Session info  ```{r} library(devtools) session_info() ```  # References",29955
"5","parody","MultipleComparison","--- title: ""parody: parametric and resistant outlier dytection"" shorttitle: ""outlier dytection methods"" author: ""Vincent J. Carey, stvjc at channing.harvard.edu"" date: ""`r format(Sys.time(), '%B %d, %Y')`"" vignette: >   %\VignetteEngine{knitr::rmarkdown}   %\VignetteIndexEntry{parody: parametric and resistant outlier dytection}   %\VignetteEncoding{UTF-8} output:   BiocStyle::html_document:     highlight: pygments     number_sections: yes     theme: united     toc: yes --- ```{r setuppp,echo=FALSE} suppressPackageStartupMessages({ library(parody) library(BiocStyle) }) ```  # Introduction  Outliers are data points that seem anomalous. Precise quantitative measurement of anomalousness is easiest when a parametric probability model is adopted.  Significant statistical and computational research has been devoted to devising and rationalizing criteria for outlyingness that do not require adoption of a parametric probability model.  This package provides interfaces to various methods presented in the literature of statistical methods for measuring outlyingness in univariate and multivariate samples.  It is offered as a resource through bioconductor.org because formal testing for outliers is a common concern of genome scale data analysis, in many different contexts.  # Univariate samples  Davies and Gather (1992) present a detailed study of criteria for outlyingness in univariate samples.  Inlier boundaries are defined, with form $(\hat{m}-c(n)\hat{s}, \hat{m} + c(n) \hat{s})$, where $\hat{m}$ is a location parameter estimate, $\hat{s}$ is a scale parameter estimate, and $c(n)$ are multipliers depending on sample size. Candidates for $m$ are trimmed mean, median, midpoint of shorth; candidates for $s$ include trimmed standard deviation, median absolute deviation (MAD), length of shorth.  The familiar boxplot outlier labeling rules do not fall immediately into this framework, as the left and right inlier boundaries may not be equidistant from the location parameter value.  This package provides functions implementing components of the various labeling rules; users may mix components to define their own procedures.  ```{r do1} library(parody) ```  We define a dataset presented in Rosner (1983). The data concern tapping frequencies achieved by children with various exposures to lead. ```{r dun21} lead <- c(83, 70, 62, 55, 56, 57, 57, 58, 59, 50, 51,          52, 52, 52, 54, 54, 45, 46, 48,         48, 49, 40, 40, 41, 42, 42, 44,          44, 35, 37, 38, 38, 34, 13, 14) ``` We can use a boxplot to visualize the distribution.  This includes an outlier labeling criterion, and the dots beyond the whiskers are declared to be outlying. ```{r dobox1} boxplot(lead) ```  The first formal assessment uses the familiar boxplot rules. For this, we need to supply a scaling function as described in the manual page. ```{r dobox2} calout.detect(lead,alpha=.05,method=""boxplot"", scaling= function(n,alpha)1.5) ``` We can see that this agrees with the R visualization.  Theoretical work on the calibration of the boxplot rule is deployed when `ftype` is set to `""ideal""`. See the manual page for references. ```{r dobox3} calout.detect(lead,alpha=.05,method=""boxplot"",ftype=""ideal"") ```  Another procedure that does not fit in the Davies and Gather framework is the generalized extreme studentized deviate procedure due to B Rosner.  In this procedure, we prespecify the number of possibly outlying points $k$ (which may be any number less than half the sample size), and the $k$ most extreme studentized deviates are obtained and recorded from the highest to lowest value.  Then repeated `outward testing' is conducted using analytic critical values that bound the rate of false outlier labeling for the entire testing procedure, whether or not outliers are present.  Here we apply Rosner's procedure: ```{r dor} calout.detect(lead,alpha=.05,method=""GESD"",k=5) ``` It gives results that agree with the standard boxplot.  Davies and Gather characterize a rule based on median and MAD using scaling functions $c(n)$ that they associate with Hampel.  This procedure uses the scaling function defined in their expression (3). ```{r domemd} calout.detect(lead,alpha=.05,method=""medmad"",scaling=hamp.scale.3) ``` Finally we use the shorth-based detector. ```{r dosh} calout.detect(lead,alpha=.05,method=""shorth"") ```  # Multivariate samples  A sample dataset called `tcost` is supplied with the package.  Before visualizing, we compute a default multivariate outlier criterion, due to Caroni and Prescott, that generalizes Rosner's GESD: ```{r dot} data(tcost) ostr = mv.calout.detect(tcost) ostr ``` We see that indices 9 and 21 are flagged.  Let's create a pairs plot with coloring. ```{r doparis} thecol = rep(""black"", nrow(tcost)) thecol[ostr$ind] = ""red"" pairs(tcost, col=thecol, pch=19) ```  Another display of interest employs principal components: ```{r dobipi} pc = prcomp(tcost) biplot(pc) ```  To get a sense of the robustness (or lack thereof) of the biplot, let us replot after removing the datapoints that were marked as outliers by the Caroni and Prescott procedure. ```{r mvmv} ftcost = tcost[-c(9,21),] fpc = prcomp(ftcost) biplot(fpc) ```  ",5176
"6","flowWorkspace","ImmunoOncology:FlowCytometry:DataImport:Preprocessing","--- title: ""flowWorkspace Introduction: A Package to store and maninpulate gated flow data"" output:   html_document:     number_sections: yes     theme: united     toc: yes     toc_float: yes   pdf_document:     toc: yes author: Greg Finak <greg@ozette.ai>, Mike Jiang <mike@ozette.ai> vignette: >       %\VignetteKeywords{flow cytometry, single cell assay, import}   %\VignettePackage{flowWorkspace}     %\VignetteIndexEntry{flowWorkspace Introduction: A Package to store and maninpulate gated flow data}   %\VignetteEngine{knitr::rmarkdown} ---  ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE, results = ""markup"", message = FALSE) ```   ## Purpose The purpose of this package is to provide the infrastructure to store, represent and exchange gated flow data. By this we mean accessing the samples, groups, transformations, compensation matrices, gates, and population statistics in the gating tree, which is represented as a `GatingSet` object in `R`.  There are several ways to generate a `GatingSet`: * built from scratch within `R` (which will be demonstrated later) * imported from the XML workspace files exported from other software (e.g. FlowJo, Diva, CytoBank). Details on the importing xml are documented in [CytoML](https://www.bioconductor.org/packages/release/bioc/html/CytoML.html) package. * generated by automated gating framework from [openCyto](https://www.bioconductor.org/packages/release/bioc/html/openCyto.html) package * loaded from the existing GatingSet archive (that was previously saved by `save_gs()` call)  Here we simply load an example `GatingSet` archive to illustrate how to interact with a `GatingSet` object. ```{r parsews,message=FALSE, warning=FALSE} library(flowWorkspace) dataDir <- system.file(""extdata"",package=""flowWorkspaceData"") gs_archive <- list.files(dataDir, pattern = ""gs_bcell_auto"",full = TRUE) gs <- load_gs(gs_archive) gs  ``` We have loaded a `GatingSet` with `r length(gs)` samples, each  of which has `r length(gs_get_pop_paths(gs))-1` associated gates.  To list the samples stored in `GatingSet`: ```{r sampleNames} sampleNames(gs) ```  ## Basics on GatingSet  Subsets of a `GatingSet` can be accessed using the standard R subset syntax `[`. ```{r subset} gs[1] ```  We can plot the gating tree: ```{r plotTree} plot(gs, bool = TRUE) ``` The boolean gates(notes) are highlighted in blue color.  We can list the nodes (populations) in the gating hierarchy: ```{r gs_get_pop_paths-path-1} gs_get_pop_paths(gs, path = 2) ``` Note that the `path` argument specifies the depth of the gating path for each population.  As shown, `depth` of `1` (i.e. leaf or terminal node name) may not be sufficient to uniquely identify each population. The issue can be resolved by increasing the `path` or simply returning the full path of the node: ```{r gs_get_pop_paths-path-full} gs_get_pop_paths(gs, path = ""full"") ``` But `full` path may not be necessary and could be too long to be visualized. So we provide the `path = 'auto'` option to determine the shortest path that is still unique within the gating tree. ```{r gs_get_pop_paths-path-auto} nodelist <- gs_get_pop_paths(gs, path = ""auto"") nodelist ```  We can get the gate associated with the specific population:  ```{r gh_pop_get_gate} node <- nodelist[3] g <- gs_pop_get_gate(gs, node) g ``` We can retrieve the population statistics : ```{r getStats} gs_pop_get_stats(gs)[1:10,] ``` We can plot individual gates. Note the scale of the transformed axes. The second argument is the node path of any depth as long as it is uniquely identifiable. ```{r autoplot-nodeName} library(ggcyto) autoplot(gs, node) ``` More details about gate visualization can be found [here](http://bioconductor.org/packages/release/bioc/html/ggcyto.html).  If we have  metadata associated with the experiment, it can be attached to the `GatingSet`. ```{r annotate} d <- data.frame(sample=factor(c(""sample 1"", ""sample 2"")),treatment=factor(c(""sample"",""control"")) ) pd <- pData(gs) pd <- cbind(pd,d) pData(gs) <- pd pData(gs) ``` We can subset the `GatingSet` by its `pData` directly: ```{r} subset(gs, treatment == ""control"") ```  The underlying `flow data` can be retrieved by: ```{r} cs <- gs_pop_get_data(gs) class(cs) nrow(cs[[1]]) ``` Because `GatingSet` is a purely reference class, the class type returned by `gs_pop_get_data` is a `cytoset`, which is the purely reference class analog of a `flowSet` and will be discussed in more detail below. Also note that the data is already compensated and transformed during the parsing.  We can retrieve the subset of data associated with a population node: ```{r getData-gh} cs <- gs_pop_get_data(gs, node) nrow(cs[[1]]) ``` ## GatingHierarchy We can retrieve a single gating hierarchical tree (corresponding to one sample) by using the `[[` extraction operator ```{r gh} gh <- gs[[1]] gh ``` Note that the index can be either numeric or character (the `guid` returned by the `sampleNames` method)  The `autoplot` method without specifying any node will lay out all the gates in the same plot ```{r} autoplot(gh) ```  We can retrieve the indices specifying if an event is included inside or outside a gate using: ```{r getInd} table(gh_pop_get_indices(gh,node)) ``` The indices returned are relative to the parent population (member of parent AND member of current gate), so they reflect the true hierarchical gating structure.   ## Build the GatingSet from scratch `GatingSet` provides methods to build a gating tree from raw FCS files and add or remove flowCore gates (or populations) to or from it.  We start from a `flowSet` that contains three ungated flow samples: ```{r create gs} library(flowCore) data(GvHD) #select raw flow data fs <- GvHD[1:2] ``` Then construct a `GatingSet` from the `flowSet`: ```{r GatingSet constructor} gs <- GatingSet(fs) ```  Then compensate it: ```{r compensate} cfile <- system.file(""extdata"",""compdata"",""compmatrix"", package=""flowCore"") comp.mat <- read.table(cfile, header=TRUE, skip=2, check.names = FALSE) ## create a compensation object  comp <- compensation(comp.mat) #compensate GatingSet gs <- compensate(gs, comp) ```  <span style = ""color:red"">**New**: You can now pass a list of `compensation` objects with elements named by `sampleNames(gs)` to achieve sample-specific compensations. e.g. </span> ```{r eval=FALSE} gs <- compensate(gs, comp.list) ```    Then we can transform it with any transformation defined by the user through `trans_new` function of `scales` package. ```{r user-transformation} require(scales) trans.func <- asinh inv.func <- sinh trans.obj <- trans_new(""myAsinh"", trans.func, inv.func) ``` The `inverse` transformation is required so that the gates and data can be visualized in `transformed` scale while the axis label still remains in the raw scale. Optionally, the `breaks` and `format` functions can be supplied to further customize the appearance of axis labels.  Besides doing all these by hand, we also provide some buildin transformations: `asinhtGml2_trans`, `flowjo_biexp_trans`, `flowjo_fasinh_trans` and `logicle_trans`. These are all very commonly used transformations in flow data analysis. User can construct the transform object by simply one-line of code. e.g. ```{r transform-build-in} trans.obj <- asinhtGml2_trans() trans.obj ```  Once a `transformer` object is created, we must convert it to `transformerList` for `GatingSet` to use. ```{r transformerList} chnls <- colnames(fs)[3:6]  transList <- transformerList(chnls, trans.obj) ``` Alternatively, the overloaded `estimateLogicle` method can be used directly on `GatingHierarchy` to generate a `transformerList` object automatically. ```{r estimateLogicle} estimateLogicle(gs[[1]], chnls) ```  Now we can transform our `GatingSet` with this `transformerList` object. It will also store the transformation in the `GatingSet` and can be used to inverse-transform the data. ```{r transform-gs} gs <- transform(gs, transList) gs_get_pop_paths(gs)  ``` It now only contains the root node. We can add our first `rectangleGate`: ```{r add-rectGate} rg <- rectangleGate(""FSC-H""=c(200,400), ""SSC-H""=c(250, 400), filterId=""rectangle"") nodeID <- gs_pop_add(gs, rg) nodeID gs_get_pop_paths(gs)   ``` Note that the gate is added to the root node by default if the parent is not specified. Then we add a `quadGate` to the new population generated by the `rectangleGate` which is named after the `filterId` of the gate because the name was not specified when the `add` method was called. ```{r add-quadGate} qg <- quadGate(""FL1-H""= 0.2, ""FL2-H""= 0.4) nodeIDs <- gs_pop_add(gs,qg,parent=""rectangle"") nodeIDs  gs_get_pop_paths(gs) ``` Here `quadGate` produces four population nodes/populations named after the dimensions of the gate if names are not specified.  A Boolean gate can also be defined and added to GatingSet: ```{r add-boolGate} bg <- booleanFilter(`CD15 FITC-CD45 PE+|CD15 FITC+CD45 PE-`) bg nodeID2 <- gs_pop_add(gs,bg,parent=""rectangle"") nodeID2 gs_get_pop_paths(gs) ``` The gating hierarchy is plotted by: ```{r plot-gh} plot(gs, bool=TRUE) ``` Note that Boolean gate is skipped by default and thus needs to be enabled explictily.  Now all the gates are added to the gating tree but the actual data is not gated yet. This is done by calling the `recompute` method explictily: ```{r recompute} recompute(gs) ``` After gating is finished, gating results can be visualized by the `autoplot` method: ```{r autoplot-rect} autoplot(gs,""rectangle"") #plot one Gate ``` Multiple gates can be plotted on the same panel: ```{r autoplot-multiple} autoplot(gs, gs_pop_get_children(gs[[1]], ""rectangle"")[1:4]) ``` We may also want to plot all the gates without specifying the gate index: ```{r autoplot-gh-bool} autoplot(gs[[1]]) ```  We can retrieve all the compensation matrices from the `GatingHierarchy` in case we wish to use the compensation or transformation for the new data,   ```{r getCMAT} gh <- gs[[1]] gh_get_compensations(gh); ```  Or we can retrieve transformations:  ```{r getTrans,results='markup'} trans <- gh_get_transformations(gh) names(trans) trans[[1]] ```  If we want to remove one node, simply: ```{r rm} Rm('rectangle', gs) gs_get_pop_paths(gs) ``` As we see, removing one node causes all its descendants to be removed as well.  ### Archive and Clone  Oftentimes, we need to save a `GatingSet` including the gated flow data, gates, and populations to disk and reload it later on. This can be done by: ```{r archive,eval=FALSE} tmp <- tempdir() save_gs(gs,path = file.path(tmp,""my_gs"")) gs <- load_gs(file.path(tmp,""my_gs"")) ```  We also provide the `gs_clone` method to make a full copy of an existing `GatingSet`: ```{r clone,eval=FALSE} gs1 <- gs_clone(gs) ```  To only copy the gates and populations without copy the underlying cyto data.  ```{r copy-tree,eval=FALSE} gs2 <- gs_copy_tree_only(gs) ``` This is a lightweight copying which is faster than `gs_clone`. But be aware the new `GatingSet` share the same events data (i.e. `gs_cyto_data(gs)`) with the original one.  Note that the `GatingSet` is a purely reference class with an external pointer that points to the internal 'C' data structure. So make sure to use these methods in order to save or make a copy of an existing `GatingSet` object. The regular R assignment (<-) or `save` routine doesn't work as expected for `GatingSet` objects.  ## The cytoframe and cytoset classes  The `GatingSet` class no longer uses `flowFrame` and `flowSet` objects for containing the underlying flow data, but rather now uses the analogous `cytoframe` and `cytoset` classes. `cytoframe` and `cytoset` are essentially reference classes with pointers to internal 'C' data structures and thus enable `GatingSet` operations to be performed more efficiently.  While working with `GatingSet` objects will often entail working with `cytoframe` and `cytoset` objects implicitly, it is also possible to directly work with objects of both of these classes.  ### Reading a `cytoframe`  Instead of `read.FCS()`, `cytoframe` objects can be created from FCS files with the `load_cytoframe_from_fcs()` method. The optional `num_threads` argument allows for parallelization of the read operation. ```{r load_cf} files <- list.files(dataDir, ""Cyto"", full.names = TRUE) cf <- load_cytoframe_from_fcs(files[1], num_threads = 4) cf ```  Instead of using `read.FCSheader()` to obtain only the header of the file, just use the `text.only` argument to `load_cytoframe_from_fcs()`. ```{r load_cf_header} cfh <- load_cytoframe_from_fcs(files[1], text.only = TRUE) cfh ```  ### `cytoframe` Accessors   The accessor methods function the same as they would for a `flowFrame` ```{r dim_cf} dim(cf) ``` ```{r colnames_cf} colnames(cf) ``` ```{r exprs_cf} head(exprs(cf)) ``` ```{r spill_cf} spillover(cf) ``` ```{r keys_cf} head(keyword(cf)) ```  ### Pass By Reference  As `cytoframe` and `cytoset` are reference classes, copying objects of either class by the assignment operator (`<-`) will simply provide a copy of the external pointer and so changes made to the copy will also affect the original object.   ```{r ref1_cf} cf1 <- cf # cf is a reference colnames(cf1)[1] ``` ```{r ref2_cf} colnames(cf1)[1] <- ""t"" colnames(cf)[1] # The change affects the original cf object ```  ### Views  Extracting a subset of a `cytoframe` is not computationally intensive, as it merely constructs a view of the data of the original `cytoframe`. However, both objects still share the same underlying pointer to all of the data and thus changes to a view will affect the data of the original `cytoframe`.  ```{r view_cf} cf1 <- cf[1:10, 2:3] dim(cf1) exprs(cf)[2,3] exprs(cf1)[2,2] <- 0 # data change affects the orignal cf exprs(cf)[2,3] ``` To construct a new view of an entire `cytoframe`, use the `[]` method rather than the `<-` operator. This will ensure that a new view is created to the full underlying dataset. ```{r shallow_cf} cf1 <- cf[] ```  ### Deep Copy  It is also possible to perform a deep copy of a `cytoframe` or a view of it, resulting in two objects pointing to distinct C-level representations of the data. This is accomplished with the `realize_view` method. ```{r deep_cf} cf <- load_cytoframe_from_fcs(files[1], num_threads = 4) # starting fresh cf1 <- realize_view(cf[1:10, 2:3]) dim(cf1) exprs(cf)[2,3] exprs(cf1)[2,2] <- 0 # data change no longer affects the original cf exprs(cf)[2,3] exprs(cf1)[2,2] # but does affect the separate data of cf1 ``` Similarly, if a deep copy of all of the data is desired (not a subset), simply call `realize_view` on the original `cytoframe`.  ### Interconversion between `cytoframe` and `flowFrame`  Conversion of objects between the `cytoframe` and `flowFrame` classes is accomplished with a few coercion methods ```{r coerce_cf} fr <- cytoframe_to_flowFrame(cf) class(fr) cf_back <- flowFrame_to_cytoframe(fr) class(cf_back) ``` Of course (as a side note), here `flowFrame_to_cytoframe()` had no knowledge of the `cytoframe` origin of `fr`, so `cf_back` points to a new copy of the underlying data. ```{r pnt_cmp} identical(cf@pointer, cf_back@pointer) # These point to distinct copies of the data ```  ### Saving/Loading a `cytoframe` in h5  A couple of methods handle the task of writing or reading a `cytoframe` in the HDF5 format on disk ```{r h5} tmpfile <- tempfile(fileext = "".h5"") cf_write_h5(cf, tmpfile) loaded <- load_cytoframe(tmpfile) ```  ### `cytoset` methods  Most of the above methods for `cytoframe` objects have `cytoset` analogs.  For reading in a `cytoset` from FCS files, use `load_cytoset_from_fcs` ```{r load_cs} files <- list.files(dataDir, ""Cyto"", full.names = TRUE) cs <- load_cytoset_from_fcs(files, num_threads = 4) cs ``` Once constructed, it can be saved/loaded through more efficient archive format.  ```{r} tmp <- tempfile() save_cytoset(cs, tmp) cs <- load_cytoset(tmp, backend_readonly = FALSE) ``` note that `backend_readonly` is set to `TRUE` by default to protect the data from accidental changes. So it has to be turned off explicitly if your want to modify the loaded `cs`   The accessor methods function the same as they would for a `flowSet` ```{r colnames_cs} colnames(cs) ```  Subsetting using `[` will work in a manner similar to that for a `flowSet`, but will result in another `cytoset` that is a view in to the data of the original `cytoset`. The `Subset()` method, when called on a `cytoset`, will also return a `cytoset` that is a view in to the orignal data rather than a deep copy.  ```{r subset_cs} sub_cs <- cs[1] ```  **Important:** xtraction using `[[` on a `cytoset` will by default return a `cytoframe` and so will represent a reference of the underlying data. Thus, altering the result of the extraction **will** alter the underlying data of the original `cytoset`.  ```{r extract1_cf} sub_fr <- cs[[1]] exprs(cs[[1]])[2,2] exprs(sub_fr)[2,2] <- 0 # This WILL affect the original data exprs(cs[[1]])[2,2] ```  To return a `flowFrame` that represents a copy of the data of the `original` cytoset, you need to use the `returnType` argument.  ```{r extract2_cf} sub_cf <- cs[[1, returnType = ""flowFrame""]] exprs(cs[[1]])[2,2] exprs(sub_cf)[2,2] <- 100 # This WILL NOT affect the original data exprs(cs[[1]])[2,2] ```  Alternatively, if it is easier to remember, `get_cytoframe_from_cs` will accomplish the same goal ```{r extract3_cf} sub_cf <- get_cytoframe_from_cs(cs,1) ```  Finally, the `[]` and `realize_view()` methods work in a similar manner for `cytoset` objects as `cytoframe` objects. `[]` will return a view in to the original data while `realize_view()` will perform a deep copy.  ## Troubleshooting and error reporting If this package is throwing errors when parsing your workspace, contact the package author by emails for post an issue on https://github.com/RGLab/flowWorkspace/issues. If you can send your workspace by email, we can test, debug, and fix the package so that it works for you. Our goal is to provide a tool that works, and that people find useful.  ",17907
"7","flowWorkspace","ImmunoOncology:FlowCytometry:DataImport:Preprocessing","--- title: ""How to merge/standardize GatingSets"" output:   html_document:     number_sections: yes     theme: united     toc: yes     toc_float: yes   pdf_document:     toc: yes author: Greg Finak <greg@ozette.ai>, Mike Jiang <mike@ozette.ai> vignette: >       %\VignetteIndexEntry{How to merge GatingSets}   %\VignetteEngine{knitr::rmarkdown} ---  How to merge/standardize GatingSets ========================================================  Usage ------------------------------------------------------ ```{r eval=FALSE} gs_split_by_tree(x) gs_check_redundant_nodes(x) gs_remove_redundant_nodes(x,toRemove) gs_remove_redundant_channels(gs, ...) ```  Arguments ------------------------------------------------------ **x**   'GatingSet' objects or or list of groups (each group member is a list of 'GatingSet`)  **toRemove**   list of the node sets to be removed. its length must equals to the length of argument **x**  **...** other arguments	  ```{r echo=FALSE, message=FALSE, results='hide'} library(flowWorkspace) flowDataPath <- system.file(""extdata"", package = ""flowWorkspaceData"") gs <- load_gs(file.path(flowDataPath,""gs_manual"")) gs1 <- gs_clone(gs) sampleNames(gs1) <- ""1.fcs""  # simply the tree nodes <- gs_get_pop_paths(gs1) for(toRm in nodes[grepl(""CCR"", nodes)])   gs_pop_remove(gs1, toRm)  # remove two terminal nodes gs2 <- gs_clone(gs1) sampleNames(gs2) <- ""2.fcs"" gs_pop_remove(gs2, ""DPT"") gs_pop_remove(gs2, ""DNT"")  # remove singlets gate gs3 <- gs_clone(gs2) gs_pop_remove(gs3, ""singlets"") gs_pop_add(gs3, gs_pop_get_gate(gs2, ""CD3+""), parent = ""not debris"") for(tsub in c(""CD4"", ""CD8""))   {     gs_pop_add(gs3, gs_pop_get_gate(gs2, tsub), parent = ""CD3+"")     for(toAdd in gs_pop_get_children(gs2, tsub))     {         thisParent <- gs_pop_get_parent(gs2[[1]], toAdd,path=""auto"")         gs_pop_add(gs3, gs_pop_get_gate(gs2, toAdd), parent = thisParent)      }   } sampleNames(gs3) <- ""3.fcs""  # spin the branch to make it isomorphic gs4 <- gs_clone(gs3) # rm cd4 branch first gs_pop_remove(gs4, ""CD4"") # add it back gs_pop_add(gs4, gs_pop_get_gate(gs3, ""CD4""), parent = ""CD3+"") # add all the chilren back for(toAdd in gs_pop_get_children(gs3, ""CD4"")) {     thisParent <- gs_pop_get_parent(gs3[[1]], toAdd)     gs_pop_add(gs4, gs_pop_get_gate(gs3, toAdd), parent = thisParent) } sampleNames(gs4) <- ""4.fcs""  gs5 <- gs_clone(gs4) # add another redundant node gs_pop_add(gs5, gs_pop_get_gate(gs, ""CD4/CCR7+ 45RA+"")[[1]], parent = ""CD4"") gs_pop_add(gs5, gs_pop_get_gate(gs, ""CD4/CCR7+ 45RA-"")[[1]], parent = ""CD4"") sampleNames(gs5) <- ""5.fcs""  library(knitr) opts_chunk$set(fig.show = 'hold', fig.width = 4, fig.height = 4, results= 'asis')  ```  ## Remove the redudant leaf/terminal nodes ```{r echo=FALSE} plot(gs1) plot(gs2) ```  Leaf nodes **DNT** and **DPT** are redudant for the analysis and should be **removed** before merging.  ## Hide the non-leaf nodes ```{r echo=FALSE} plot(gs2) plot(gs3) ```  **singlets** node is not present in the second tree. But we **can't** remove it because it will remove all its descendants. We can **hide** it instead.  ```{r} invisible(gs_pop_set_visibility(gs2, ""singlets"", FALSE)) plot(gs2) plot(gs3) ```  Note that even gating trees look the same but **singlets** still physically exists so  we must refer the populations by **relative path** (`path = ""auto""`)  instead of **full path**. ```{r results='hold'} gs_get_pop_paths(gs2)[5] gs_get_pop_paths(gs3)[5] ```  ```{r results='hold'} gs_get_pop_paths(gs2, path = ""auto"")[5] gs_get_pop_paths(gs3, path = ""auto"")[5] ```  ## Isomorphism ```{r echo=FALSE} #restore gs2 invisible(gs_pop_set_visibility(gs2, ""singlets"", TRUE)) ```  ```{r echo=FALSE} plot(gs3) plot(gs4) ```  These two trees are **not identical** due to the **different order** of **CD4** and **CD8**. However they are still mergable thanks to the **reference by gating path** instead of `by numeric indices`  ## convenient wrapper for merging To ease the process of merging large number of batches of experiments, here is some **internal wrappers** to make it **semi-automated**.  ### Grouping by tree structures ```{r} gslist <- list(gs1, gs2, gs3, gs4, gs5) gs_groups <- gs_split_by_tree(gslist) length(gs_groups) ```  This divides all the `GatingSet`s into different groups, each group shares the same tree structure. Here we have `4` groups,  ## Check if the discrepancy can be resolved by dropping leaf nodes ```{r error=TRUE} res <- try(gs_check_redundant_nodes(gs_groups), silent = TRUE) print(res[[1]]) ```  Apparently the non-leaf node (`singlets`) fails this check, and it is up to user to decide whether to hide this node or keep this group separate from further merging.Here we try to hide it.  ```{r} for(gp in gs_groups)   plot(gp[[1]]) ```  Based on the tree structure of each group (usually there aren't as many groups as `GatingSet` objects itself), we will hide `singlets` for `group 2` and `group 4`.  ```{r} for(i in c(2,4))   for(gs in gs_groups[[i]])     invisible(gs_pop_set_visibility(gs, ""singlets"", FALSE)) ```  Now check again with `.gs_check_redundant_nodes` ```{r} toRm <- gs_check_redundant_nodes(gs_groups) toRm ```  Based on this, these groups can be consolidated by dropping  * `CCR7+ 45RA+` and `CCR7+ 45RA-` from `group 1`. * `DNT` and `DPT` from `group 2`.  Sometime it could be difficult to inspect and distinguish tree difference by simply plotting the enire gating tree or looking at this simple flat list of nodes (especially when the entire subtree is missing from cerntain groups). It is helpful to visualize and highlight only the tree hierarchy difference with the helper function `gs_plot_diff_tree`  ```{r, message=FALSE} gs_plot_diff_tree(gs_groups) ```  To proceed the deletion of these nodes, `.gs_remove_redundant_nodes` can be used instead of doing it manually ```{r results='hide'} gs_remove_redundant_nodes(gs_groups, toRm) ```  Now they can be merged into a single `GatingSetList`. ```{r} GatingSetList(gslist) ```  Remove the redundant channels from `GatingSet` ------------------------------------------------------ Sometime there may be the extra `channels` in one data set that prevents it from being merged with other. If these channels are not used by any gates, then they can be safely removed. ```{r} gs_remove_redundant_channels(gs1) ```",6275
"8","Rdisop","ImmunoOncology:MassSpectrometry:Metabolomics","--- title: ""Mass decomposition with the Rdisop package"" author:   - Steffen Neumann$^\dagger$, Anton Pervukhin$^\ddagger$, Sebastian Böcker$^\ddagger$   - $^\dagger$Leibniz Institute of Plant Biochemistry, sneumann@IPB-Halle.DE   - $^\ddagger$Friedrich-Schiller-University Jena, apervukh@minet.uni-jena.de, boecker@minet.uni-jena.de   # - Steffen Neumann^[Leibniz Institute of Plant Biochemistry, Department of Stress and Developmental Biology, sneumann@IPB-Halle.DE]   # - Anton Pervukhin^[Bioinformatics, Friedrich-Schiller-University Jena, apervukh@minet.uni-jena.de]   # - Sebastian Böcker^[Bioinformatics, Friedrich-Schiller-University Jena, boecker@minet.uni-jena.de] date: ""`r Sys.Date()`"" output: rmarkdown::html_vignette     # toc: true # toc and numbering is not supported     # number_sections: true bibliography: Rdisop.bib vignette: >   %\VignetteIndexEntry{Mass decomposition with the Rdisop package}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8} ---  ```{r, include = FALSE} knitr::opts_chunk$set(   collapse = TRUE,   comment = ""#>"" ) ```  ```{r setup} library(Rdisop) ```  ## Introduction  The BioConductor `Rdisop` package is designed to determine the sum formula of  metabolites solely from their exact mass and isotope pattern as obtained from  high resolution mass spectrometry measurements. Algorithms are described in  [@boecker06decomposing; @boecker08decomp; @boecker09sirius; @boecker07fast].  It is designed with compatibility to the Bioconductor packages `XCMS`,  `MassSpecWavelet` and `rpubchem` in mind.  ## Decomposing isotope patterns  After preprocessing, the output of a mass spectrometer is a list of peaks which  corresponds to the masses of the sample molecules and their abundance, i.e., the  amount of sample compounds with a certain mass. In fact, sum formulas of small  molecules can be identified using only accurate output masses. However, even  with very high mass accuracy (<1ppm), many chemically possible formulas are  found in higher mass regions. It has been shown that applying only this data therefore does not suffice to identify a compound, and more information, such as  isotopic abundances, needs to be taken into account. High resolution mass  spectrometry allows us to obtain the isotope pattern of sample molecule with  outstanding accuracy.  ### Chemical background  Atoms are composed of electrons, protons, and neutrons. The number of protons  (the atomic number) is fixed and defines what element the atom is. The number of  neutrons, on the other hand, can vary: Atoms with the same number of protons but  different numbers of neutrons are called *isotopes* of the element. Each of  these isotopes occurs in nature with a certain abundance. The *nominal mass* of  a molecule is the sum of protons and neutrons of the constituting atoms. The  *mass* of the molecule is the sum of masses of these atoms.  The *monoisotopic (nominal) mass* of a molecule is the sum of (nominal) masses  of the constituting atoms where for every element its most abundant natural  isotope is chosen. Clearly, nominal mass and mass depend on the isotopes the  molecule consists of, thus on the *isotope species* of the molecule.  No present-day analysis technique is capable of resolving isotope species with  identical nominal mass. Instead, these isotope species appear as one single peak  in the mass spectrometry output. For this reason, we merge isotope species with  identical nominal mass and refer to the resulting distribution as the molecule's  *isotope pattern*.  ### Identification schema  Obtaining an accurate isotope pattern from a high resolution mass spectrometer,  we apply this information to identify the elemental composition of the sample  molecule. Our input is a list of masses with normalized abundances that  corresponds to the isotope pattern of the sample molecule. We want to find that  molecule's elemental composition whose isotope pattern best matches the input.   Solving this task is divided into the following parts: First, all elemental  compositions are calculated that share some property, for example monoisotopic  mass, with the input spectrum. Second, to remove those compositions that do not  exist in nature, chemical bonding rules are applied, discarding formulas that  have negative or non-integer degree of unsaturation. And third, for every  remaining composition, its theoretical isotope pattern is calculated and  compared to the measured isotope pattern. Candidate patterns are ranked using Bayesian Statistics, and the one with the highest score is chosen.  ## Working with molecules and isotope peaklists  The central object in `Rdisop` is the molecule, which is a list containing the  (sum-)formula, its isotope pattern, a score and other information. Molecules can  either be created explicitly through functions `getMolecule`, `initializeXXX`,  or through `decomposeMass` and `decomposeIsotopes`. Most functions operate only  on a subset of the periodic system of elements (PSE) given as *elements*  argument.  \subsection{Handling of Molecules}  The function `getMolecule` returns a list object containing the information for  a named single atom or a more complex molecule.  ```{r exmpl1} molecule <- getMolecule(""C2H5OH"") getFormula(molecule) getMass(molecule) ```  Note that the formula is in a canonical form, and the mass includes the decimals  (the nominal mass for ethanol would be just 46).  Without further arguments only the elements C, H, N, O, P and S are available.  For metabolomics research, these are the most relevant ones. A different subset  of the PSE can be returned and passed to the functions, but keep in mind that a  larger set of elements yields a (much) larger result set when decomposing masses  later.  ```{r exmpl2} essentialElements <- initializeCHNOPSMgKCaFe() chlorophyll <- getMolecule(""C55H72MgN4O5H"", z=1, elements=essentialElements) isotopes <- getIsotope(chlorophyll, seq(1,4)) isotopes ```  In this case we have created a complex molecule with a charge (z=+1) containing  a metal ion and check its first four isotope peaks. For a visual inspection the  isotope pattern can be plotted.  ```{r fig1, fig.cap=""Isotope pattern for a protonated chlorophyll ion, which could be observed on a high-resolution mass spectrometer in positive mode.""} plot(t(isotopes), type=""h"", xlab=""m/z"", ylab=""Intensity"") ```  ### Functions `decomposeMass` and `decomposeIsotopes`  The function `decomposeMass` returns a list of molecules which have a given  exact mass (within an error window in ppm):  ```{r exmpl3} molecules <- decomposeMass(46.042, ppm=20, maxisotopes = 4) molecules ```  This call produces a list of potential molecules (with a single element in this  case). The larger the masses, the allowed ppm deviation and the allowed elements  list, the larger the result list will grow. For each hypothesis there is its  formula and weight and score. The parity, validity (using the nitrogen rule) and  double bond equivalents (DBE) are simple, yet commonly used hints for the  plausibility of a solution and can be used for filtering the results list. For  an amino acid this simple method guesses already eight hypotheses:  ```{r exmpl4} length(decomposeMass(147.053)) ```  On modern mass spectrometers a full isotope pattern can be obtained for a  molecule, and the masses and intensities improve the accuracy of the sum formula  prediction. Accessor functions return only subsets of the molecule data  structure:  ```{r exmpl5} # glutamic acid (C5H9NO4) masses <- c(147.053, 148.056) intensities <- c(93, 5.8)  molecules <- decomposeIsotopes(masses, intensities) data.frame(getFormula(molecules), getScore(molecules), getValid(molecules)) ```  The first ranked solution already has a score close to one, and if using an  N-rule filter, only one solution would remain. These cases are not removed by  default, because a few compound classes do not obey the N-rule, which after all  is just a simple heuristic.  If the masses were obtained by an LC-ESI-MS, it is likely that the measured mass  signal actually resembles an adduct ion, such as [M+H]^+^. The sum formula  obtained through `decomposeIsotopes` will have one H too much, and will not be  found in PubChem or other libraries, unless the adduct has been removed:  ```{r exmpl6} getFormula(subMolecules(""C5H10NO4"", ""H"")) ```  Similarly, if during ionization an in-source fragmentation occurred, the lost  fragment can be added before querying using `addMolecules`.  ### Interaction with other BioConductor packages  This section will give some suggestions how the `Rdisop` functionality can be  combined with other BioConductor packages.  Usually the masses and intensities will be obtained from a high-resolution mass  spectrometer such as an FT-ICR-MS or QTOF-MS. BioConductor currently has two  packages dealing with peak picking on raw machine data, `MassSpecWavelet` and  `XCMS`. The latter contains a wrapper for `MassSpecWavelet`, so we need to deal  with `XCMS` peak lists only. The `ESI` package^[not part of BioConductor, see  [http://msbi.ipb-halle.de/](http://msbi.ipb-halle.de/)] can extract a set of  isotope clusters from peak lists.  After `Rdisop` has created a set of candidate molecular formulae, the  open-access compound databases PubChem or ChEBI can be queried whether any  information about this compound exists. *Nota bene*: a hit or non-hit does not  indicate a correct or incorrect formula, but merely helps in further  verification or structure elucidation steps. For other cheminformatics  functionality in BioConductor see e.g. `RCDK`.  ## Acknowledgments  AP supported by Deutsche Forschungsgemeinschaft (BO 1910/1), additional  programming by Marcel Martin, whom we thank for his unfailing support, and by  Marco Kortkamp.  ## References  <div id=""refs""></div>",9797
"9","glmGamPoi","Regression:RNASeq:Software:SingleCell","--- title: ""glmGamPoi Quickstart"" author: Constantin Ahlmann-Eltze date: ""`r Sys.Date()`"" output: BiocStyle::html_document vignette: >   %\VignetteIndexEntry{glmGamPoi Quickstart}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8} ---  ```{r, include = FALSE} knitr::opts_chunk$set(   collapse = TRUE,   comment = ""#>"" ) set.seed(2) ```   > Fit Gamma-Poisson Generalized Linear Models Reliably.  The core design aims of `gmlGamPoi` are:  * Fit the Gamma-Poisson models on arbitrarily large or small datasets * Be faster than alternative methods, such as `DESeq2` or `edgeR` * Calculate exact or approximate results based on user preference * Support in memory or on-disk data * Follow established conventions around tools for RNA-seq analysis * Present a simple user-interface * Avoid unnecessary dependencies * Make integration into other tools easy   # Installation  You can install the release version of `r BiocStyle::Biocpkg(""glmGamPoi"")` from BioConductor:  ``` r if (!requireNamespace(""BiocManager"", quietly = TRUE))     install.packages(""BiocManager"")  BiocManager::install(""glmGamPoi"") ```  For the latest developments, see the `r BiocStyle::Githubpkg(""const-ae/glmGamPoi"", ""GitHub"")` repo.   # Example  Load the glmGamPoi package  ```{r} library(glmGamPoi) ```   To fit a single Gamma-Poisson GLM do:  ```{r} # overdispersion = 1/size counts <- rnbinom(n = 10, mu = 5, size = 1/0.7)  # design = ~ 1 means that an intercept-only model is fit fit <- glm_gp(counts, design = ~ 1) fit  # Internally fit is just a list: as.list(fit)[1:2] ```  The `glm_gp()` function returns a list with the results of the fit. Most importantly, it contains the estimates for the coefficients β and the overdispersion.  Fitting repeated Gamma-Poisson GLMs for each gene of a single cell dataset is just as easy:  I will first load an example dataset using the `TENxPBMCData` package. The dataset has 33,000 genes and 4340 cells. It takes roughly 1.5 minutes to fit the Gamma-Poisson model on the full dataset. For demonstration purposes, I will subset the dataset to 300 genes, but keep the 4340 cells: ```{r, warning=FALSE, message = FALSE} library(SummarizedExperiment) library(DelayedMatrixStats) ```  ```{r} # The full dataset with 33,000 genes and 4340 cells # The first time this is run, it will download the data pbmcs <- TENxPBMCData::TENxPBMCData(""pbmc4k"")  # I want genes where at least some counts are non-zero non_empty_rows <- which(rowSums2(assay(pbmcs)) > 0) pbmcs_subset <- pbmcs[sample(non_empty_rows, 300), ] pbmcs_subset ```  I call `glm_gp()` to fit one GLM model for each gene and force the calculation to happen in memory.  ```{r} fit <- glm_gp(pbmcs_subset, on_disk = FALSE) summary(fit) ```    # Benchmark  I compare my method (in-memory and on-disk) with `r BiocStyle::Biocpkg(""DESeq2"")` and `r BiocStyle::Biocpkg(""edgeR"")`. Both are classical methods for analyzing RNA-Seq datasets and have been around for almost 10 years. Note that both tools can do a lot more than just fitting the Gamma-Poisson model, so this benchmark only serves to give a general impression of the performance.    ```{r, warning=FALSE} # Explicitly realize count matrix in memory so that it is a fair comparison pbmcs_subset <- as.matrix(assay(pbmcs_subset)) model_matrix <- matrix(1, nrow = ncol(pbmcs_subset))   bench::mark(   glmGamPoi_in_memory = {     glm_gp(pbmcs_subset, design = model_matrix, on_disk = FALSE)   }, glmGamPoi_on_disk = {     glm_gp(pbmcs_subset, design = model_matrix, on_disk = TRUE)   }, DESeq2 = suppressMessages({     dds <- DESeq2::DESeqDataSetFromMatrix(pbmcs_subset,                         colData = data.frame(name = seq_len(4340)),                         design = ~ 1)     dds <- DESeq2::estimateSizeFactors(dds, ""poscounts"")     dds <- DESeq2::estimateDispersions(dds, quiet = TRUE)     dds <- DESeq2::nbinomWaldTest(dds, minmu = 1e-6)   }), edgeR = {     edgeR_data <- edgeR::DGEList(pbmcs_subset)     edgeR_data <- edgeR::calcNormFactors(edgeR_data)     edgeR_data <- edgeR::estimateDisp(edgeR_data, model_matrix)     edgeR_fit <- edgeR::glmFit(edgeR_data, design = model_matrix)   }, check = FALSE, min_iterations = 3 ) ```  On this dataset, `glmGamPoi` is more than 5 times faster than `edgeR` and more than 18 times faster than `DESeq2`. `glmGamPoi` does **not** use approximations to achieve this performance increase. The performance comes from an optimized algorithm for inferring the overdispersion for each gene. It is tuned for datasets typically encountered in single RNA-seq with many samples and many small counts, by avoiding duplicate calculations.  To demonstrate that the method does not sacrifice accuracy, I compare the parameters that each method estimates. The means and β coefficients are identical, but that the overdispersion estimates from `glmGamPoi` are more reliable:  ```{r message=FALSE, warning=FALSE} # Results with my method fit <- glm_gp(pbmcs_subset, design = model_matrix, on_disk = FALSE)  # DESeq2 dds <- DESeq2::DESeqDataSetFromMatrix(pbmcs_subset,                          colData = data.frame(name = seq_len(4340)),                         design = ~ 1) sizeFactors(dds)  <- fit$size_factors dds <- DESeq2::estimateDispersions(dds, quiet = TRUE) dds <- DESeq2::nbinomWaldTest(dds, minmu = 1e-6)  #edgeR edgeR_data <- edgeR::DGEList(pbmcs_subset, lib.size = fit$size_factors) edgeR_data <- edgeR::estimateDisp(edgeR_data, model_matrix) edgeR_fit <- edgeR::glmFit(edgeR_data, design = model_matrix) ```   ```{r coefficientComparison, fig.height=5, fig.width=10, warning=FALSE, echo = FALSE} par(mfrow = c(2, 4), cex.main = 2, cex.lab = 1.5) plot(fit$Beta[,1], coef(dds)[,1] / log2(exp(1)), pch = 16,       main = ""Beta Coefficients"", xlab = ""glmGamPoi"", ylab = ""DESeq2"") abline(0,1) plot(fit$Beta[,1], edgeR_fit$unshrunk.coefficients[,1], pch = 16,      main = ""Beta Coefficients"", xlab = ""glmGamPoi"", ylab = ""edgeR"") abline(0,1)  plot(fit$Mu[,1], assay(dds, ""mu"")[,1], pch = 16, log=""xy"",      main = ""Gene Mean"", xlab = ""glmGamPoi"", ylab = ""DESeq2"") abline(0,1) plot(fit$Mu[,1], edgeR_fit$fitted.values[,1], pch = 16, log=""xy"",      main = ""Gene Mean"", xlab = ""glmGamPoi"", ylab = ""edgeR"") abline(0,1)  plot(fit$overdispersions, rowData(dds)$dispGeneEst, pch = 16, log=""xy"",      main = ""Overdispersion"", xlab = ""glmGamPoi"", ylab = ""DESeq2"") abline(0,1) plot(fit$overdispersions, edgeR_fit$dispersion, pch = 16, log=""xy"",      main = ""Overdispersion"", xlab = ""glmGamPoi"", ylab = ""edgeR"") abline(0,1)  ```  I am comparing the gene-wise estimates of the coefficients from all three methods. Points on the diagonal line are identical. The inferred Beta coefficients and gene means agree well between the methods, however the overdispersion differs quite a bit. `DESeq2` has problems estimating most of the overdispersions and sets them to `1e-8`. `edgeR` only approximates the overdispersions which explains the variation around the overdispersions calculated with `glmGamPoi`.    ## Scalability  The method scales linearly, with the number of rows and columns in the dataset. For example: fitting the full `pbmc4k` dataset with subsampling on a modern MacBook Pro in-memory takes ~1 minute and on-disk a little over 4 minutes. Fitting the `pbmc68k` (17x the size) takes ~73 minutes (17x the time) on-disk.   ## Differential expression analysis  `glmGamPoi` provides an interface to do quasi-likelihood ratio testing to identify differentially expressed genes. To demonstrate this feature, we will use the data from [Kang _et al._ (2018)](https://www.ncbi.nlm.nih.gov/pubmed/29227470) provided by the `MuscData` package. This is a single cell dataset of 8 Lupus patients for which 10x droplet-based scRNA-seq was performed before and after treatment with interferon beta. The `SingleCellExperiment` object conveniently provides the patient id (`ind`), treatment status (`stim`) and cell type (`cell`):  ```{r} sce <- muscData::Kang18_8vs8() colData(sce) ```  For demonstration purpose, I will work on a subset of the genes and cells:  ```{r} set.seed(1) # Take highly expressed genes and proper cells: sce_subset <- sce[rowSums(counts(sce)) > 100,                    sample(which(sce$multiplets == ""singlet"" &                                ! is.na(sce$cell) &                               sce$cell %in% c(""CD4 T cells"", ""B cells"", ""NK cells"")),                           1000)] # Convert counts to dense matrix counts(sce_subset) <- as.matrix(counts(sce_subset)) # Remove empty levels because glm_gp() will complain otherwise sce_subset$cell <- droplevels(sce_subset$cell) ```  We will identify which genes in CD4 positive T-cells are changed most by the treatment. We will fit a full model including the interaction term `stim:cell`. The interaction term will help us identify cell type specific responses to the treatment:  ```{r} fit <- glm_gp(sce_subset, design = ~ cell + stim +  stim:cell - 1,               reference_level = ""NK cells"") summary(fit) ```   To see how the coefficient of our model are called, we look at the `colnames(fit$Beta)`:  ```{r} colnames(fit$Beta) ```   In our example, we want to find the genes that change specifically in T cells. Finding cell type specific responses to a treatment is a big advantage of single cell data over bulk data. To get a proper estimate of the uncertainty (cells from the same donor are **not** independent replicates), we create a pseudobulk for each sample:  ```{r} # The contrast argument specifies what we want to compare # We test the expression difference of stimulated and control T-cells # # There is no sample label in the colData, so we create it on the fly # from `stim` and `ind` columns in colData(fit$data). de_res <- test_de(fit, contrast = `stimstim` + `cellCD4 T cells:stimstim`,                    pseudobulk_by = paste0(stim, ""-"", ind))   # The large `lfc` values come from groups were nearly all counts are 0 # Setting them to Inf makes the plots look nicer de_res$lfc <- ifelse(abs(de_res$lfc) > 20, sign(de_res$lfc) * Inf, de_res$lfc)  # Most different genes head(de_res[order(de_res$pval), ]) ```  The test is successful and we identify interesting genes that are differentially expressed in interferon-stimulated T cells:  _IFI6_, _IFIT3_, and _IRF7_ literally stand for _Interferon Induced/Regulated Protein_.  To get a more complete overview of the results, we can make a volcano plot that compares the log2-fold change (LFC) vs the logarithmized p-values.  ```{r} library(ggplot2) ggplot(de_res, aes(x = lfc, y = -log10(pval))) +   geom_point(size = 0.6, aes(color = adj_pval < 0.1)) +   ggtitle(""Volcano Plot"", ""Genes that change most through interferon-beta treatment in T cells"") ```   Another important task in single cell data analysis is the identification of marker genes for cell clusters. For this we can also use our Gamma-Poisson fit.   Let's assume we want to find genes that differ between T cells and the B cells. We can directly compare the corresponding coefficients and find genes that differ in the control condition:  ```{r} marker_genes <- test_de(fit, `cellCD4 T cells` - `cellB cells`, sort_by = pval) head(marker_genes) ```  If we want find genes that differ in the stimulated condition, we just include the additional coefficients in the contrast:  ```{r} marker_genes2 <- test_de(fit, (`cellCD4 T cells` + `cellCD4 T cells:stimstim`) -                                 (`cellB cells` + `cellB cells:stimstim`),                          sort_by = pval)  head(marker_genes2) ```  We identify many genes related to the human leukocyte antigen (HLA) system that is important for antigen presenting cells like B-cells, but are not expressed by T helper cells. The plot below shows the expression differences.  A note of caution: applying `test_de()` to single cell data without the pseudobulk gives overly optimistic p-values. This is due to the fact that cells from the same sample are not independent replicates! It can still be fine to use the method for identifying marker genes, as long as one is aware of the difficulties interpreting the results.  ```{r} # Create a data.frame with the expression values, gene names, and cell types tmp <- data.frame(gene = rep(marker_genes$name[1:6], times = ncol(sce_subset)),                   expression = c(counts(sce_subset)[marker_genes$name[1:6], ]),                   celltype = rep(sce_subset$cell, each = 6))  ggplot(tmp, aes(x = celltype, y = expression)) +   geom_jitter(height = 0.1) +   stat_summary(geom = ""crossbar"", fun = ""mean"", color = ""red"") +   facet_wrap(~ gene, scales = ""free_y"") +   ggtitle(""Marker genes of B vs. T cells"") ```         # Session Info  ```{r} sessionInfo() ```   ",12628
"10","glmGamPoi","Regression:RNASeq:Software:SingleCell","--- title: ""Pseudobulk and differential expression"" author: ""Constantin Ahlmann-Eltze"" date: ""`r Sys.Date()`"" output: BiocStyle::html_document vignette: >   %\VignetteIndexEntry{Pseudobulk and differential expression}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8} ---  ```{r, include = FALSE} knitr::opts_chunk$set(   collapse = TRUE,   comment = ""#>"" ) set.seed(2) ```   # Pseudobulk  A pseudobulk sample is formed by aggregating the expression values from a group of cells from the same individual. The cells are typically grouped by clustering or cell type assignment. Individual refers to the experimental unit of replication (e.g., the individual mice or patients).  Forming pseudobulk samples is important to perform accurate differential expression analysis. Cells from the same individual are more similar to each other than to cells from another individual. This means treating each cell as an independent sample leads to underestimation of the variance and misleadingly small p-values. Working on the level of pseudobulks ensures reliable statistical tests because the samples correspond to the units of replication.  We can use pseudobulks for example to find the expression changes between two conditions for one cell type.  # Example  ```{r out.width = '70%', echo = FALSE} knitr::include_graphics(""img/kang_data_overview.png"") ```  I load a `SingleCellExperiment` object containing gene expression counts from eight Lupus patient before and after interferon beta stimulation. The creator of the dataset has already annotated the cell types and if cell is a singlet.  ```{r} sce <- muscData::Kang18_8vs8()  # Keep only genes with more than 5 counts sce <- sce[rowSums(counts(sce)) > 5,] colData(sce) ```  The `pseudobulk` functions emulates the `group_by` and `summarize` pattern popularized by the `tidyverse`. You provide the columns from the `colData` that you want to use for grouping the data (akin to `group_by`) and named arguments specifiying how you summarize the remaining columns (akin to `summarize`). Using the `aggregation_functions` you can set how the `assay`'s and `reducedDim`'s are summarized with a named list.  Here, I create a pseudobulk sample for each patient, condition, and cell type. This means for example that the counts of the 119 B-cells from patient 101 in the control condition are summed to one column in the reduced dataset.   The first argument is a `SingleCellExperiment` object. The `group_by` argument uses `vars()` to quote the grouping columns The `fraction_singlet` and `n_cells` arguments demonstrate how additional columns from the `colData` are summarized. For `fraction_singlet`, I use the fact that `mean` automatically coerces a boolean vector to zeros and ones and `n_cells` demonstrates the `n()` function that returns the number of cells that are aggregated for each group. ```{r} library(glmGamPoi) reduced_sce <- pseudobulk(sce, group_by = vars(ind, condition = stim, cell),                            fraction_singlet = mean(multiplets == ""singlet""), n_cells = n()) colData(reduced_sce) ``` You can simulate the pseudobulk sample generation and check if you are using the correct arguments by calling `dplyr::group_by`. Note that the order of the output differs because `group_by` automatically sorts the keys. ```{r} library(dplyr, warn.conflicts = FALSE) colData(sce) %>%   as_tibble() %>%   group_by(ind, condition = stim, cell) %>%   summarize(n_cells = n(), .groups = ""drop"")  ```    With the reduced data, we can conduct differential expression analysis the same way we would analyze bulk RNA-seq data (using tools like `DESeq2` and `edgeR`). For example we can find the genes that change most upon treatment in the B-cells  ```{r} # Remove NA's reduced_sce <- reduced_sce[,!is.na(reduced_sce$cell)] # Use DESeq2's size factor calculation procedure fit <- glm_gp(reduced_sce, design = ~ condition*cell + ind, size_factor = ""ratio"", verbose = TRUE) res <- test_de(fit, contrast = cond(cell = ""B cells"", condition = ""stim"") - cond(cell = ""B cells"", condition = ""ctrl"")) ```  A volcano plot gives a quick impression of the overall distribution of the expression changes. ```{r} library(ggplot2, warn.conflicts = FALSE) ggplot(res, aes(x = lfc, y = - log10(pval))) +   geom_point(aes(color = adj_pval < 0.01), size = 0.5) ```    # Legacy  Originally, `glmGamPoi`'s API encouraged forming pseudobulks after fitting the model (i.e., within `test_de()`). The advantage was that this reduced the number of functions. Yet, internally `glmGamPoi` basically threw away the original fit and re-ran it on the aggregated data. This meant that computation time was wasted. Thus the original approach forming the pseudobulk in `test_de` is now deprecated in favor of first calling `pseudobulk()` and then proceed by calling `glm_gp()` and `test_de()` on the aggregated data.   ",4853
"11","rhdf5","Infrastructure:DataImport","--- title: ""**rhdf5** Practical Tips"" author: - name: Mike L. Smith   affiliation:      - EMBL Heidelberg     - German Network for Bioinformatics Infrastructure (de.NBI) package: rhdf5 output:   BiocStyle::html_document:     toc_float: true abstract: |   Provides discussion and practical examples for effectively using *rhdf5*    and the HDF5 file format. vignette: |   %\VignetteIndexEntry{rhdf5 Practical Tips}   %\VignetteEncoding{UTF-8}   %\VignetteEngine{knitr::rmarkdown} editor_options:    chunk_output_type: console ---  ```{r setup, echo = FALSE, include=FALSE} set.seed(1234)  library(rhdf5) library(dplyr) library(ggplot2) library(BiocParallel) ```  # Introduction  There are scenarios where the most intuitive approach to working with *rhdf5* or HDF5 will not be the most efficient.  This may be due to unfamiliar bottlenecks when working with data on-disk rather than in memory, or idiosyncrasies in either the HDF5 library itself or the *rhdf5* package.  This vignette is intended to present a collection of hints for circumventing some common pitfalls.  # Reading subsets of data  One of the cool features about the HDF5 file format is the ability to read subsets of the data without (necessarily) having to read the entire file, keeping both the memory usage and execution times of these operations to a minimum.  However this is not always as performant as one might hope.  To demonstrate we'll create some example data.  This takes the form of a matrix with 100 rows and 20,000 columns, where the content of each column is the index of the column i.e. column 10 contains the value 10 repeated, column 20 contains 20 repeated etc.  This is just so we can easily check we've extracted the correct columns. We then write this matrix to an HDF5 file, calling the dataset 'counts'. ^[You'll probably see a warning here regarding chunking, something we'll touch on later]  ```{r create data, echo=TRUE, warning=FALSE} m1 <- matrix(rep(1:20000, each = 100), ncol = 20000, byrow = FALSE) ex_file <- tempfile(fileext = "".h5"") h5write(m1, file = ex_file, name = ""counts"", level = 6) ``` ## Using the `index` argument  Now we'll use the `index` argument to selectively extract the first 10,000 columns and time how long this takes.  ```{r extract1, echo = TRUE} system.time(   res1 <- h5read(file = ex_file, name = ""counts"",                   index = list(NULL, 1:10000)) ) ```  Next, instead of selecting 10,000 consecutive columns we'll ask for every other column.  This should still return the same amount of data and since our dataset is not chunked involves reading the same volume from disk.  ```{r extract2, echo = TRUE} index <- list(NULL, seq(from = 1, to = 20000, by = 2)) system.time(   res2 <- h5read(file = ex_file, name = ""counts"",                   index = index) ) ```  We can see this is marginally slower, because there's a small overhead in selecting this disjoint set of columns, but it's only marginal and using the `index` argument looks sufficient.  ## Using hyperslab selections  If you're new to R, but have experience with HDF5 you might be more familiar with using HDF5's hyperslab selection method^[The parameters for defining hyperslab selection `start`, `stride`, `block`, & `count` are not particularly intuitive if you are used to R's index selection methods.  More examples discussing how to specify them can be found at [www.hdfgroup.org](https://portal.hdfgroup.org/display/HDF5/Reading+From+or+Writing+To+a+Subset+of+a+Dataset).  The following code defines the parameters to select every other column, the same as in our previous example.  ```{r extract3, echo = TRUE} start <- c(1,1) stride <- c(1,2) block <- c(100,1) count <- c(1,10000) system.time(   res3 <- h5read(file = ex_file, name = ""counts"", start = start,                  stride = stride, block = block, count = count) ) identical(res2, res3) ```  This runs in a similar time to when we used the `index` argument in the example above, and the call to `identical()` confirms we're returning the same data.  In fact, under the hood, *rhdf5* converts the `index` argument into `start`, `stride`, `block`, & `count` before accessing the file, which is why the performance is so similar.   If there is a easily described pattern to the regions you want to access e.g. a single block or a regular spacing, then either of these approaches is effective.  ## Irregular selections  However, things get a little more tricky if you want an irregular selection of data, which is actually a pretty common operation.  For example, imagine wanting to select a random set of columns from our data.  If there isn't a regular pattern to the columns you want to select, what are the options?  Perhaps the most obvious thing we can try is to skip the use of either `index` or the hyperslab parameters and use 10,000 separate read operations instead.  Below we choose a random selection of columns^[in the interested of time we actually select only 1,000 columns here] and then apply the function `f1()` to each in turn.  ```{r singleReads, cache = TRUE} columns <- sample(x = seq_len(20000), size = 10000, replace = FALSE) %>%   sort()  f1 <- function(cols, name) {    h5read(file = ex_file, name = name,           index = list(NULL, cols))   } system.time(res4 <- vapply(X = columns, FUN = f1,                             FUN.VALUE = integer(length = 100),                             name = 'counts')) ```  This is clearly a terrible idea, it takes ages!  For reference, using the `index` argument with this set of columns takes `r system.time(h5read(file = ex_file, name = ""counts"", index = list(NULL, columns)))['elapsed']` seconds.  This poor performance is driven by two things:  1. Our dataset was created as a single chunk.  This means for each access the entire dataset is read from disk, which we end up doing thousands of times. 2. *rhdf5* does a lot of validation on the objects that are passed around internally.  Within a call to `h5read()` HDF5 identifiers are created for the file, dataset, file dataspace, and memory dataspace, each of which are checked for validity.  This overhead is negligible when only one call to `h5read()` is made, but becomes significant when we make thousands of separate calls.  There's not much more you can do if the dataset is not chunked appropriately, and using the `index` argument is reasonable.  However storing data in this format defeats one of HDF5's key utilities, namely rapid random access.  As such it's probably fairly rare to encounter datasets that aren't chunked in a more meaningful manner.  With this in mind we'll create a new dataset in our file, based on the same matrix but this time split into 100 $\times$ 100 chunks.   ```{r createChunked, echo = TRUE, eval = TRUE, results='hide'} h5createDataset(file = ex_file, dataset = ""counts_chunked"",                  dims = dim(m1), storage.mode = ""integer"",                  chunk = c(100,100), level = 6) h5write(obj = m1, file = ex_file, name = ""counts_chunked"") ```  If we rerun the same code, but reading from the chunked datasets we get an idea for how much time is wasted extracting the entire dataset over and over.  ```{r read_chunked, eval = TRUE} system.time(res5 <- vapply(X = columns, FUN = f1,                             FUN.VALUE = integer(length = 100),                             name = 'counts_chunked')) ```  This is still quite slow, and the remaining time is being spent on the overheads associated with multiple calls to `h5read()`.  To reduce these the function `f2()`^[This is not the greatest function ever, things like the file name are hardcoded out of sight, but it illustrates the technique.] defined below splits the list of columns we want to return into sets grouped by the parameter `block_size`.  In the default case this means any columns between 1 & 100 will be placed together, then any between 101 & 200, etc.  We then `lapply` our previous `f1()` function over these groups.  The effect here is to reduce the number of calls to `h5read()`, while keeping the number of hyperslab unions down by not having too many columns in any one call.  ```{r} f2 <- function(block_size = 100) {   cols_grouped <- split(columns,  (columns-1) %/% block_size)   res <-  lapply(cols_grouped, f1, name = 'counts_chunked') %>%     do.call('cbind', .) } system.time(f2()) ```  ```{r benchmark, echo = FALSE, cache = TRUE} bm <- bench::mark(   f2(10), f2(25), f2(50), f2(100),    f2(250), f2(500), f2(1000),    f2(2000), f2(5000), f2(10000),   iterations = 3, check = FALSE, time_unit = ""s"", memory = FALSE, filter_gc = FALSE )  bm2 <- data.frame(block_size = gsub("".*\\(([0-9]+)\\)"", ""\\1"", bm$expression) |>                     as.integer() |>                     rep(each = 3),                    time = unlist(bm$time)) ```  We can see this has a significant effect, although it's still an order of magnitude slower than when we were dealing with regularly spaced subsets.  The efficiency here will vary based on a number of factors including the size of the dataset chunks and the sparsity of the column index, and you varying the `block_size` argument will produce differing performances.  The plot below shows the timings achived by providing a selection of values to `block_size`.  It suggests the optimal parameter in this case is probably a block size of `r bm2 %>% arrange(time) %>% slice(1) %>% .$block_size`, which took `r bm2 %>% arrange(time) %>% slice(1) %>% .$time %>% round(2)` seconds - noticeably faster than when passing all columns to the `index` argument in a single call.  ```{r, echo = FALSE, fig.width=6, fig.height=3, fig.wide = TRUE} ggplot(bm2, aes(x = block_size, y = time)) +    geom_point() +    scale_x_log10() +   theme_bw() +    ylab('time (seconds)') ```  ### Using hyperslab selection tools  If we were stuck with the single-chunk dataset and want to minimise the number of read operations, it's necessary to create larger selections than the single column approach used above.  We could again consider using the HDF5 hyperslab selection tools, and if it's not easy to discern an underlying pattern to the selection, perhaps the simplest way of approaching this with the would be to create one selection for each column.  You could then use functions like `H5Scombine_hyperslab()` or `H5Scombine_select()` to iteratively join these selections until all columns were selected, and then perform the read operation.  ### Slowdown when selecting unions of hyperslabs  Unfortunately, this approach doesn't scale very well.  This is because creating unions of hyperslabs is currently very slow in HDF5 (see [Union of non-consecutive hyperslabs is very slow](https://forum.hdfgroup.org/t/union-of-non-consecutive-hyperslabs-is-very-slow/5062) for another report of this behaviour), with the performance penalty increasing exponentially relative to the number of unions.  The plot below shows the the exponential increase in time as the number of hyberslab unions increases.  ```{r, eval = TRUE, echo = FALSE, fig.width=6, fig.height=3, fig.wide = TRUE, fig.cap='The time taken to join hyperslabs increases expontentially with the number of join operations.  These timings are taken with no reading occuring, just the creation of a dataset selection.'} ## this code demonstrates the exponential increase in time as the  ## number of hyberslab unions increases  select_index <- function(n = 1) {    ## open the dataspace for the count table   fid <- H5Fopen(ex_file)   did  <- H5Dopen(fid, name = ""counts"")   sid <- H5Dget_space(did)      ## column choice based on number of unions required   columns <- c(head(1:10001, n = -n), head(seq(10001-n+2, 20000, 2), n = n-1))   index <- list(100, columns)   H5Sselect_index(sid, index = index)      ## tidy up   H5Sclose(sid)   H5Dclose(did)   H5Fclose(fid) }  bm <- bench::mark(   select_index(1), select_index(2), select_index(5),    select_index(10), select_index(20), select_index(50),   select_index(100), select_index(200), select_index(500),   select_index(1000), select_index(2000), select_index(5000),   select_index(10000),   iterations = 3, check = FALSE, time_unit = ""s"", memory = FALSE, filter_gc = FALSE )  bm2 <- data.frame(n = gsub("".*\\(([0-9]+)\\)"", ""\\1"", bm$expression) |>                             as.integer() |>                             rep(each = 3),                    time = unlist(bm$time))  ggplot(bm2,aes(x = n, y = time)) +   geom_point() +    scale_x_log10() +    scale_y_log10() +   theme_bw() +   xlab('number of hyperslab unions') +   ylab('time (seconds)') ```  ## Summary  Efficiently extracting arbitrary subsets of a HDF5 dataset with *rhdf5* is a balancing act between the number of hyperslab unions, the number of calls to `h5read()`, and the number of times a chunk is read. Many of the lessons learnt will creating this document have been incorporated into the `h5read()` function.  Internally, this function attempts to find the balance by looking for patterns in the data selection requested, and minimises the number of hyperslab unions and read operations required to extract the requested data.   # Writing in parallel  Using `r BiocStyle::Biocpkg('rhdf')` it isn't possible to open an HDF5 file and write multiple datasets in parallel.  However we can try to mimic this behaviour by writing each dataset to it's own HDF5 file in parallel and then using the function `H5Ocopy()` to efficiently populate a complete final file.  We'll test this approach here.  ## Example data  First lets create some example data to we written to our HDF5 files.  The code below creates a list of 10 matrices, filled with random values between 0 and 1.  We then name the entries in the list `dset_1` etc.  ```{r example-dsets} dsets <- lapply(1:10, FUN = \(i) { matrix(runif(10000000), ncol = 100)} ) names(dsets) <- paste0(""dset_"", 1:10) ```  ## Serial writing of datasets  Now lets define a function that takes our list of datasets and writes all of them to a single HDF5 file.    ```{r} simple_writer <- function(file_name, dsets) {      fid <- H5Fcreate(name = file_name)   on.exit(H5Fclose(fid))      for(i in seq_along(dsets)) {     dset_name = paste0(""dset_"", i)     h5createDataset(file = fid, dataset = dset_name,                      dims = dim(dsets[[i]]), chunk = c(10000, 10))     h5writeDataset(dsets[[i]], h5loc = fid, name = dset_name)   }    } ```  An example of calling this function would look like: `simple_writer(file_name = ""my_datasets.h5"", dsets = dsets)`.  This would create the file `my_datasets.h5` and it will contain the 10 datasets we created above, each named `dset_1` etc, which are the names we gave the list elements.  ## Parallel writing of datasets  Now lets created two functions to tests our split / gather approach to creating the final file.  The first of the functions below will create a temporary file with a random name and write a single dataset to this file.  The second function expects to be given a table of temporary files and the name of the dataset they contain.  It will then use `H5Ocopy()` to write each of these into a single output file.   ```{r} ## Write a single dataset to a temporary file ## Arguments:  ## - dset_name: The name of the dataset to be created ## - dset: The dataset to be written split_tmp_h5 <- function(dset_name, dset) {    ## create a tempory HDF5 file for this dataset     file_name <- tempfile(pattern = ""par"", fileext = "".h5"")   fid <- H5Fcreate(file_name)   on.exit(H5Fclose(fid))      ## create and write the dataset   ## we use some predefined chunk sizes    h5createDataset(file = fid, dataset = dset_name,                    dims = dim(dset), chunk = c(10000, 10))   h5writeDataset(dset, h5loc = fid, name = dset_name)      return(c(file_name, dset_name)) }  ## Gather scattered datasets into a final single file ## Arguments:  ## - output_file: The path to the final HDF5 to be created ## - input: A data.frame with two columns containing the paths to the temp ## files and the name of the dataset inside that file gather_tmp_h5 <- function(output_file, input) {      ## create the output file   fid <- H5Fcreate(name = output_file)   on.exit(H5Fclose(fid))      ## iterate over the temp files and copy the named dataset into our new file   for(i in seq_len(nrow(input))) {     fid2 <- H5Fopen(input$file[i])     H5Ocopy(fid2, input$dset[i], h5loc_dest = fid, name_dest = input$dset[i])     H5Fclose(fid2)   }    } ```  Finally we need to create a wrapper function that brings our split and gather functions together.  Like the `simple_writer()` function we created earlier, this takes the name of an output file and the list of datasets to be written as input.  We can also provide a `BiocParallelParam` instance from `r BiocStyle::Biocpkg(""BiocParallel"")` to trial writing the temporary file in parallel.  If the `BPPARAM` argument isn't provided then they will be written in serial.  ```{r, define-split-gather} split_and_gather <- function(output_file, input_dsets, BPPARAM = NULL) {      if(is.null(BPPARAM)) { BPPARAM <- BiocParallel::SerialParam() }      ## write each of the matrices to a separate file   tmp <-      bplapply(seq_along(input_dsets),             FUN = function(i) {              split_tmp_h5(dset_name = names(input_dsets)[i],                            dset = input_dsets[[i]])            },             BPPARAM = BPPARAM)       ## create a table of file and the dataset names   input_table <- do.call(rbind, tmp) |>      as.data.frame()   names(input_table) <- c(""file"", ""dset"")      ## copy all datasets from temp files in to final output   gather_tmp_h5(output_file = output_file, input = input_table)      ## remove the temporary files   file.remove(input_table$file) } ```  An example of calling this using two cores on your local machine is:   ```{r, eval = FALSE} split_and_gather(tempfile(), input_dsets = dsets,                  BPPARAM = MulticoreParam(workers = 2)) ```  Below we can see some timings comparing calling `simple_writer()` with `split_and_gather()` using 1, 2, and 4 cores.  ```{r, run-writing-benchmark, cache = TRUE, message=FALSE, echo=FALSE} bench_results <- bench::mark(   ""simple writer"" = simple_writer(file_name = tempfile(), dsets = dsets),   ""split/gather - 1 core"" = split_and_gather(tempfile(), input_dsets = dsets,                                               BPPARAM = NULL),   ""split/gather - 2 cores"" = split_and_gather(tempfile(), input_dsets = dsets,                                                BPPARAM = MulticoreParam(workers = 2)),    ""split/gather - 4 cores"" = split_and_gather(tempfile(), input_dsets = dsets,                                                BPPARAM = MulticoreParam(workers = 4)),   iterations = 3, check = FALSE, time_unit = ""s"", memory = FALSE, filter_gc = FALSE ) bench_results |> select(expression, min, median) ```  We can see from our benchmark results that there is some performance improvement to be achieved by using the parallel approach.  Based on the median times of out three iterations using two cores sees an speedup of `r round(bench_results$median[1] / bench_results$median[3], 2)` and `r round(bench_results$median[1] / bench_results$median[4], 1)` with 4 cores.  This isn't quite linear, presumably be cause there are overheads involved both in using a two-step process and initialising the parallel workers, but it is a noticeable improvement.  # Session info {.unnumbered}  ```{r sessionInfo, echo=FALSE} sessionInfo() ```",19350
"12","rhdf5","Infrastructure:DataImport","--- title: ""Reading HDF5 Files In The Cloud"" author: - name: Mike L. Smith   affiliation: de.NBI & EMBL Heidelberg package: rhdf5 output:   BiocStyle::html_document vignette: |   %\VignetteIndexEntry{Reading HDF5 Files In The Cloud}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8} ---  ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ```  The `r BiocStyle::Biocpkg(""rhdf5"")` provides limited support for read-only access to HDF5 files stored in Amazon S3 buckets.  This is implemented via the [HDF5 S3 Virtual File Driver](https://portal.hdfgroup.org/display/HDF5/Virtual+File+Drivers+-+S3+and+HDFS) and allows access to HDF5 files hosted in both public and private S3 buckets.    Currently only the functions `h5ls()`, `h5dump()` and `h5read()` are supported.  ```{r, load-library} library(rhdf5) ```  # Public S3 Buckets  To access a file in a public Amazon S3 bucket you provide the file's URL to the `file` argument.  You also need to set the argument `s3 = TRUE`, otherwise `h5ls()` will treat the URL as a path on the local disk fail.  ```{r, public-h5ls, error = TRUE, purl = FALSE} public_S3_url <- ""https://rhdf5-public.s3.eu-central-1.amazonaws.com/h5ex_t_array.h5"" h5ls(file = public_S3_url,      s3 = TRUE) ```  The same arguments are also valid for using `h5dump()` to retrieve the contents of a file.  ```{r, public-h5dump, error = TRUE, purl = FALSE} public_S3_url <- ""https://rhdf5-public.s3.eu-central-1.amazonaws.com/h5ex_t_cmpd.h5"" h5dump(file = public_S3_url,      s3 = TRUE) ```  In addition to examining and reading whole files, we can also extract just a subset, without needed to read or download the entire file.  In the example below we use `h5ls()` to examine a file in an S3 bucket and identify the name of a dataset within it (`a1`) and the number of dimensions for that dataset (3).  We can then use `h5read()` along with the `name` and `index` arguments to read only a subset of the dataset into our R session.  ```{r, public-h5read, error = TRUE, purl = FALSE} public_S3_url <- 'https://rhdf5-public.s3.eu-central-1.amazonaws.com/rhdf5ex_t_float_3d.h5' h5ls(file = public_S3_url, s3 = TRUE) h5read(public_S3_url,         name = ""a1"",         index = list(1:2, 3, NULL),        s3 = TRUE) ```  # Private S3 Buckets  To access files in a private Amazon S3 bucket you will need to provide three additional details: The AWS region where the files are hosted, your AWS access key ID, and your AWS secret access key.  More information on how to obtain AWS access keys can be found under [AWS Security Credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys).  These three values need to be stored in a list like below.  *Important note: for now they must be in this specific order.*  ```{r, private-mock-credentials} ## these are example credentials and will not work s3_cred <- list(     aws_region = ""eu-central-1"",     access_key_id = ""AKIAIOSFODNN7EXAMPLE"",     secret_access_key = ""wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"" ) ```  Finally we pass this list to `h5ls()` via the `s3credentials` argument.    ```{r, private-h5ls, eval = FALSE} public_S3_url <- ""https://rhdf5-private.s3.eu-central-1.amazonaws.com/h5ex_t_array.h5"" h5ls(file = public_S3_url,      s3 = TRUE,      s3credentials = s3_cred) ```  The `s3credentials` arguments is used in exactly the same way for `h5dump()` and `h5read()`.  # Session Info ```{r sessioninfo} sessionInfo() ``` ",3477
"13","rhdf5","Infrastructure:DataImport","--- title: ""rhdf5 - HDF5 interface for R"" author: - name: Bernd Fischer   affiliation: DKFZ - name: Mike L. Smith   affiliation: de.NBI & EMBL Heidelberg package: rhdf5 output:   BiocStyle::html_document vignette: |   %\VignetteIndexEntry{rhdf5 - HDF5 interface for R}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8} ---  ```{r cleanup, echo=FALSE, include=FALSE} if(file.exists('myhdf5file.h5'))     file.remove('myhdf5file.h5') if(file.exists('newfile.h5'))     file.remove('newfile.h5') if(file.exists('newfile2.h5'))     file.remove('newfile2.h5') if(file.exists('newfile3.h5'))     file.remove('newfile3.h5') ```  # Introduction The package is an R interface for HDF5. On the one hand it implements **R** interfaces to many of the low level functions from the C interface. On the other hand it provides high level convenience functions on **R** level to make a usage of HDF5 files more easy.  #Installation of the HDF5 package To install the `r BiocStyle::Biocpkg(""rhdf5"")` package, you need a current version (>3.5.0) of **R** (www.r-project.org). After installing **R** you can run the following commands from the **R** command shell to install `r BiocStyle::Biocpkg(""rhdf5"")`.  ```{r installation,eval=FALSE} install.packages(""BiocManager"") BiocManager::install(""rhdf5"") ```  # High level R-HDF5 functions  ## Creating an HDF5 file and group hierarchy  An empty HDF5 file is created by ```{r createHDF5file} library(rhdf5) h5createFile(""myhdf5file.h5"") ```  The HDF5 file can contain a group hierarchy. We create a number of groups and list the file content afterwards. ```{r create groups} h5createGroup(""myhdf5file.h5"",""foo"") h5createGroup(""myhdf5file.h5"",""baa"") h5createGroup(""myhdf5file.h5"",""foo/foobaa"") h5ls(""myhdf5file.h5"") ```  ## Writing and reading objects  Objects can be written to the HDF5 file. Attributes attached to an object are written as well, if `write.attributes=TRUE` is given as argument to `h5write`. Note that not all **R**-attributes can be written as HDF5 attributes.  ```{r writeMatrix} A = matrix(1:10,nr=5,nc=2) h5write(A, ""myhdf5file.h5"",""foo/A"") B = array(seq(0.1,2.0,by=0.1),dim=c(5,2,2)) attr(B, ""scale"") <- ""liter"" h5write(B, ""myhdf5file.h5"",""foo/B"") C = matrix(paste(LETTERS[1:10],LETTERS[11:20], collapse=""""),   nr=2,nc=5) h5write(C, ""myhdf5file.h5"",""foo/foobaa/C"") df = data.frame(1L:5L,seq(0,1,length.out=5),   c(""ab"",""cde"",""fghi"",""a"",""s""), stringsAsFactors=FALSE) h5write(df, ""myhdf5file.h5"",""df"") h5ls(""myhdf5file.h5"") D = h5read(""myhdf5file.h5"",""foo/A"") E = h5read(""myhdf5file.h5"",""foo/B"") F = h5read(""myhdf5file.h5"",""foo/foobaa/C"") G = h5read(""myhdf5file.h5"",""df"") ```  If a dataset with the given `name` does not yet exist, a dataset is created in the HDF5 file and the object `obj` is written to the HDF5 file. If a dataset with the given `name` already exists and the datatype and the dimensions are the same as for the object `obj`, the data in the file is overwritten. If the dataset already exists and either the datatype or the dimensions are different, `h5write()` fails.  ## Writing and reading objects with file, group and dataset handles  File, group and dataset handles are a simpler way to read (and partially to write) HDF5 files. A file is opened by `H5Fopen`.  ```{r accessorH5Fopen} h5f = H5Fopen(""myhdf5file.h5"") h5f ```  The `$` and `&` operators can be used to access the next group level. While the `$` operator reads the object from disk, the `&` operator returns a group or dataset handle.  ```{r accessorDF} h5f$df h5f&'df' ```  Both of the following code lines return the matrix `C`. Note however, that the first version reads the whole tree `/foo` in memory and then subsets to `/foobaa/C`, and the second version only reads the matrix `C`. The first `$` in `h5f$foo$foobaa$C` reads the dataset, the other `$` are accessors of a list. Remind that this can have severe consequences for large datasets and datastructures.  ```{r accessorC1} h5f$foo$foobaa$C h5f$""/foo/foobaa/C"" ```  One can as well return a dataset handle for a matrix and then read the matrix in chunks for out-of-memory computations. \warning{Please read the next section on subsetting, chunking and compression for more details}.  ```{r accessorB1} h5d = h5f&""/foo/B"" h5d[] h5d[3,,] ```  The same works as well for writing to datasets. \warning{Remind that it is only guarenteed that the data is written on disk after a call to `H5Fflush` or after closing of the file.} ```{r accessorB2} h5d[3,,] = 1:4 H5Fflush(h5f) ```  Remind again that in the following code the first version does not change the data on disk, but the second does. ```{r accessorB3,eval=FALSE} h5f$foo$B = 101:120 h5f$""/foo/B"" = 101:120 ```  It is important to close all dataset, group, and file handles when not used anymore ```{r accessorClose1} H5Dclose(h5d) H5Fclose(h5f) ``` or close all open HDF5 handles in the environment by ```{r accessorClose2} h5closeAll() ```  \subsection{Writing and reading with subsetting, chunking and compression} The `r BiocStyle::Biocpkg(""rhdf5"")` package provides two ways of subsetting. One can specify the submatrix with the **R**-style index lists or with the HDF5 style hyperslabs. Note, that the two next examples below show two alternative ways for reading and writing the exact same submatrices. Before writing subsetting or hyperslabbing, the dataset with full dimensions has to be created in the HDF5 file. This can be achieved by writing once an array with full dimensions as in Section \ref{sec_writeMatrix} or by creating a dataset. Afterwards the dataset can be written sequentially.  \paragraph{Influence of chunk size and compression level} The chosen chunk size and compression level have a strong impact on the reading and writing time as well as on the resulting file size. In an example an integer vector of size 10e7 is written to an HDF5 file. The file is written in subvectors of size 10'000. The definition of the chunk size influences the reading as well as the writing time. If the chunk size is much smaller or much larger than actually used, the runtime performance decreases dramatically. Furthermore the file size is larger for smaller chunk sizes, because of an overhead. The compression can be much more efficient when the chunk size is very large. The following figure illustrates the runtime and file size behaviour as a function of the chunk size for a small toy dataset.   ![](../inst/demoChunkSize/chunksize-1.png)  After the creation of the dataset, the data can be written sequentially to the HDF5 file. Subsetting in **R**-style needs the specification of the argument index to `h5read()` and `h5write()`.  ```{r writeMatrixSubsetting} h5createDataset(""myhdf5file.h5"", ""foo/S"", c(5,8),                 storage.mode = ""integer"", chunk=c(5,1), level=7) h5write(matrix(1:5,nr=5,nc=1), file=""myhdf5file.h5"",         name=""foo/S"", index=list(NULL,1)) h5read(""myhdf5file.h5"", ""foo/S"") h5write(6:10, file=""myhdf5file.h5"",         name=""foo/S"", index=list(1,2:6)) h5read(""myhdf5file.h5"", ""foo/S"") h5write(matrix(11:40,nr=5,nc=6), file=""myhdf5file.h5"",         name=""foo/S"", index=list(1:5,3:8)) h5read(""myhdf5file.h5"", ""foo/S"") h5write(matrix(141:144,nr=2,nc=2), file=""myhdf5file.h5"",         name=""foo/S"", index=list(3:4,1:2)) h5read(""myhdf5file.h5"", ""foo/S"") h5write(matrix(151:154,nr=2,nc=2), file=""myhdf5file.h5"",         name=""foo/S"", index=list(2:3,c(3,6))) h5read(""myhdf5file.h5"", ""foo/S"") h5read(""myhdf5file.h5"", ""foo/S"", index=list(2:3,2:3)) h5read(""myhdf5file.h5"", ""foo/S"", index=list(2:3,c(2,4))) h5read(""myhdf5file.h5"", ""foo/S"", index=list(2:3,c(1,2,4,5))) ```  The HDF5 hyperslabs are defined by some of the arguments `start`, `stride`, `count`, and `block`. These arguments are not effective, if the argument `index` is specified.  ```{r writeMatrixHyperslab} h5createDataset(""myhdf5file.h5"", ""foo/H"", c(5,8), storage.mode = ""integer"",                 chunk=c(5,1), level=7) h5write(matrix(1:5,nr=5,nc=1), file=""myhdf5file.h5"", name=""foo/H"",         start=c(1,1)) h5read(""myhdf5file.h5"", ""foo/H"") h5write(6:10, file=""myhdf5file.h5"", name=""foo/H"",         start=c(1,2), count=c(1,5)) h5read(""myhdf5file.h5"", ""foo/H"") h5write(matrix(11:40,nr=5,nc=6), file=""myhdf5file.h5"", name=""foo/H"",         start=c(1,3)) h5read(""myhdf5file.h5"", ""foo/H"") h5write(matrix(141:144,nr=2,nc=2), file=""myhdf5file.h5"", name=""foo/H"",         start=c(3,1)) h5read(""myhdf5file.h5"", ""foo/H"") h5write(matrix(151:154,nr=2,nc=2), file=""myhdf5file.h5"", name=""foo/H"",         start=c(2,3), stride=c(1,3)) h5read(""myhdf5file.h5"", ""foo/H"") h5read(""myhdf5file.h5"", ""foo/H"",        start=c(2,2), count=c(2,2)) h5read(""myhdf5file.h5"", ""foo/H"",        start=c(2,2), stride=c(1,2),count=c(2,2)) h5read(""myhdf5file.h5"", ""foo/H"",        start=c(2,1), stride=c(1,3),count=c(2,2), block=c(1,2)) ```  ## Saving multiple objects to an HDF5 file (h5save)  A number of objects can be written to the top level group of an HDF5 file with the function `h5save()` (as analogous to the base **R** function `save()`). ```{r h5save} A = 1:7;  B = 1:18; D = seq(0,1,by=0.1) h5save(A, B, D, file=""newfile2.h5"") h5dump(""newfile2.h5"") ```  ## List the content of an HDF5 file  The function `h5ls()` provides some ways of viewing the content of an HDF5 file.  ```{r h5ls} h5ls(""myhdf5file.h5"") h5ls(""myhdf5file.h5"", all=TRUE) h5ls(""myhdf5file.h5"", recursive=2) ```  ## Dump the content of an HDF5 file  The function `h5dump()` is similar to the function `h5ls()`. If used with the argument `load=FALSE`, it produces the same result as `h5ls()`, but with the group structure resolved as a hierarchy of lists. If the default argument `load=TRUE` is used all datasets from the HDF5 file are read.  ```{r h5dump} h5dump(""myhdf5file.h5"",load=FALSE) D <- h5dump(""myhdf5file.h5"") ```  ## Reading HDF5 files with external software  The content of the HDF5 file can be checked with the command line tool **h5dump** (available on linux-like systems with the HDF5 tools package installed) or with the graphical user interface **HDFView** (http://www.hdfgroup.org/hdf-java-html/hdfview/) available for all major platforms.  ```{r h5dump2, eval=FALSE} system2(""h5dump"", ""myhdf5file.h5"") ``` *Please note, that arrays appear as transposed matrices when opening it with a C-program (**h5dump** or **HDFView**). This is due to the fact the fastest changing dimension on C is the last one, but on R it is the first one (as in Fortran).*  ## Removing content from an HDF5 file  As well as adding content to an HDF5 file, it is possible to remove entries using the function `h5delete()`.  To demonstrate it's use, we'll first list the contents of a file and examine the size of the file in bytes.  ```{r h5delete1} h5ls(""myhdf5file.h5"", recursive=2) file.size(""myhdf5file.h5"") ```  We then use `h5delete()` to remove the **df** dataset by providing the file name and the name of the dataset, e.g.  ```{r h5delete2} h5delete(file = ""myhdf5file.h5"", name = ""df"") h5ls(""myhdf5file.h5"", recursive=2) ```  We can see that the **df** entry has now disappeared from the listing.  In most cases, if you have a heirachy within the file, `h5delete()`  will remove children of the deleted entry too.  In this example we remove **foo** and the datasets below it are deleted too.  Notice too that the size of the file as decreased.  ```{r h5delete3} h5delete(file = ""myhdf5file.h5"", name = ""foo"") h5ls(""myhdf5file.h5"", recursive=2) file.size(""myhdf5file.h5"") ```  *N.B. `h5delete()` does not explicitly traverse the tree to remove child nodes.  It only removes the named entry, and HDF5 will then remove child nodes if they are now orphaned.  Hence it won't delete child nodes if you have a more complex structure where a child node has multiple parents and only one of these is removed.*  # 64-bit integers  **R** does not support a native datatype for 64-bit integers. All integers in **R** are 32-bit integers. When reading 64-bit integers from a HDF5-file, you may run into troubles. `r BiocStyle::Biocpkg(""rhdf5"")` is able to deal with 64-bit integers, but you still should pay attention.  As an example, we create an HDF5 file that contains 64-bit integers.  ```{r bit64integer1} x = h5createFile(""newfile3.h5"")  D = array(1L:30L,dim=c(3,5,2)) d = h5createDataset(file=""newfile3.h5"", dataset=""D64"", dims=c(3,5,2),H5type=""H5T_NATIVE_INT64"") h5write(D,file=""newfile3.h5"",name=""D64"") ```  There are three different ways of reading 64-bit integers in **R**. `H5Dread()` and `h5read()` have the argument `bit64conversion` the specify the conversion method.  By setting `bit64conversion='int'`, a coercing to 32-bit integers is enforced, with the risk of data loss, but with the insurance that numbers are represented as native integers.  ```{r bit64integer2} D64a = h5read(file=""newfile3.h5"",name=""D64"",bit64conversion=""int"") D64a storage.mode(D64a) ```  `bit64conversion='double'` coerces the 64-bit integers to floating point numbers. doubles can represent integers with up to 54-bits, but they are not represented as integer values anymore. For larger numbers there is still a data loss.  ```{r bit64integer3} D64b = h5read(file=""newfile3.h5"",name=""D64"",bit64conversion=""double"") D64b storage.mode(D64b) ```  `bit64conversion='bit64'` is the recommended way of coercing. It represents the 64-bit integers as objects of class *integer64* as defined in the package `r BiocStyle::CRANpkg(""bit64"")`. Make sure that you have installed `r BiocStyle::CRANpkg(""bit64"")`. *The datatype *integer64* is not part of base **R**, but defined in an external package. This can produce unexpected behaviour when working with the data.* When choosing this option the package `r BiocStyle::CRANpkg(""bit64"")` will be loaded.  ```{r bit64integer4} D64c = h5read(file=""newfile3.h5"",name=""D64"",bit64conversion=""bit64"") D64c class(D64c) ```  <!-- ##Large integer data types  The following table gives an overview of the limits of the different integer representations in R and in HDF5.  \definecolor{Gray1}{gray}{0.85} \definecolor{Gray2}{gray}{0.8} \definecolor{Gray3}{gray}{0.75} \definecolor{LightBlue}{rgb}{0.88,0.88,1}  \newcommand{\N}{-} \newcommand{\Y}{\cellcolor{Gray2}+}  \begin{tabular}{|l|r||c|c|c||c|c|c|c|} \hline \rowcolor{LightBlue} \multicolumn{2}{|c||}{value} & \multicolumn{3}{c||}{R-datatype} & \multicolumn{4}{c|}{HDF5 datatype}\\ \rowcolor{LightBlue} \multicolumn{2}{|c||}{} & integer & double & integer64 & I32 & U32 & I64 & U64 \\ \hline \hline $\phantom{-}2^{64}$ & 18446744073709551616 & \phantom{\vdots} - \phantom{\vdots}& - & - & - & - & - & - \\ \hline $\phantom{-}2^{64}-1$ & 18446744073709551615 & \phantom{\vdots} - \phantom{\vdots}& - & - & - & - & - & \Y \\\hline $\phantom{-}$\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots \\ \hline $\phantom{-}2^{63}$ & 9223372036854775808 & \phantom{\vdots} - \phantom{\vdots} & - & - & - & - & - & \Y \\\hline $\phantom{-}2^{63}-1$ & 9223372036854775807  & \phantom{\vdots} - \phantom{\vdots} & - & \Y & - & - & \Y & \Y \\\hline $\phantom{-}$\vdots & \vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots \\ \hline $\phantom{-}2^{53}$ & 9007199254740992 & \phantom{\vdots} - \phantom{\vdots} & - & \Y & - & - & \Y & \Y \\\hline $\phantom{-}2^{53}-1$ & 9007199254740991 & \phantom{\vdots} - \phantom{\vdots} & \Y & \Y & - & - & \Y & \Y \\\hline $\phantom{-}$\vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots \\ \hline $\phantom{-}2^{32}$ & 4294967296 & \phantom{\vdots} - \phantom{\vdots} & \Y & \Y & - & - & \Y & \Y \\\hline $\phantom{-}2^{32}-1$ & 4294967295 & \phantom{\vdots} - \phantom{\vdots} & \Y & \Y & - & \Y & \Y & \Y \\\hline $\phantom{-}$\vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots \\ \hline $\phantom{-}2^{31}$ & 2147483648 & \phantom{\vdots} - \phantom{\vdots} & \Y & \Y & - & \Y & \Y & \Y \\\hline $\phantom{-}2^{31}-1$ & 2147483647 & \phantom{\vdots}\Y\phantom{\vdots} & \Y & \Y & \Y & \Y & \Y & \Y \\\hline $\phantom{-}$\vdots & \vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots \\ \hline $\phantom{-}2^0$ & 1 & \phantom{\vdots}\Y\phantom{\vdots} & \Y & \Y & \Y & \Y & \Y & \Y \\\hline $\phantom{-}0$ & 0 & \phantom{\vdots}\Y\phantom{\vdots} & \Y & \Y & \Y & \Y & \Y & \Y \\\hline $-2^0$ & -1 & \phantom{\vdots}\Y\phantom{\vdots} & \Y & \Y & \Y & - & \Y & - \\\hline $\phantom{-}$\vdots & \vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \vdots & \cellcolor{Gray2}\vdots & \vdots \\ \hline $-2^{31}+1$ & -2147483647 & \phantom{\vdots}\Y\phantom{\vdots} & \Y & \Y & \Y & - & \Y & - \\\hline $-2^{31}$ & -2147483648 & \cellcolor{Gray2}\phantom{\vdots}NA\phantom{\vdots} & \Y & \Y & \Y & - & \Y & - \\\hline $-2^{31}-1$ & -2147483649 & \phantom{\vdots} - \phantom{\vdots} & \Y & \Y & - & - & \Y & - \\\hline $\phantom{-}$\vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \cellcolor{Gray2}\vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \vdots \\ \hline $-2^{53}+1$ & -9007199254740991 & \phantom{\vdots} - \phantom{\vdots} & \Y & \Y & - & - & \Y & - \\\hline $-2^{53}$ & -9007199254740992 & \phantom{\vdots} - \phantom{\vdots} & - & \Y & - & - & \Y & - \\\hline $\phantom{-}$\vdots & \vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \vdots & \vdots & \cellcolor{Gray2}\vdots & \vdots \\ \hline $-2^{63}+1$ & -9223372036854775807 & \phantom{\vdots} - \phantom{\vdots} & - & \Y & - & - & \Y & - \\\hline $-2^{63}$ & -9223372036854775808 & \phantom{\vdots} - \phantom{\vdots} & - & \cellcolor{Gray2}NA & - & - & \Y & - \\\hline $-2^{63}-1$ & -9223372036854775809 & \phantom{\vdots} - \phantom{\vdots} & - & - & - & - & - & - \\\hline \end{tabular}   From the table it becomes obvious that some integer values in HDF5 files cannot be displayed in R. Note that this can happen for both 64-bit integer as well as for unsigned 32-bit integer. When generating an HDF5 file, it is recommended to use signed 32-bit integers. -->  # Low level HDF5 functions ## Creating an HDF5 file and a group hierarchy  Create a file. ```{r createfile,quiet=FALSE} library(rhdf5) h5file = H5Fcreate(""newfile.h5"") h5file ```  and a group hierarchy ```{r create_groups, quiet=FALSE} h5group1 <- H5Gcreate(h5file, ""foo"") h5group2 <- H5Gcreate(h5file, ""baa"") h5group3 <- H5Gcreate(h5group1, ""foobaa"") h5group3 ```  ## Writing data to an HDF5 file  Create 4 different simple and scalar data spaces. The data space sets the dimensions for the datasets.  ```{r createdataspace, quiet=FALSE} d = c(5,7) h5space1 = H5Screate_simple(d,d) h5space2 = H5Screate_simple(d,NULL) h5space3 = H5Scopy(h5space1) h5space4 = H5Screate(""H5S_SCALAR"") h5space1 H5Sis_simple(h5space1) ```  Create two datasets, one with integer and one with floating point numbers.  ```{r create_dataset} h5dataset1 = H5Dcreate( h5file, ""dataset1"", ""H5T_IEEE_F32LE"", h5space1 ) h5dataset2 = H5Dcreate( h5group2, ""dataset2"", ""H5T_STD_I32LE"", h5space1 ) h5dataset1 ```  Now lets write data to the datasets. ```{r writedata} A = seq(0.1,3.5,length.out=5*7) H5Dwrite(h5dataset1, A) B = 1:35 H5Dwrite(h5dataset2, B) ```  To release resources and to ensure that the data is written on disk, we have to close datasets, dataspaces, and the file. There are different functions to close datasets, dataspaces, groups, and files. ```{r closefile} H5Dclose(h5dataset1) H5Dclose(h5dataset2)  H5Sclose(h5space1) H5Sclose(h5space2) H5Sclose(h5space3) H5Sclose(h5space4)  H5Gclose(h5group1) H5Gclose(h5group2) H5Gclose(h5group3)  H5Fclose(h5file) ```  <!-- % We can now check how it looks like on disk. % ```{r h5dump} % system(""h5dump newfile2.h5"") % ``` % Please note, that arrays appear as transposed matrices when opening it with a C-program (h5dump). This is due to the fact the fastest changing dimension on C is the last one, but on \R~ it is the first one (as in Fortran). -->  # Session Info  ```{r sessioninfo} sessionInfo() ```  ```{r cleanup_after, echo=FALSE, include=FALSE} for(file in c('myhdf5file.h5', 'newfile.h5', 'newfile2.h5', 'newfile3.h5')) {     if(file.exists(file)) {         file.remove( file )     } } ```",20386
"14","Biostrings","SequenceMatching:Alignment:Sequencing:Genetics","%\VignetteIndexEntry{A short presentation of the basic classes defined in Biostrings 2} %\VignetteKeywords{DNA, RNA, Sequence, Biostrings, Sequence alignment}  %\VignettePackage{Biostrings}  % % NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is % likely to be overwritten. % \documentclass[11pt]{article}  %\usepackage[authoryear,round]{natbib} %\usepackage{hyperref}   \textwidth=6.2in \textheight=8.5in %\parskip=.3cm \oddsidemargin=.1in \evensidemargin=.1in \headheight=-.3in  \newcommand{\scscst}{\scriptscriptstyle} \newcommand{\scst}{\scriptstyle}   \newcommand{\Rfunction}[1]{{\texttt{#1}}} \newcommand{\Robject}[1]{{\texttt{#1}}} \newcommand{\Rpackage}[1]{{\textit{#1}}} \newcommand{\Rmethod}[1]{{\texttt{#1}}} \newcommand{\Rfunarg}[1]{{\texttt{#1}}} \newcommand{\Rclass}[1]{{\textit{#1}}}  \textwidth=6.2in  \bibliographystyle{plainnat}    \begin{document} %\setkeys{Gin}{width=0.55\textwidth}  \title{The \Rpackage{Biostrings}~2 classes (work in progress)} \author{Herv\'e Pag\`es} \maketitle  \tableofcontents   % ---------------------------------------------------------------------------  \section{Introduction}  This document briefly presents the new set of classes implemented in the \Rpackage{Biostrings}~2 package. Like the \Rpackage{Biostrings}~1 classes (found in \Rpackage{Biostrings} v~1.4.x), they were designed to make manipulation of big strings (like DNA or RNA sequences) easy and fast. This is achieved by keeping the 3 following ideas from the \Rpackage{Biostrings}~1 package: (1) use R external pointers to store the string data, (2) use bit patterns to encode the string data, (3) provide the user with a convenient class of objects where each instance     can store a set of views {\it on the same} big string (these views being     typically the matches returned by a search algorithm).  However, there is a flaw in the \Rclass{BioString} class design that prevents the search algorithms to return correct information about the matches (i.e. the views) that they found. The new classes address this issue by replacing the \Rclass{BioString} class (implemented in \Rpackage{Biostrings}~1) by 2 new classes: (1) the \Rclass{XString} class used to represent a {\it single} string, and (2) the \Rclass{XStringViews} class used to represent a set of views     {\it on the same} \Rclass{XString} object, and by introducing new     implementations and new interfaces for these 2 classes.   % ---------------------------------------------------------------------------  \section{The \Rclass{XString} class and its subsetting operator~\Rmethod{[}}  The \Rclass{XString} is in fact a virtual class and therefore cannot be instanciated. Only subclasses (or subtypes) \Rclass{BString}, \Rclass{DNAString}, \Rclass{RNAString} and \Rclass{AAString} can. These classes are direct extensions of the \Rclass{XString} class (no additional slot).  A first \Rclass{BString} object: <<a1>>= library(Biostrings) b <- BString(""I am a BString object"") b length(b) @  A \Rclass{DNAString} object: <<a2>>= d <- DNAString(""TTGAAAA-CTC-N"") d length(d) @ The differences with a \Rclass{BString} object are: (1) only letters from the {\it IUPAC extended genetic alphabet} + the gap letter ({\tt -}) are allowed and (2) each letter in the argument passed to the \Rfunction{DNAString} function is encoded in a special way before it's stored in the \Rclass{DNAString} object.  Access to the individual letters: <<a3>>= d[3] d[7:12] d[] b[length(b):1] @ Only {\it in bounds} positive numeric subscripts are supported.  In fact the subsetting operator for \Rclass{XString} objects is not efficient and one should always use the \Rmethod{subseq} method to extract a substring from a big string: <<a4>>= bb <- subseq(b, 3, 6) dd1 <- subseq(d, end=7) dd2 <- subseq(d, start=8) @  To {\it dump} an \Rclass{XString} object as a character vector (of length 1), use the \Rmethod{toString} method: <<a5>>= toString(dd2) @  Note that \Robject{length(dd2)} is equivalent to \Robject{nchar(toString(dd2))} but the latter would be very inefficient on a big \Rclass{DNAString} object.  {\it [TODO: Make a generic of the substr() function to work with XString objects. It will be essentially doing toString(subseq()).]}   % ---------------------------------------------------------------------------  \section{The \Rmethod{==} binary operator for \Rclass{XString} objects}  The 2 following comparisons are \Robject{TRUE}: <<b1,results=hide>>= bb == ""am a"" dd2 != DNAString(""TG"") @  When the 2 sides of \Rmethod{==} don't belong to the same class then the side belonging to the ``lowest'' class is first converted to an object belonging to the class of the other side (the ``highest'' class). The class (pseudo-)order is \Rclass{character} < \Rclass{BString} < \Rclass{DNAString}. When both sides are \Rclass{XString} objects of the same subtype (e.g. both are \Rclass{DNAString} objects) then the comparison is very fast because it only has to call the C standard function {\tt memcmp()} and no memory allocation or string encoding/decoding is required.  The 2 following expressions provoke an error because the right member can't be ``upgraded'' (converted) to an object of the same class than the left member: <<b2,echo=FALSE>>= cat('> bb == """"') cat('> d == bb') @  When comparing an \Rclass{RNAString} object with a \Rclass{DNAString} object, U and T are considered equals: <<b3>>= r <- RNAString(d) r r == d @   % ---------------------------------------------------------------------------  \section{The \Rclass{XStringViews} class and its subsetting operators~\Rmethod{[} and~\Rmethod{[[}}  An \Rclass{XStringViews} object contains a set of views {\it on the same} \Rclass{XString} object called the {\it subject} string. Here is an \Rclass{XStringViews} object with 4 views: <<c1>>= v4 <- Views(dd2, start=3:0, end=5:8) v4 length(v4) @  Note that the 2 last views are {\it out of limits}.  You can select a subset of views from an \Rclass{XStringViews} object: <<c3>>= v4[4:2] @  The returned object is still an \Rclass{XStringViews} object, even if we select only one element. You need to use double-brackets to extract a given view as an \Rclass{XString} object: <<c4>>= v4[[2]] @  You can't extract a view that is {\it out of limits}: <<c6,echo=FALSE>>= cat('> v4[[3]]') cat(try(v4[[3]], silent=TRUE)) @  Note that, when \Robject{start} and \Robject{end} are numeric vectors and \Robject{i} is a {\it single} integer, \Robject{Views(b, start, end)[[i]]} is equivalent to \Robject{subseq(b, start[i], end[i])}.  Subsetting also works with negative or logical values with the expected semantic (the same as for R built-in vectors): <<c7>>= v4[-3] v4[c(TRUE, FALSE)] @ Note that the logical vector is recycled to the length of \Robject{v4}.   % ---------------------------------------------------------------------------  \section{A few more \Rclass{XStringViews} objects}  12 views (all of the same width): <<d1>>= v12 <- Views(DNAString(""TAATAATG""), start=-2:9, end=0:11) @  This is the same as doing \Robject{Views(d, start=1, end=length(d))}: <<d2,results=hide>>= as(d, ""Views"") @  Hence the following will always return the \Robject{d} object itself: <<d3,results=hide>>= as(d, ""Views"")[[1]] @  3 \Rclass{XStringViews} objects with no view: <<d4,results=hide>>= v12[0] v12[FALSE] Views(d) @   % ---------------------------------------------------------------------------  \section{The \Rmethod{==} binary operator for \Rclass{XStringViews} objects}  This operator is the vectorized version of the \Rmethod{==} operator defined previously for \Rclass{XString} objects: <<e1>>= v12 == DNAString(""TAA"") @  To display all the views in \Robject{v12} that are equals to a given view, you can type R cuties like: <<e2>>= v12[v12 == v12[4]] v12[v12 == v12[1]] @  This is \Robject{TRUE}: <<e2,results=hide>>= v12[3] == Views(RNAString(""AU""), start=0, end=2) @   % ---------------------------------------------------------------------------  \section{The \Rmethod{start}, \Rmethod{end} and \Rmethod{width} methods}  <<f1>>= start(v4) end(v4) width(v4) @  Note that \Robject{start(v4)[i]} is equivalent to \Robject{start(v4[i])}, except that the former will not issue an error if \Robject{i} is out of bounds (same for \Rmethod{end} and \Rmethod{width} methods).  Also, when \Robject{i} is a {\it single} integer, \Robject{width(v4)[i]} is equivalent to \Robject{length(v4[[i]])} except that the former will not issue an error if \Robject{i} is out of bounds or if view \Robject{v4[i]} is {\it out of limits}.   \end{document}",8513
"15","Biostrings","SequenceMatching:Alignment:Sequencing:Genetics","%\VignetteIndexEntry{Biostrings Quick Overview} %\VignetteKeywords{DNA, RNA, Sequence, Biostrings, Sequence alignment}  %\VignettePackage{Biostrings}  % % NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is % likely to be overwritten. % \documentclass[10pt]{article}  \usepackage{times} \usepackage{hyperref}  \usepackage[margin=0.65in]{geometry}  \newcommand{\scscst}{\scriptscriptstyle} \newcommand{\scst}{\scriptstyle}  \newcommand{\R}{{\textsf{R}}} \newcommand{\code}[1]{{\texttt{#1}}} \newcommand{\term}[1]{{\emph{#1}}} \newcommand{\Rpackage}[1]{\textsf{#1}} \newcommand{\Rfunction}[1]{\texttt{#1}} \newcommand{\Robject}[1]{\texttt{#1}} \newcommand{\Rclass}[1]{{\textit{#1}}} \newcommand{\Rmethod}[1]{{\textit{#1}}} \newcommand{\Rfunarg}[1]{{\textit{#1}}}  \bibliographystyle{plainnat}    \begin{document} %\setkeys{Gin}{width=0.55\textwidth}  \title{Biostrings Quick Overview} \author{Herv\'e Pag\`es \\   Fred Hutchinson Cancer Research Center \\   Seattle, WA} \date{\today} \maketitle  %\tableofcontents  Most but not all functions defined in the \Rpackage{Biostrings} package are summarized here.  %-----------------------------------------------------------------------------  \begin{table}[ht] \begin{center} \begin{tabular}{p{2.5in}|p{4in}} {\bf Function} & {\bf Description} \\ \hline \Rfunction{length} & Return the number of sequences in an object. \\ \hline \Rfunction{names} & Return the names of the sequences in an object. \\ \hline \Rfunction{[} & Extract sequences from an object. \\ \hline \Rfunction{head}, \Rfunction{tail} & Extract the first or last sequences     from an object. \\ \hline \Rfunction{rev} & Reverse the order of the sequences in an object. \\ \hline \Rfunction{c} & Combine in a single object the sequences from 2 or more objects. \\ \hline \Rfunction{width}, \Rfunction{nchar} & Return the sizes (i.e. number of     letters) of all the sequences in an object.\\ \hline \Rfunction{==}, \Rfunction{!=} & Element-wise comparison of the sequences     in 2 objects. \\ \hline \Rfunction{match}, \Rfunction{\%in\%} &     Analog to \Rfunction{match} and \Rfunction{\%in\%} on character vectors. \\ \hline \Rfunction{duplicated}, \Rfunction{unique} &     Analog to \Rfunction{duplicated} and \Rfunction{unique} on character     vectors. \\ \hline \Rfunction{sort}, \Rfunction{order} &     Analog to \Rfunction{sort} and \Rfunction{order} on character vectors,     except that the ordering of DNA or Amino Acid sequences doesn't     depend on the locale. \\ \hline \Rfunction{relist}, \Rfunction{split}, \Rfunction{extractList} &     Analog to \Rfunction{relist} and \Rfunction{split} on character vectors,     except that the result is a \Rclass{DNAStringSetList} or     \Rclass{AAStringSetList} object.     \Rfunction{extractList} is a generalization of \Rfunction{relist} and     \Rfunction{split} that supports \emph{arbitrary} groupings. \\ \hline \end{tabular} \end{center} \caption{\bf Low-level manipulation of \Rclass{DNAStringSet} and          \Rclass{AAStringSet} objects.} \label{table:Low_level_manipulation} \end{table}  %-----------------------------------------------------------------------------  \begin{table}[ht] \begin{center} \begin{tabular}{p{2.5in}|p{4in}} {\bf Function} & {\bf Description} \\ \hline \Rfunction{alphabetFrequency}\par \Rfunction{letterFrequency} &     Tabulate the letters (all the letters in the alphabet for     \Rfunction{alphabetFrequency}, only the specified letters for     \Rfunction{letterFrequency}) in a sequence or set of sequences. \\ \hline \Rfunction{uniqueLetters} &     Extract the unique letters from a sequence or set of sequences. \\ \hline \Rfunction{letterFrequencyInSlidingView} &     Specialized version of \Rfunction{letterFrequency} that tallies the     requested letter frequencies for a fixed-width view that is conceptually     slid along the input sequence. \\ \hline \Rfunction{consensusMatrix} &     Computes the consensus matrix of a set of sequences. \\ \hline \Rfunction{dinucleotideFrequency}\par \Rfunction{trinucleotideFrequency}\par \Rfunction{oligonucleotideFrequency} &     Fast 2-mer, 3-mer, and k-mer counting for DNA or RNA. \\ \hline \Rfunction{nucleotideFrequencyAt} &     Tallies the short sequences formed by extracting the nucleotides found     at a set of fixed positions from each sequence of a set of DNA or RNA     sequences. \\ \hline \end{tabular} \end{center} \caption{\bf Counting / tabulating.} \label{table:Counting_tabulating} \end{table}  %-----------------------------------------------------------------------------  \begin{table}[ht] \begin{center} \begin{tabular}{p{2.5in}|p{4in}} {\bf Function} & {\bf Description} \\ \hline \Rfunction{reverse}\par \Rfunction{complement}\par \Rfunction{reverseComplement} &     Compute the reverse, complement, or reverse-complement, of a set of     DNA sequences. \\ \hline \Rfunction{translate} &     Translate a set of DNA sequences into a set of Amino Acid sequences. \\ \hline \Rfunction{chartr}\par \Rfunction{replaceAmbiguities} &     Replace letters in a sequence or set of sequences. \\ \hline \Rfunction{subseq}, \Rfunction{subseq<-}\par \Rfunction{extractAt}, \Rfunction{replaceAt} &     Extract/replace arbitrary substrings from/in a string or set of strings. \\ \hline \Rfunction{replaceLetterAt} & Replace the letters specified by a set of     positions by new letters. \\ \hline \Rfunction{padAndClip}, \Rfunction{stackStrings} & Pad and clip strings. \\ \hline \Rfunction{strsplit}, \Rfunction{unstrsplit} & \Rfunction{strsplit} splits the     sequences in a set of sequences according to a pattern.     \Rfunction{unstrsplit} is the reverse operation i.e. a fast implementation     of \code{sapply(x, paste0, collapse=sep)} for collapsing the list elements     of a \Rclass{DNAStringSetList} or \Rclass{AAStringSetList} object. \\ \hline \end{tabular} \end{center} \caption{\bf Sequence transformation and editing.} \label{table:Sequence_editing} \end{table}  %-----------------------------------------------------------------------------  \begin{table}[ht] \begin{center} \begin{tabular}{p{2.5in}|p{4in}} {\bf Function} & {\bf Description} \\ \hline \Rfunction{matchPattern}\par \Rfunction{countPattern} &     Find/count all the occurrences of a given pattern (typically short)     in a reference sequence (typically long).     Support mismatches and indels. \\ \hline \Rfunction{vmatchPattern}\par \Rfunction{vcountPattern} &     Find/count all the occurrences of a given pattern (typically short)     in a set of reference sequences.     Support mismatches and indels. \\ \hline \Rfunction{matchPDict}\par \Rfunction{countPDict}\par \Rfunction{whichPDict} &     Find/count all the occurrences of a set of patterns in a reference     sequence. (\Rfunction{whichPDict} only identifies which patterns in     the set have at least one match.)     Support a small number of mismatches. \\ \hline \Rfunction{vmatchPDict}\par \Rfunction{vcountPDict}\par \Rfunction{vwhichPDict} &     [Note: \Rfunction{vmatchPDict} not implemented yet.]     Find/count all the occurrences of a set of patterns in a set of     reference sequences. (\Rfunction{whichPDict} only identifies for each     reference sequence which patterns in the set have at least one match.)     Support a small number of mismatches. \\ \hline \Rfunction{pairwiseAlignment} &     Solve (Needleman-Wunsch) global alignment, (Smith-Waterman) local     alignment, and (ends-free) overlap alignment problems. \\ \hline \Rfunction{matchPWM}\par \Rfunction{countPWM} &     Find/count all the occurrences of a Position Weight Matrix in a reference     sequence. \\ \hline \Rfunction{trimLRPatterns} &     Trim left and/or right flanking patterns from sequences. \\ \hline \Rfunction{matchLRPatterns} &     Find all paired matches in a reference sequence i.e. matches specified by     a left and a right pattern, and a maximum distance between them. \\ \hline \Rfunction{matchProbePair} &     Find all the amplicons that match a pair of probes in a reference     sequence. \\ \hline \Rfunction{findPalindromes} &     Find palindromic regions in a sequence. \\ \hline \end{tabular} \end{center} \caption{\bf String matching / alignments.} \label{table:String_matching_alignments} \end{table}  %-----------------------------------------------------------------------------  \begin{table}[ht] \begin{center} \begin{tabular}{p{2.5in}|p{4in}} {\bf Function} & {\bf Description} \\ \hline \Rfunction{readBStringSet}\par \Rfunction{readDNAStringSet}\par \Rfunction{readRNAStringSet}\par \Rfunction{readAAStringSet} &     Read ordinary/DNA/RNA/Amino Acid sequences from files (FASTA or FASTQ     format). \\ \hline \Rfunction{writeXStringSet} &     Write sequences to a file (FASTA or FASTQ format). \\ \hline \Rfunction{writePairwiseAlignments} &     Write pairwise alignments (as produced by \Rfunction{pairwiseAlignment})     to a file (``pair'' format). \\ \hline \Rfunction{readDNAMultipleAlignment}\par \Rfunction{readRNAMultipleAlignment}\par \Rfunction{readAAMultipleAlignment}&     Read multiple alignments from a file (FASTA, ``stockholm'',     or ``clustal'' format). \\ \hline \Rfunction{write.phylip} &     Write multiple alignments to a file (Phylip format). \\ \hline \end{tabular} \end{center} \caption{\bf I/O functions.} \label{table:I_O_functions} \end{table}  \end{document} ",9331
"16","Biostrings","SequenceMatching:Alignment:Sequencing:Genetics","--- title: ""Using oligonucleotide microarray reporter sequence information   for preprocessing and quality assessment"" author: - name: Wolfgang Huber - name: Robert Gentleman date: ""`r format(Sys.time(), '%d %B, %Y')`"" package: Biostrings vignette: >   %\VignetteIndexEntry{Handling probe sequence information}   %\VignetteEngine{knitr::rmarkdown}   %\VignetteEncoding{UTF-8}   %\VignetteDepends{Biostrings, hgu95av2probe, hgu95av2cdf, affy, affydata}   %\VignetteKeywords{Expression Analysis}   %\VignettePackage{Biostrings} output:   BiocStyle::html_document fig_caption: true ---  ```{r setup,include=FALSE} library(BiocStyle) ```  # Overview  This document presents some basic and simple tools for dealing with the oligonucleotide microarray reporter sequence information in the Bioconductor *probe* packages. This information is used, for example, in the `r Biocpkg(""gcrma"")` package.  *Probe* packages are a convenient way for distributing and storing the probe sequences (and related information) of a given chip.  As an example, the package `r Biocpkg(""hgu95av2probe"")` provides microarray reporter sequences for Affymetrix' *HgU95a version 2* genechip, and for almost all major Affymetrix genechips, the corresponding packages can be downloaded from the Bioconductor website. If you have the reporter sequence information of a particular chip, you can also create such a package yourself. This is described in the makeProbePackage vignette of the `r Biocpkg(""AnnotationForge"")` package.  This document assumes some basic familiarity with R and with the design of the *AffyBatch* class in the `r Biocpkg(""affy"")` package, Bioconductor's basic container for Affymetrix genechip data.  First, let us load the `r Biocpkg(""Biostrings"")` package and some other packages we will use.  ```{r loadPackages,message=FALSE,warning=FALSE} library(Biostrings)  library(hgu95av2probe)  library(hgu95av2cdf)  ```  # Using probe packages  Help for the probe sequence data packages can be accessed through  ```{r hgu95av2probe,eval=FALSE} ?hgu95av2probe ```  One of the issues that you have to deal with is that the *probe* packages do not provide the reporter sequences of all the features present in an *AffyBatch*. Some sequences are missing, some are implied; in particular, the data structure in the *probe* packages does not explicitly contain the sequences of the mismatch probes, since they are implied by the perfect match probes. Also, some other features, typically harboring control probes or empty, do not have sequences. This is the choice that Affymetrix made when they made files with probe sequences available, and we followed it.  Practically, this means that the vector of probe sequences in a *probe* package does not align 1:1 with the rows of the corresponding *AffyBatch*; you need to keep track of this mapping, and some tools for this are provided and explained below (see Section [2.2](#subsec.relating)). It also means that some functions from the `r Biocpkg(""affy"")` package, such as `pm`, cannot be used when the sequences of the probes corresponding to their result are needed; since `pm` reports the intensities, but not the identity of the probes it has selected, yet the latter would be needed to retrieve their sequences.  ## Basic functions  Let us look at some basic operations on the sequences.  ### Reverse and complementary sequence  DNA sequences can be reversed and/or complemented with the `reverse`, `complement` and `reverseComplement` functions.  ```{r reverseComplement,eval=FALSE} ?reverseComplement ```  ### Matching sets of probes against each other  ```{r MatchingSetsofProbesAgasintEachOther} pm <- DNAStringSet(hgu95av2probe)  dict <- pm[3801:4000] pdict <- PDict(dict) m <- vcountPDict(pdict, pm) dim(m)  table(rowSums(m)) which(rowSums(m) == 3) ii <- which(m[77, ] != 0) pm[ii] ```  ### Base content  The base content (number of occurrence of each character) of the sequences can be computed with the function `alphabetFrequency`.  ```{r alphabetFrequency} bcpm <- alphabetFrequency(pm, baseOnly=TRUE) head(bcpm)  alphabetFrequency(pm, baseOnly=TRUE, collapse=TRUE) ```  ## Relating to the features of an *AffyBatch* {#subsec.relating}  ```{r hgu95av2dimncol} nc = hgu95av2dim$NCOL nc ```  ```{r hgu95av2dimnrow} nr = hgu95av2dim$NROW nr ```  Each column of an *AffyBatch* corresponds to an array, each row to a certain probe on the arrays. The probes are stored in a way that is related to their geometrical position on the array. For example, the *hgu95av2* array is geometrically arranged into `r nc` columns and `r nr` rows. We call the column and row indices the `x`- and `y`-coordinates, respectively. This results in `r nc` ⨉ `r nr` = `r format(nc*nr, scientific=FALSE)` probes of the *AffyBatch*; we also call them indices. To convert between `x`-and `y`-coordinates and indices, you can use the functions `xy2indices` and `indices2xy` from the `r Biocpkg(""affy"")` package.  The sequence data in the *probe* packages is addressed by their `x` and `y`-coordinates. Let us construct a vector `abseq` that aligns with the indices of an *hgu95av2* *AffyBatch* and fill in the sequences:  ```{r abseq} library(affy)  abseq = rep(as.character(NA), nc*nr)  ipm = with(hgu95av2probe, xy2indices(x, y, nc=nc))  any(duplicated(ipm)) # just a sanity check  abseq[ipm] = hgu95av2probe$sequence table(is.na(abseq)) ```  The mismatch sequences are not explicitly stored in the probe packages. They are implied by the match sequences, by flipping the middle base. This can be done with the `pm2mm` function defined below. For Affymetrix GeneChips the length of the probe sequences is 25, and since we start counting at 1, the middle position is 13.  ```{r pm2mm} mm <- pm subseq(mm, start=13, width=1) <- complement(subseq(mm, start=13, width=1)) cat(as.character(pm[[1]]), as.character(mm[[1]]), sep=""\n"") ```  We compute `imm`, the indices of the mismatch probes, by noting that each mismatch has the same `x`-coordinate as its associated perfect match, while its `y`-coordinate is increased by 1.  ```{r imm} imm = with(hgu95av2probe, xy2indices(x, y+1, nc=nc)) intersect(ipm, imm) # just a sanity check abseq[imm] = as.character(mm) table(is.na(abseq)) ```  See Figures \@ref(fig:bap)-\@ref(fig:p2p) for some applications of the probe sequence information to preprocessing and data quality related plots.  # Some sequence related ""preprocessing and quality"" plots  The function `alphabetFrequency` counts the number of occurrences of each of the four bases A, C, G, T in each probe sequence.  ```{r alphabetFrequency2} freqs <- alphabetFrequency(DNAStringSet(abseq[!is.na(abseq)]), baseOnly=TRUE) bc <- matrix(nrow=length(abseq), ncol=5) colnames(bc) <- colnames(freqs) bc[!is.na(abseq), ] <- freqs head(na.omit(bc)) ```  Let us define an ordered factor variable for GC content:  ```{r gc} GC = ordered(bc[,""G""] + bc[,""C""]) colores = rainbow(nlevels(GC)) ```  And let us create an *AffyBatch* object.  ```{r bap, fig.cap=""Distribution of probe GC content. The height of each bar corresponds to the number of probes with the corresponding GC content.""} library(affydata) f <- system.file(""extracelfiles"", ""CL2001032020AA.cel"", package=""affydata"") pd <- new(""AnnotatedDataFrame"", data=data.frame(fromFile=I(f), row.names=""f"")) abatch <- read.affybatch(filenames=f, compress=TRUE, phenoData=pd) barplot(table(GC), col=colores, xlab=""GC"", ylab=""number"") ```  ```{r bxp, fig.cap=""Boxplots of log~2~ intensity stratifed by probe GC content.""} boxplot(log2(exprs(abatch)[,1]) ~ GC, outline=FALSE,         col=colores, , xlab=""GC"", ylab=expression(log[2]~intensity)) ```  ```{r p2p, fig.cap=""Scatterplot of PM vs MM intensities, colored by probe GC content.""} plot(exprs(abatch)[ipm,1], exprs(abatch)[imm,1], asp=1, pch=""."", log=""xy"",      xlab=""PM"", ylab=""MM"", col=colores[GC[ipm]]) abline(a=0, b=1, col=""#404040"", lty=3) ```  ```{r devoff,include=FALSE} dev.off() ```",7903
"17","Biostrings","SequenceMatching:Alignment:Sequencing:Genetics","--- title: ""MultipleAlignment Objects"" author:  - name: ""Marc Carlson""   affiliation: ""Bioconductor Core Team, Fred Hutchinson Cancer Research Center, Seattle, WA"" - name: ""Beryl Kanali""   affiliation: ""Vignette conversion from Sweave to Rmarkdown"" date: ""Edited: November 1, 2022; Compiled: `r format(Sys.time(), '%d %B, %Y')`""  vignette: >   %\VignetteIndexEntry{Multiple Alignments}   %\VignetteKeywords{DNA, RNA, Sequence, Biostrings, Sequence alignment}   %\VignettePackage{Biostrings}   %\VignetteEncoding{UTF-8}   %\VignetteEngine{knitr::rmarkdown} output:   BiocStyle::html_document:         number_sections: true         toc: true         toc_depth: 4 editor_options:    markdown:      wrap: 72 ---  # Introduction  The `DNAMultipleAlignment`, `RNAMultipleAlignment` and `AAMultipleAlignment` classes allow users to represent groups of aligned DNA, RNA or amino acid sequences as a single object.  The frame of reference for aligned sequences is static, so manipulation of these objects is confined to be non-destructive.  In practice, this means that these objects contain slots to mask ranges of rows and columns on the original sequence.  These masks are then respected by methods that manipulate and display the objects, allowing the user to remove or expose columns and rows without invalidating the original alignment.  # Creation and masking  To create a `MultipleAlignment`, call the appropriate read function to read in and parse the original alignment.  There are functions to read clustaW, Phylip and Stolkholm data formats.  ```{r objectCreation, message=FALSE} library(Biostrings) origMAlign <-   readDNAMultipleAlignment(filepath =                            system.file(""extdata"",                                        ""msx2_mRNA.aln"",                                        package=""Biostrings""),                            format=""clustal"")  phylipMAlign <-   readAAMultipleAlignment(filepath =                           system.file(""extdata"",                                       ""Phylip.txt"",                                       package=""Biostrings""),                           format=""phylip"") ```  Rows can be renamed with `rownames`.  ```{r renameRows} rownames(origMAlign) rownames(origMAlign) <- c(""Human"",""Chimp"",""Cow"",""Mouse"",""Rat"",                           ""Dog"",""Chicken"",""Salmon"") origMAlign ```  To see a more detailed version of your `MultipleAlignment` object, you can use the `detail` method, which will show the details of the alignment interleaved and without the rows and columns that you have masked out.  ```{r detail, eval=FALSE} detail(origMAlign) ```  Applying masks is a simple matter of specifying which ranges to hide.  ```{r usingMasks} maskTest <- origMAlign rowmask(maskTest) <- IRanges(start=1,end=3) rowmask(maskTest) maskTest  colmask(maskTest) <- IRanges(start=c(1,1000),end=c(500,2343)) colmask(maskTest) maskTest ```  Remove row and column masks by assigning `NULL`:  ```{r nullOut masks} rowmask(maskTest) <- NULL rowmask(maskTest) colmask(maskTest) <- NULL colmask(maskTest) maskTest ```   When setting a mask, you might want to specify the rows or columns to keep, rather than to hide.  To do that, use the `invert` argument.  Taking the above example, we can set the exact same masks as before by specifying their inverse and using `invert=TRUE`.  ```{r invertMask} rowmask(maskTest, invert=TRUE) <- IRanges(start=4,end=8) rowmask(maskTest) maskTest colmask(maskTest, invert=TRUE) <- IRanges(start=501,end=999) colmask(maskTest) maskTest ```  In addition to being able to invert these masks, you can also choose the way in which the ranges you provide will be merged with any existing masks. The `append` argument allows you to specify the way in which new mask ranges will interact with any existing masks.  By default, these masks will be the ""union"" of the new mask and any existing masks, but you can also specify that these masks be the mask that results from when you ""intersect"" the current mask and the new mask, or that the new mask simply ""replace"" the current mask. The `append` argument can be used in combination with the `invert` argument to make things even more interesting.  In this case, the inversion of the mask will happen before it is combined with the existing mask.  For simplicity, I will only demonstrate this on `rowmask`, but it also works for `colmask`.  Before we begin, lets set the masks back to being NULL again.  ```{r setup} ## 1st lets null out the masks so we can have a fresh start. colmask(maskTest) <- NULL rowmask(maskTest) <- NULL ```  Then we can do a series of examples, starting with the default which uses the ""union"" value for the `append` argument.  ```{r appendMask} ## Then we can demonstrate how the append argument works rowmask(maskTest) <- IRanges(start=1,end=3) maskTest  rowmask(maskTest,append=""intersect"") <- IRanges(start=2,end=5) maskTest  rowmask(maskTest,append=""replace"") <- IRanges(start=5,end=8) maskTest  rowmask(maskTest,append=""replace"",invert=TRUE) <- IRanges(start=5,end=8) maskTest  rowmask(maskTest,append=""union"") <- IRanges(start=7,end=8) maskTest ```  The function `maskMotif` works on `MultipleAlignment` objects too, and takes the same arguments that it does elsewhere. `maskMotif` is useful for masking occurances of a string from columns where it is present in the consensus sequence.  ```{r maskMotif} tataMasked <- maskMotif(origMAlign, ""TATA"") colmask(tataMasked) ```  `maskGaps` also operates on columns and will mask collumns based on the fraction of each column that contains gaps `min.fraction` along with the width of columns that contain this fraction of gaps `min.block.width`.   ```{r maskGaps} autoMasked <- maskGaps(origMAlign, min.fraction=0.5, min.block.width=4) autoMasked ```   Sometimes you may want to cast your `MultipleAlignment` to be a matrix for usage eslewhere.  `as.matrix` is supported for these circumstances.  The ability to convert one object into another is not very unusual so why mention it?  Because when you cast your object, the masks WILL be considered so that the masked rows and columns will be left out of the matrix object.  ```{r asmatrix} full = as.matrix(origMAlign) dim(full) partial = as.matrix(autoMasked) dim(partial) ```  One example of where you might want to use `as.matrix` is when using the `r CRANpkg(""ape"")` package. For example if you needed to use the `dist.dna` function you would want to use `as.matrix` followed by `as.alignment` and then the `as.DNAbin` to create a `DNAbin` object for the `dist.dna`.  # Analytic utilities  Once you have masked the sequence, you can then ask questions about the properties of that sequence.  For example, you can look at the alphabet frequency of that sequence.  The alphabet frequency will only be for the masked sequence.  ```{r alphabetFreq} alphabetFrequency(autoMasked) ```  You can also calculate a consensus matrix, extract the consensus string or look at the consensus views.  These methods too will all consider the masking when you run them.    ```{r consensus} consensusMatrix(autoMasked, baseOnly=TRUE)[, 84:90] substr(consensusString(autoMasked),80,130) consensusViews(autoMasked) ```  You can also cluster the alignments based on their distance to each other.  Because you must pass in a DNAStringSet, the clustering will also take into account the masking. So for example, you can see how clustering the unmasked `DNAMultipleAlignment` will draw a funky looking tree.    ```{r cluster} sdist <- stringDist(as(origMAlign,""DNAStringSet""), method=""hamming"") clust <- hclust(sdist, method = ""single"") png(file=""badTree.png"") plot(clust) dev.off() ``` ```{r figure, echo=FALSE, fig=TRUE, eps=FALSE, fig.align = 'center', fig.cap='Funky tree produced by using unmasked strings'} knitr::include_graphics(""badTree.png"") ``` But, if we use the gap-masked `DNAMultipleAlignment`, to remove the long uninformative regions, and then make our plot, we can see the real relationships.  ```{r cluster2} sdist <- stringDist(as(autoMasked,""DNAStringSet""), method=""hamming"") clust <- hclust(sdist, method = ""single"") png(file=""goodTree.png"") plot(clust) dev.off() fourgroups <- cutree(clust, 4) fourgroups ``` ```{r figure1, echo=FALSE, fig=TRUE, eps=FALSE, width=0.6,fig.align = 'center', height=5, fig.cap='A tree produced by using strings with masked gaps'} knitr::include_graphics(""goodTree.png"") ```  In the ""good"" plot, the Salmon sequence is once again the most distant which is what we expect to see.  A closer examination of the sequence reveals that the similarity between the mouse, rat and human sequences was being inflated by virtue of the fact that those sequences were simply much longer (had more information than) the other species represented.  This is what caused the ""funky"" result. The relationship between the sequences in the funky tree was being driven by extra ""length"" in the rodent/mouse/human sequences, instead of by the similarity of the conserved regions.   # Exporting to file  One possible export option is to write to fasta files If you need to write your `MultipleAlignment` object out as a fasta file, you can cast it to a `DNAStringSet` and then write it out as a fasta file like so:  ```{r fastaExample, eval=FALSE} DNAStr = as(origMAlign, ""DNAStringSet"") writeXStringSet(DNAStr, file=""myFile.fa"") ```  One other format that is of interest is the Phylip format.  The Phylip format stores the column masking of your object as well as the sequence that you are exporting. So if you have masked the sequence and you write out a Phylip file, this mask will be recorded into the file you export.  As with the fasta example above, any rows that you have masked out will be removed from the exported file.  ```{r write.phylip, eval=FALSE} write.phylip(phylipMAlign, filepath=""myFile.txt"") ```   # Session Information  All of the output in this vignette was produced under the following conditions:  ```{r sessionInfo, echo=FALSE} sessionInfo() ```",9922
"18","Biostrings","SequenceMatching:Alignment:Sequencing:Genetics","%\VignetteIndexEntry{Pairwise Sequence Alignments} %\VignetteKeywords{DNA, RNA, Sequence, Biostrings, Sequence alignment}  %\VignettePackage{Biostrings}  % % NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is % likely to be overwritten. % \documentclass[10pt]{article}  \usepackage{times} \usepackage{hyperref}  \textwidth=6.5in \textheight=8.5in %\parskip=.3cm \oddsidemargin=-.1in \evensidemargin=-.1in \headheight=-.3in  \newcommand{\scscst}{\scriptscriptstyle} \newcommand{\scst}{\scriptstyle}  \newcommand{\R}{{\textsf{R}}} \newcommand{\code}[1]{{\texttt{#1}}} \newcommand{\term}[1]{{\emph{#1}}} \newcommand{\Rpackage}[1]{\textsf{#1}} \newcommand{\Rfunction}[1]{\texttt{#1}} \newcommand{\Robject}[1]{\texttt{#1}} \newcommand{\Rclass}[1]{{\textit{#1}}} \newcommand{\Rmethod}[1]{{\textit{#1}}} \newcommand{\Rfunarg}[1]{{\textit{#1}}}  \bibliographystyle{plainnat}    \begin{document} %\setkeys{Gin}{width=0.55\textwidth}  \title{Pairwise Sequence Alignments} \author{Patrick Aboyoun \\   Gentleman Lab \\   Fred Hutchinson Cancer Research Center \\   Seattle, WA} \date{\today} \maketitle   This vignette and all the tools described in it have moved to the \Rpackage{pwalign} package.  \end{document}",1205
"19","Biostrings","SequenceMatching:Alignment:Sequencing:Genetics","--- title: ""Biostrings Package Report, by pkgnet"" output:      rmarkdown::html_vignette:         toc: true vignette: >     %\VignetteIndexEntry{Package Report, by pkgnet}     %\VignetteEngine{knitr::rmarkdown}     %\VignetteEncoding{UTF-8} ---  This report on **Biostrings** is generated by [**pkgnet**](https://uptakeopensource.github.io/pkgnet/), an R package for analyzing other R packages through the lens of [network theory](https://en.wikipedia.org/wiki/Network_theory).   ```{r setup, include=FALSE} library(pkgnet)  knitr::opts_chunk$set(     echo = FALSE     , warning=FALSE     , out.width='100%' ) pkgnet:::silence_logger()  reporters <- list(DependencyReporter$new(), FunctionReporter$new())  reporters <- lapply(           X = reporters           , FUN = function(reporter){               reporter$set_package(pkg_name = ""Biostrings"")               return(reporter)           }       )  ```  ```{r warning=FALSE} reportSections <- lapply(reporters, function(reporter) {   report_env <- list2env(list(reporter = reporter))   knitr::knit_child(     reporter$report_markdown_path     , envir = report_env   ) })  ```  ```{r results=""asis""} cat(paste0(paste(reportSections, collapse = '\n'))) ```  ---  ```{r results=""asis""} cat(sprintf(""<sup>This report built with **pkgnet v%s**.</sup>"", packageVersion('pkgnet'))) ```  <sup>[**pkgnet**](https://uptakeopensource.github.io/pkgnet/) is an open-source R package, copyright &copy; 2017-2019 Uptake, made available under the 3-Clause BSD License.</sup>  ```{r echo = FALSE} pkgnet:::unsilence_logger() ```",1562
"20","affy","Microarray:OneChannel:Preprocessing","% -*- mode: noweb; noweb-default-code-mode: R-mode; -*- %\VignetteIndexEntry{1. Primer} %\VignetteKeywords{Preprocessing, Affymetrix} %\VignetteDepends{affy} %\VignettePackage{affy} %documentclass[12pt, a4paper]{article} \documentclass[12pt]{article}  \usepackage{amsmath} \usepackage{hyperref} \usepackage[authoryear,round]{natbib}  \textwidth=6.2in \textheight=8.5in %\parskip=.3cm \oddsidemargin=.1in \evensidemargin=.1in \headheight=-.3in  \newcommand{\scscst}{\scriptscriptstyle} \newcommand{\scst}{\scriptstyle} \newcommand{\Rfunction}[1]{{\texttt{#1}}} \newcommand{\Robject}[1]{{\texttt{#1}}} \newcommand{\Rpackage}[1]{{\textit{#1}}}  \author{Laurent Gautier, Rafael Irizarry, Leslie Cope, and Ben Bolstad} \begin{document} \title{Description of affy}  \maketitle \tableofcontents \section{Introduction} The \Rpackage{affy} package is part of the BioConductor\footnote{\url{http://bioconductor.org/}} project. It is meant to be an extensible, interactive environment for data analysis and exploration of Affymetrix oligonucleotide array probe level data.  The software utilities provided with the Affymetrix software suite summarizes the probe set intensities to form one {\it expression measure} for each gene. The expression measure is the data available for analysis. However, as pointed out by \cite{li:wong:2001a}, much can be learned from studying the individual probe intensities, or as we call them, the {\it probe level data}. This is why we developed this package. The package includes plotting functions for the probe level data useful for quality control, RNA degradation assessments, different probe level normalization and background correction procedures, and flexible functions that permit the user to convert probe level data to expression measures. The package includes utilities for computing expression measures similar to MAS 4.0's AvDiff \citep{affy4}, MAS 5.0's signal \citep{affy5}, DChip's MBEI \citep{li:wong:2001a}, and RMA \citep{iriz:etal:2003}.  We assume that the reader is already familiar with oligonucleotide arrays and with the design of the Affymetrix GeneChip arrays. If you are not, we recommend the Appendix of the Affymetrix MAS manual \cite{affy4,affy5}.  The following terms are used throughout this document: \begin{description} \item[probe] oligonucleotides of 25 base pair length used to probe RNA targets. \item[perfect match] probes intended to match perfectly the target sequence. \item[$PM$] intensity value read from the perfect matches. \item[mismatch] the probes having one base mismatch   with the target sequence intended to account for non-specific binding. \item[$MM$] intensity value read from the mis-matches. \item[probe pair] a unit composed of a  perfect match and its mismatch. \item[affyID] an identification for a probe set (which can be a gene or a fraction of a gene) represented on the array. \item[probe pair set] $PM$s and $MM$s related to a common {\it affyID}. \item[{\it CEL} files] contain measured intensities and locations for an array that has been hybridized. \item[{\it CDF} file] contain the information relating probe pair sets to locations on the array. \end{description}  Section \ref{whatsnew} describes the main differences between version 1.5 and this version (1.6). Section \ref{sec:get.started} describes a quick way of getting started and getting expression measures. Section \ref{qc} describes some quality control tools. Section \ref{s1.4} describes normalization routines. Section \ref{classes} describes the different classes in the package. \ref{sec:probesloc} describes our strategy to map probe locations to probe set membership. Section \ref{configure.options} describes how to change the package's default options. Section \ref{whatwasnew} describes earlier changes.  %%%make sure to change this when we get a publication about version 2. {\bf Note:} If you use this package please cite \cite{gaut:cope:bols:iriz:2003} and/or \cite{iriz:gaut:cope:2003}.  \section{Changes for affy in BioC 1.8 release} \label{whatsnew} There were relatively few changes.  \begin{itemize} \item MAplot now accepts the argument \Rfunction{plot.method} which can be used to call smoothScatter. \item \Rfunction{normalize.quantiles.robust} has had minor changes. \item \Rfunction{ReadAffy} can optionally return the SD values stored in the cel file. \item The C parsing code has been moved to the \Rpackage{affyio} package, which is now a dependency of the affy package. This change should be transparent to users as \Rpackage{affyio} will be automatically loaded when affy is loaded. \item Added a cdfname argument to \Rfunction{justRMA} and \Rfunction{ReadAffy} to allow for the use of alternative cdf packages. \end{itemize}  \section{Getting Started: From probe level data to expression values} \label{sec:get.started}  The first thing you need to do is {\bf load the package}. \begin{Sinput} R> library(affy) ##load the affy package \end{Sinput} <<echo=F,results=hide>>= library(affy) @ This release of the \Rpackage{affy} package will automatically download the appropriate cdf environment when you require it. However, if you wish you may download and install the cdf environment you need from \url{http://bioconductor.org/help/bioc-views/release/data/annotation/} manually. If there is no cdf environment currently built for your particular chip and you have access to the CDF file then you may use the \Rpackage{makecdfenv} package to create one yourself. To make the cdf packaes, Microsoft Windows users will need to use the tools described in \url{http://www.murdoch-sutherland.com/Rtools/}.   \subsection{Quick start} If all you want is to go from probe level data ({\it Cel} files) to expression measures here are some quick ways.  If you want is RMA, the quickest way of reading in data and getting expression measures  is the following: \begin{enumerate} \item Create a directory, move all the relevant {\it CEL} files to that directory \item If using linux/unix, start R in that directory. \item If using the Rgui for Microsoft Windows make sure your working directory contains the {\it Cel} files (use ``File -> Change Dir'' menu item). \item Load the library. \begin{Sinput} R> library(affy) ##load the affy package \end{Sinput} \item Read in the data and create an expression, using RMA for example. \begin{Sinput} R> Data <- ReadAffy() ##read data in working directory R> eset <- rma(Data) \end{Sinput} \end{enumerate}  Depending on the size of your dataset and on the memory available to your system, you might experience errors like `Cannot allocate vector \ldots'. An obvious option is to increase the memory available to your R process (by adding memory and/or closing external applications\footnote{UNIX-like systems users might also want to check {\it ulimit} and/or compile {\bf R} and the package for 64 bits when possible.}. An another option is to use the function \Rfunction{justRMA}. \begin{Sinput} R> eset <- justRMA() \end{Sinput} This reads the data and performs the `RMA' way to preprocess them at the {\it C} level. One does not need to call \verb+ReadAffy+, probe level data is never stored in an AffyBatch. \verb+rma+ continues to be the recommended function for computing RMA.   The \Rfunction{rma} function was written in C for speed and efficiency. It uses the expression measure described in \cite{iriz:etal:2003}.  For other popular methods use \Rfunction{expresso} instead of \Rfunction{rma} (see Section \ref{expresso}). For example for our version of MAS 5.0 signal uses expresso (see code). To get mas 5.0  you can use \begin{Sinput} R> eset <- mas5(Data) \end{Sinput} which will also normalize the expression values. The normalization can be turned off through the \verb+normalize+ argument.  In all the above examples, the variable \Robject{eset} is an object of class \Robject{ExpressionSet} described in the Biobase vignette. Many of the packages in BioConductor work on objects of this class. See the \Rpackage{genefilter} and \Rpackage{geneplotter} packages for some examples.  If you want to use some other analysis package, you can write out the expression values to file using the following command: \begin{Sinput} R> write.exprs(eset, file=""mydata.txt"") \end{Sinput}  \subsection{Reading CEL file information} The function \Rfunction{ReadAffy} is quite flexible. It lets you specify the filenames, phenotype, and MIAME information. You can enter them by reading files (see the help file) or widgets (you need to have the tkWidgets package installed and working).  \begin{Sinput} R> Data <- ReadAffy(widget=TRUE) ##read data in working directory \end{Sinput} This function call will pop-up a file browser widget, see Figure \ref{fig:widget.filechooser}, that provides an easy way of choosing cel files.  \newpage \begin{figure}[htbp]   \begin{center}     \includegraphics{widgetfilechooser}     \caption{\label{fig:widget.filechooser}Graphical display for selecting       {\it CEL} files. This widget is part of the {\it tkWidgets} package.       (function written by Jianhua (John) Zhang).     }   \end{center} \end{figure}  Next, a widget (not shown) permits the user to enter the \verb+phenoData+. %%See Figure \ref{fig:widget.pd}. %% \begin{figure}[htbp] %% \begin{center} %% \begin{tabular}{c} %% \includegraphics{numcovariates}\\ %% \includegraphics{namecovariates}\\ %% \includegraphics{assigncovariates} %% \end{tabular} %% \caption{\label{fig:widget.pd}Graphical display for entering phenoData %% This widget is part %% of the {\it tkWidgets} package.} %% % (functions written by Majnu John.} %% \end{center} %% \end{figure} Finally the a widget is presented for the user to enter MIAME information. %%Seen in Figure \ref{fig:widget.tkMIAME}.   %% \begin{figure}[htbp] %% \begin{center} %% \includegraphics[width=0.5\textwidth]{widgettkMIAME} %% \caption{\label{fig:widget.tkMIAME}Graphical display for entering {\it %%     MIAME} informations. This widget is part of the {\it tkWidgets} %%     package.} %% % (function written by Majnu John).} %% \end{center} %% \end{figure}  Notice that it is not necessary to use widgets to enter this information. Please read the help file for more information on how to read it from flat files or to enter it programmatically.  The function \Rfunction{ReadAffy} is a wrapper for the functions \Rfunction{read.affybatch}, \Rfunction{tkSampleNames}, \Rfunction{read.AnnotatedDataFrame}, and \Rfunction{read.MIAME}. The function \Rfunction{read.affybatch} has some nice feature that make it quite flexible. For example, the \verb+compression+ argument permit the user to read compressed {\it CEL} files. The argument {\it compress} set to {\it TRUE} will inform the readers that your files are compressed and let you read them while they remain compressed. The compression formats {\it zip} and {\it gzip} are known to be recognized.  A comprehensive description of all these options is found in the help file: \begin{Sinput} R> ?read.affybatch R> ?read.AnnotatedDataFrame R> ?read.MIAME \end{Sinput}   \subsection{Expression measures}  The most common operation is certainly to convert  probe level data to expression values. Typically this is achieved through the following sequence: \begin{enumerate} \item reading in probe level data. \item background correction. \item normalization. \item probe specific background correction, e.g. subtracting $MM$. \item summarizing the probe set values into one expression measure and, in some cases, a standard error for this summary. \end{enumerate} We detail what we believe is a good way to proceed below. As mentioned the function \Rfunction{expresso} provides many options. For example, \begin{Sinput} R> eset <- expresso(Dilution, normalize.method=""qspline"",                bgcorrect.method=""rma"",pmcorrect.method=""pmonly"",                summary.method=""liwong"") \end{Sinput}  This will store expression values, in the object \Robject{eset}, as an object of class \Robject{ExpressionSet} (see the \Rpackage{Biobase} package). You can either use R and the BioConductor packages to analyze your expression data or if you rather use another package you can write it out to a tab delimited file like this  \begin{Sinput} R> write.exprs(eset, file=""mydata.txt"") \end{Sinput}  In the \verb+mydata.txt+ file, row will represent genes and columns will represent samples/arrays. The first row will be a header describing the columns. The first column will have the {\it affyID}s. The \Rfunction{write.exprs} function is quite flexible on what it writes (see the help file).   \subsubsection{expresso} \label{expresso} The function \Rfunction{expresso} performs the steps background correction, normalization, probe specific correction, and summary value computation. We now show this using an \Robject{AffyBatch} included in the package for examples. The command \verb+data(Dilution)+ is used to load these data.  Important parameters for the expresso function are:  \begin{description} \item[bgcorrect.method]. The background correction method to use. The available methods are <<>>= bgcorrect.methods() @  \item[normalize.method]. The normalization method to use. The    available methods can be queried by using \verb+normalize.methods+. <<>>= library(affydata) data(Dilution) ##data included in the package for examples normalize.methods(Dilution) @  \item[pmcorrect.method] The method for probe specific correction. The available methods are <<>>= pmcorrect.methods() @  \item[summary.method]. The summary method to use. The available methods are <<>>= express.summary.stat.methods() @ Here we use \Rfunction{mas} to refer to the methods described in the Affymetrix manual version 5.0.  \item[widget] Making the \verb+widget+ argument \verb+TRUE+,  will let you select missing parameters (like the normalization method, the background correction method or the summary method). Figure \ref{fig:expressochooser} shows the widget for the selection of preprocessing methods for each of the steps.  \begin{Sinput} R> expresso(Dilution, widget=TRUE) \end{Sinput}  \begin{figure}[htbp] \begin{center} \includegraphics[width=0.5\textwidth]{EWSnap} \caption{\label{fig:expressochooser}Graphical display for selecting expresso methods.} \end{center} \end{figure}  \end{description}  There is a separate vignette {\bf affy: Built-in Processing Methods} which explains in more detail what each of the preprocessing options does.  \subsubsection{MAS 5.0} To obtain expression values that correspond to those from MAS 5.0, use \Rfunction{mas5}, which wraps \Rfunction{expresso} and \Rfunction{affy.scalevalue.exprSet}.  <<>>= eset <- mas5(Dilution) @  To obtain MAS 5.0 presence calls you can use the \verb+mas5calls+ method.  <<>>= Calls <- mas5calls(Dilution) @ This returns an \verb+ExpressionSet+ object containing P/M/A calls and their associated Wilcoxon p-values.  \subsubsection{Li and Wong's MBEI (dchip)} To obtain our version of Li and Wong's MBEI one can use \begin{Sinput} R> eset <- expresso(Dilution, normalize.method=""invariantset"",                  bg.correct=FALSE,                  pmcorrect.method=""pmonly"",summary.method=""liwong"") \end{Sinput}  This gives the current $PM$-only default. The reduced model (previous default) can be obtained using \verb+pmcorrect.method=""subtractmm""+.  \subsubsection{C implementation of RMA} One of the quickest ways to compute expression using the \Rpackage{affy} package is to use the \Rfunction{rma} function. We have found that this method allows a user to compute the RMA expression measure in a matter of minutes for datasets that may have taken hours in previous versions of \Rpackage{affy}. The function serves as an interface to a hard coded C implementation of the RMA method \citep{iriz:etal:2003}. Generally, the following would be sufficient to compute RMA expression measures: <<>>= eset <- rma(Dilution) @  Currently the \Rfunction{rma} function implements RMA in the following manner \begin{enumerate} \item Probe specific correction of the PM probes using a model based   on observed intensity being the sum of signal and noise \item Normalization of corrected PM probes using quantile   normalization \citep{bols:etal:2003} \item Calculation of Expression measure using median polish. \end{enumerate}  The \Rfunction{rma} function is likely to be improved and extended in the future as the RMA method is fine-tuned.  \newpage   \section{Quality Control through Data Exploration} \label{qc}  For the users convenience we have included the \verb+Dilution+ sample data set:  <<>>= Dilution @  This will create the \verb+Dilution+ object of class \Robject{AffyBatch}. \Rfunction{print} (or \Rfunction{show}) will display summary information. These objects represent data from one experiment. The \Robject{AffyBatch} class combines the information of various {\it CEL} files with a common {\it CDF} file. This class is designed to keep information of one experiment. The probe level data is contained in this object.  The data in \verb+Dilution+ is a small sample of probe sets from 2 sets of duplicate arrays hybridized with different concentrations of the same RNA. This information is part of the \Robject{AffyBatch} and can be accessed with the \verb+phenoData+ and \verb+pData+ methods: <<>>= phenoData(Dilution) pData(Dilution) @  Several of the functions for plotting summarized probe level data are useful for diagnosing problems with the data. The plotting functions \Rfunction{boxplot} and \Rfunction{hist} have methods for \Robject{AffyBatch} objects.  Each of these functions presents side-by-side graphical summaries of intensity information from each array.  Important differences in the distribution of intensities are often evident in these plots. The function \Rfunction{MAplot} (applied, for example, to \verb+pm(Dilution)+), offers pairwise graphical comparison of intensity data. The option \verb+pairs+ permits you to chose between all pairwise comparisons (when \verb+TRUE+) or compared to a reference array (the default). These plots can be particularly useful in diagnosing problems in replicate sets of arrays. The function argument \verb+plot.method+ can be used to create a MAplot using a smoothScatter, rather than the default method which is to draw every point.  \begin{figure}[htbp] \begin{center} <<fig=TRUE>>= data(Dilution) MAplot(Dilution,pairs=TRUE,plot.method=""smoothScatter"") @ \end{center} \caption{Pairwise MA plots} \end{figure}   \subsection{Accessing $PM$ and $MM$ Data} The $PM$ and $MM$  intensities and corresponding {\it affyID} can be accessed with the \Rfunction{pm}, \Rfunction{mm}, and \Rfunction{probeNames} methods. These will be matrices with rows representing probe pairs and columns representing arrays. The gene name associated with the probe pair in row $i$ can be found in the $i$th entry of the vector returned by \Rfunction{probeNames}.  <<>>= Index <- c(1,2,3,100,1000,2000) ##6 arbitrary probe positions pm(Dilution)[Index,] mm(Dilution)[Index,] probeNames(Dilution)[Index] @ \verb+Index+ contains six arbitrary probe positions.  Notice that the column names of $PM$ and $MM$ matrices are the sample names and the row names are the {\it affyID}, e.g. \verb+1001_at+ and \verb+1000_at+ together with the probe number (related to position in the target sequence).  <<>>= sampleNames(Dilution) @  {\bf Quick example:} To see what percentage of the $MM$ are larger than the $PM$ simply type <<>>= mean(mm(Dilution)>pm(Dilution)) @  The \Rfunction{pm} and \Rfunction{mm} functions can be used to extract specific probe set intensities. <<>>= gn <- geneNames(Dilution) pm(Dilution, gn[100]) @ The method \Rfunction{geneNames} extracts the unique {\it affyID}s. Also notice that the 100th probe set is different from the 100th probe! The 100th probe is not part of the the 100th probe set.  The methods \Rfunction{boxplot}, \Rfunction{hist}, and \Rfunction{image} are useful for quality control. Figure \ref{f3} shows kernel density estimates (rather than histograms) of $PM$ intensities for the 1st and 2nd array of the \verb+Dilution+ also included in the package.  \subsection{Histograms, Images, and Boxplots} \begin{figure}[htbp] \begin{center} <<fig=TRUE>>= hist(Dilution[,1:2]) ##PM histogram of arrays 1 and 2 @ \caption{\label{f3} Histogram of $PM$ intensities for 1st and 2nd array} \end{center} \end{figure}  As seen in the previous example, the sub-setting method \verb+[+ can be used to extract specific arrays. {\bf NOTE: Sub-setting is different in this version. One can no longer subset by gene. We can only define subsets by one dimension: the columns, i.e. the arrays. Because the \verb+Cel+ class is no longer available \verb+[[+ is no longer available.} %]]  The method \verb+image()+ can be used to detect spatial artifacts. By default we look at log transformed intensities. This can be changed through the \verb+transfo+ argument.  <<eval=FALSE>>= par(mfrow=c(2,2)) image(Dilution) @  \begin{figure}[htbp] \begin{center} \includegraphics{image} \caption{\label{f1} Image of the log intensities.} \end{center} \end{figure}  These images are quite useful for quality control. We recommend examining these images as a first step in data exploration.  The method \Rfunction{boxplot} can be used to show $PM$, $MM$ or both intensities. \begin{figure}[htbp] \begin{center} <<fig=TRUE>>= par(mfrow=c(1,1)) boxplot(Dilution, col=c(2,3,4)) @ \caption{\label{f4}Boxplot of arrays in Dilution data.} \end{center} \end{figure} As discussed in the next section this plot shows that we need to normalize these arrays.   \subsection{RNA degradation plots} The functions \Rfunction{AffyRNAdeg}, \Rfunction{summaryAffyRNAdeg}, and \Rfunction{plotAffyRNAdeg} aid in assessment of RNA quality. Individual probes in a probeset are ordered by location relative to the $5'$ end of the targeted RNA molecule.\cite{affy4}  Since RNA degradation typically starts from the $5'$ end of the molecule, we would expect probe intensities to be systematically lowered at that end of a probeset when compared to the $3'$ end.  On each chip, probe intensities are averaged by location in probeset, with the average taken over probesets. The function \Rfunction{plotAffyRNAdeg} produces a side-by-side plots of these means, making it easy to notice any $5'$ to $3'$ trend.  The function \Rfunction{summaryAffyRNAdeg} produces a single summary statistic for each array in the batch, offering a convenient measure of the severity of degradation and significance level.  For an example <<>>= deg <- AffyRNAdeg(Dilution) names(deg) @ does the degradation analysis and returns a list with various components. A summary can be obtained using <<>>= summaryAffyRNAdeg(deg) @  Finally a plot can be created using \Rfunction{plotAffyRNAdeg}, see Figure \ref{f4.3}.  \begin{figure}[htbp] \begin{center} <<fig=TRUE>>= plotAffyRNAdeg(deg) @ \caption{\label{f4.3} Side-by-side plot produced by plotAffyRNAdeg.} \end{center} \end{figure}  \newpage  \section{Normalization} \label{s1.4} Various researchers have pointed out the need for normalization of Affymetrix arrays. See for example \cite{bols:etal:2003}. The method \verb+normalize+ lets one normalize at the probe level  <<>>= Dilution.normalized <- normalize(Dilution) @  For an extended example on normalization please refer to the vignette in the affydata package.  \section{Classes} \label{classes} \verb+AffyBatch+ is the main class in this package. There are three other auxiliary classes that we also describe in this Section. \subsection{AffyBatch} The AffyBatch class has slots to keep all the probe level information for a batch of {\it Cel} files, which usually represent an experiment. It also stores phenotypic and MIAME information as does the \verb+ExpressionSet+ class in the Biobase package (the base package for BioConductor). In fact, \verb+AffyBatch+ extends \verb+ExpressionSet+.  The expression matrix in \verb+AffyBatch+ has columns representing the intensities read from the different arrays. The rows represent the {\it cel} intensities for all position on the array. The cel intensity with physical coordinates\footnote{Note that in the {\it .CEL} files the indexing starts at zero while it starts at 1 in the package (as indexing starts at 1 in {\bf R}).} $(x,y)$ will be in row \[i = x + \mathtt{nrow} \times (y - 1)\]. The \verb+ncol+ and \verb+nrow+ slots contain the physical rows of the array. Notice that this is different from the dimensions of the expression matrix. The number of row of the expression matrix is equal to \verb+ncol+$\times$\verb+nrow+. We advice the use of the functions \verb+xy2indices+ and \verb+indices2xy+ to shuttle from X/Y coordinates to indices.  For compatibility with previous versions the accessor method \verb+intensity+ exists for obtaining the expression matrix.  The \verb+cdfName+ slot contains the necessary information for the package to find the locations of the probes for each probe set. See Section \ref{sec:probesloc} for more on this.  \subsection{ProbeSet} The \verb+ProbeSet+ class holds the information of all the probes related to an {\it affyID}. The components are \verb+pm+ and \verb+mm+.  The method \verb+probeset+ extracts probe sets from \verb+AffyBatch+ objects. It takes as arguments an \verb+AffyBatch+ object and a vector of {\it affyIDs} and returns a list of objects of class \verb+ProbeSet+ <<>>= gn <- featureNames(Dilution) ps <- probeset(Dilution, gn[1:2]) #this is what i should be using: ps show(ps[[1]]) @  The \verb+pm+ and \verb+mm+ methods can be used to extract these matrices (see below).  This function is general in the way it defines a probe set. The default is to use the definition of a probe set given by Affymetrix in the CDF file. However, the user can define arbitrary probe sets. The argument \verb+locations+ lets the user decide the row numbers in the \verb+intensity+ that define a probe set. For example, if we are interested in redefining the \verb+AB000114_at+ and \verb+AB000115_at+ probe sets, we could do the following:  First, define the locations of the $PM$ and $MM$ on the array of the \verb+1000_at+ and \verb+1001_at+ probe sets <<>>= mylocation <- list(""1000_at""=cbind(pm=c(1,2,3),mm=c(4,5,6)),                    ""1001_at""=cbind(pm=c(4,5,6),mm=c(1,2,3))) @ The first column of the matrix defines the location of the $PM$s and the second column the $MM$s.  Now we are ready to extract the \verb+ProbSet+s using the \verb+probeset+ function: <<>>= ps <- probeset(Dilution, genenames=c(""1000_at"",""1001_at""),                  locations=mylocation) @ Now, \verb+ps+ is list of \verb+ProbeSet+s. We can see the $PM$s and $MM$s of each component using the \verb+pm+ and \verb+mm+ accessor methods. <<>>= pm(ps[[1]]) mm(ps[[1]]) pm(ps[[2]]) mm(ps[[2]]) @  This can be useful in situations where the user wants to determine if leaving out certain probes improves performance at the expression level. It can also be useful to combine probes from different human chips, for example by considering only probes common to both arrays.  Users can also define their own environment for probe set location mapping. More on this in Section \ref{sec:probesloc}.  An example of a \verb+ProbeSet+ is included in the package. A spike-in data set is included in the package in the form of a list of \verb+ProbeSet+s. The help file describes the data set. Figure \ref{f5.3} uses this data set to demonstrate that the $MM$ also detect transcript signal.  \begin{figure}[htbp] \begin{center} <<fig=TRUE>>= data(SpikeIn) ##SpikeIn is a ProbeSets pms <- pm(SpikeIn) mms <- mm(SpikeIn)  ##pms follow concentration par(mfrow=c(1,2)) concentrations <- matrix(as.numeric(sampleNames(SpikeIn)),20,12,byrow=TRUE) matplot(concentrations,pms,log=""xy"",main=""PM"",ylim=c(30,20000)) lines(concentrations[1,],apply(pms,2,mean),lwd=3) ##so do mms matplot(concentrations,mms,log=""xy"",main=""MM"",ylim=c(30,20000)) lines(concentrations[1,],apply(mms,2,mean),lwd=3) @ \caption{\label{f5.3}PM and MM intensities plotted against SpikeIn concentration} \end{center} \end{figure}   \section{Location to ProbeSet Mapping} \label{sec:probesloc} On Affymetrix GeneChip arrays, several probes are used to represent genes in the form of probe sets. From a {\it CEL} file we get for each physical location, or cel, (defined by $x$ and $y$ coordinates) an intensity. The {\it CEL} file also contains the name of the {\it CDF} file needed for the location-probe-set mapping. The {\it CDF} files store the probe set related to each location on the array. The computation of a summary expression values from the probe intensities requires a fast way to map an {\it affyid} to corresponding probes. We store this mapping information in {\bf R} environments\footnote{Please refer to the {\bf R} documentation to know more about environments.}. They only contain a part of the information that can be found in the {\it CDF} files. The {\it cdfenvs} are sufficient to perform the numerical processing methods included in the package. For each {\it CDF} file there is package, available from \url{http://bioconductor.org/help/bioc-views/release/data/annotation/}, that contains exactly one of these environments. The {\it cdfenvs} we store the $x$ and $y$ coordinates as one number (see above).  In instances of {\it AffyBatch}, the {\it cdfName} slot gives the name of the appropriate {\it CDF} file for arrays represented in the \verb+intensity+ slot. The functions \verb+read.celfile+, \verb+read.affybatch+, and \verb+ReadAffy+ extract the {\it CDF} filename from the {\it CEL} files being read. Each {\it CDF} file corresponds to exactly one environment. The function \verb+cleancdfname+ converts the Affymetrix given {\it CDF} name to a BioConductor environment and annotation name. Here are two examples:  These give environment names: <<>>= cat(""HG_U95Av2 is"",cleancdfname(""HG_U95Av2""),""\n"") cat(""HG-133A is"",cleancdfname(""HG-133A""),""\n"") @  This gives annotation name: <<>>= cat(""HG_U95Av2 is"",cleancdfname(""HG_U95Av2"",addcdf=FALSE),""\n"") @  An environment representing the corner of an Hu6800 array is available with the package. In the following, we l",35599
"21","affy","Microarray:OneChannel:Preprocessing","% -*- mode: noweb; noweb-default-code-mode: R-mode; -*- %\VignetteIndexEntry{2. Built-in Processing Methods} %\VignetteKeywords{Preprocessing, Affymetrix} %\VignetteDepends{affy} %\VignettePackage{affy} %documentclass[12pt, a4paper]{article} \documentclass[12pt]{article}  \usepackage{amsmath} \usepackage{hyperref} \usepackage[authoryear,round]{natbib}  \textwidth=6.2in \textheight=8.5in %\parskip=.3cm \oddsidemargin=.1in \evensidemargin=.1in \headheight=-.3in  \newcommand{\scscst}{\scriptscriptstyle} \newcommand{\scst}{\scriptstyle} \author{Ben Bolstad} \begin{document} \title{affy: Built-in Processing Methods} \maketitle \tableofcontents \section{Introduction}  This document describes the preprocessing methods that have currently been built into the \verb+affy+ package.  Hopefully it will clarify for the reader what each of the routines does. There is a separate vignette which describes how you might write your own routines and use them in combination with the built-in routines.   As usual, loading the package in your \verb+R+ session is required.  \begin{Sinput} R> library(affy) ##load the affy package \end{Sinput} <<echo=F,results=hide>>= library(affy) @  \section{Background methods}  You can see the background correction methods that are built into the package by examining the variable \verb+bgcorrect.method+.   <<>>= bgcorrect.methods() @  \subsection{none}  Calling this method actually does nothing. It returns the object unchanged. May be used as a placeholder.   \subsection{rma/rma2}  These are background adjustment implementations for the rma method \cite{iriz:etal:2003}. They differ only in how they estimate a set of parameters (generally you should use \verb+rma+ in preference to \verb+rma2+. In both cases PM probe intensities are corrected by using a global model for the distribution of probe intensities. The model is suggested by looking at plots of the empirical distribution of probe intensities.  In particular the observed PM probes are modeled as the sum of a normal noise component N (Normal with mean $\mu$ and variance $\sigma^2$) and a exponential signal component S (exponential with mean $\alpha$). To avoid any possibility of negatives, the normal is truncated at zero. Given we have O the observed intensity, this then leads to an adjustment.   \begin{equation*} E\left(s \lvert O=o\right) = a + b \frac{\phi\left(\frac{a}{b}\right)   - \phi\left(\frac{o-a}{b}\right)}{\Phi\left(\frac{a}{b}\right) +   \Phi\left(\frac{o-a}{b}\right) - 1 }  \end{equation*} where $a =  s- \mu - \sigma^2\alpha$ and $b = \sigma$. Note that $\phi$ and $\Phi$ are the standard normal distribution density and distribution functions respectively.   Note that MM probe intensities are not corrected by either of these routines.  \subsection{mas}  This is an implementation of the background correction method outlined in the Statistical Algorithms Description Document \cite{affy:tech:2002}. The chip is broken into a grid of 16 rectangular regions. For each region the lowest 2\% of probe intensities are used to compute a background value for that grid. Each probe is then adjusted based upon a weighted average of the backgrounds for each of the regions. The weights are based on the distances between the location of the probe and the centriods of 16 different regions. Note this method corrects both PM and MM probes.   \section{Normalization Methods}  You can see the background correction methods that are built into the package by examining the variable \verb+bgcorrect.method+.  <<>>= normalize.AffyBatch.methods() @  The Quantile, Contrast and Loess normalizations have been discussed and compared in \cite{bols:etal:2003}.   \subsection{quantiles/quantiles.robust}  The quantile method was introduced by \cite{bols:etal:2003}. The goal is to give each chip the same empirical distribution. To do this we use the following algorithm where $X$ is a matrix of probe intensities (probes by arrays):    \begin{enumerate} \item Given $n$ array of length $p$, form $X$  of dimension $p \times n$  where each array is a column \item Sort each column of $X$ to give $X_{\mbox{sort}}$ \item Take the means across rows of $X_{\mbox{sort}}$ and assign this mean to each element in the row to get $X'_{\mbox{sort}}$ \item Get $X_{\mbox{normalized}}$ by rearranging each column of $X'_{\mbox{sort}}$ to have the same ordering as original $X$ \end{enumerate}   The quantile normalization method is a specific case of the transformation $x'_{i} = F^{-1}\left(G\left(x_{i}\right)\right)$, where we estimate $G$ by the empirical distribution of each array and $F$ using the empirical distribution of the averaged sample quantiles. Quantile normalization is pretty fast.   The {\tt quantiles} function performs the algorithm as above. The {\tt quantile.robust} function allows you to exclude or down-weight arrays in the computation of $\hat G$ above. In most cases we have found that the {\tt quantiles} method is sufficient for use and {\tt quantiles.robust} not required.     \subsection{loess}  There is a discussion of this method in \cite{bols:etal:2003}. It generalizes the $M$ vs $A$ methodology proposed in \cite{Dudoit:2002} to multiple arrays. It works in a pairwise manner and is thus slow when used with a large number of arrays.   \subsection{contrasts}  This method was proposed by \cite{astr:2003}. It is also a variation on the  $M$ vs $A$ methodology, but the normalization is done by transforming the data to a set of contrasts, then normalizing.   \subsection{constant}  A scaling normalization. This means that all the arrays are scaled so that they have the same mean value. This would be typical of the approach taken by Affymetrix. However, the Affymetrix normalization is usually done after summarization (you can investigate \verb+affy.scalevalue.exprSet+ if you are interested) and this normalization is carried out before summarization.   \subsection{invariantset}  A normalization similar to that used in the dChip software \cite{li:wong:2001a}. Using a baseline array, arrays are normalized by selecting invariant sets of genes (or probes) then using them to fit a non-linear relationship between the ``treatment'' and ``baseline'' arrays. The non-linear relationship is used to carry out the normalization.   \subsection{qspline}  This method is documented in \cite{workman:etal:2002}. Using a target array (either one of the arrays or a synthetic target), arrays are normalized by fitting splines to the quantiles, then using the splines to perform the normalization.  \section{PM correct methods}  <<>>= pmcorrect.methods() @ \subsection{mas}  An {\it ideal mismatch} is subtracted from PM. The ideal mismatch is documented by \cite{affy:tech:2002}. It has been designed so that you subtract MM when possible (ie MM is less than PM) or something else when it is not possible. The Ideal Mismatch will always be less than the corresponding PM and thus we can safely subtract it without risk of negative values.    \subsection{pmonly}  Make no adjustment to the pm values.  \subsection{subtractmm}  Subtract MM from PM. This would be the approach taken in MAS 4 \cite{affy4}. It could also be used in conjunction with the Li-Wong model.   \section{Summarization methods}  <<>>= express.summary.stat.methods() @  \subsection{avgdiff}  Compute the average. This is the approach that was taken in \cite{affy4}.  \subsection{liwong}  This is an implementation of the methods proposed in \cite{li:wong:2001a} and \cite{li:wong:2001b}. The Li-Wong MBEI is based upon fitting the following multi-chip model to each probeset  \begin{equation} y_{ij} = \phi_i \theta_j + \epsilon_{ij} \end{equation} where $y_{ij}$ is $PM_{ij}$ or the difference between $PM_{ij}-MM_{ij}$. The $\phi_i$ parameter is a probe response parameter and $\theta_j$ is the expression on array $j$.    \subsection{mas}  As documented in \cite{affy:tech:2002}, a robust average using 1-step Tukey biweight on $\log_2$ scale.   \subsection{medianpolish}  This is the summarization used in the RMA expression summary \cite{iriz:etal:2003}. A multichip linear model is fit to data from each probeset. In particular for a probeset $k$ with $i=1,\dots,I_k$ probes and data from $j=1,\dots,J$ arrays we fit the following model  \begin{equation*} \log_2\left(PM^{(k)}_{ij}\right) = \alpha_i^{(k)} + \beta_j^{(k)} + \epsilon_{ij}^{(k)} \end{equation*} where $\alpha_i$ is a probe effect and $\beta_j$ is the $\log_2$ expression value. The medianpolish is an algorithm (see \cite{tukey:1977}) for fitting this model robustly. Please note that expression values you get using this summary measure will be in $\log_2$ scale.   \subsection{playerout}  This method is detailed in \cite{Lazardis:etal:2002}. A non-parametric method is used to determine weights. The expression value is then the weighted average.   \section{Putting it altogether using {\tt expresso}}  The function that you should use is {\tt expresso}. It is important to note that not every preprocessing method can be combined together. In particular the \verb+rma+ backgrounds adjust only PM probe intensities and so they should only be used in conjunction with the \verb+pmonly+ PM correction. Also remember that the \verb+mas+ and \verb+medianpolish+ summarization methods $\log_2$ transform the data, thus they should not be used in conjunction with any preprocessing steps that are likely to yield negatives like the \verb+subtractmm+ pm correction method. The following is a typical call to \verb+expresso+.   \begin{Sinput}  library(affydata)  data(Dilution)  eset <- expresso(Dilution,bgcorrect.method=""rma"",                   normalize.method=""quantiles"",                   pmcorrect.method=""pmonly"",                   summary.method=""medianpolish"") \end{Sinput} %@   This would give you the RMA expression measure, but of course there are other ways of computing RMA (chiefly \verb+rma+). The true power of \verb+expresso+ becomes apparent when you start combining different methods. By choosing a method for each of the four steps ({\tt   bgcorrect.method}, {\tt normalize.method}, {\tt pmcorrect.method}, {\tt summary.method}) you can create quite a variety of expression measures. For instance   \begin{Sinput} eset <- expresso(Dilution,bgcorrect.method=""mas"",                   normalize.method=""qspline"",                   pmcorrect.method=""subtractmm"",                   summary.method=""playerout"") \end{Sinput}  would be a valid way of computing an expression measure (it is up to the user to decide whether such a concoction is sensible or not).    \bibliographystyle{plainnat} \bibliography{affy}  \end{document}",10607
"22","affy","Microarray:OneChannel:Preprocessing","% -*- mode: noweb; noweb-default-code-mode: R-mode; -*- %\VignetteIndexEntry{3. Custom Processing Methods} %\VignetteKeywords{Preprocessing, Affymetrix} %\VignetteDepends{affy} %\VignettePackage{affy} %documentclass[12pt, a4paper]{article} \documentclass[12pt]{article}  \usepackage{amsmath} \usepackage{hyperref} \usepackage[authoryear,round]{natbib}  \textwidth=6.2in \textheight=8.5in %\parskip=.3cm \oddsidemargin=.1in \evensidemargin=.1in \headheight=-.3in  \newcommand{\scscst}{\scriptscriptstyle} \newcommand{\scst}{\scriptstyle}  \newcommand{\Rfunction}[1]{{\texttt{#1}}} \newcommand{\Robject}[1]{{\texttt{#1}}} \newcommand{\Rpackage}[1]{{\textit{#1}}} \newcommand{\Rmethod}[1]{{\texttt{#1}}} \newcommand{\Rfunarg}[1]{{\texttt{#1}}} \newcommand{\Rclass}[1]{{\textit{#1}}}  \author{Laurent} \begin{document} \title{affy: Custom Processing Methods (HowTo)}  \maketitle \tableofcontents \section{Introduction} This document describes briefly how to customize the affy package by adding one's own processing methods. The types of processing methods are background correction, normalization, perfect match correction  and summary expression value computation.  We tried our best to make this as easy as we could, but we are aware that it is far from being perfect. We are still working on things to improve them. Hopefully this document should let you extend the package with supplementary processing methods easily.  As usual, loading the package in your \verb+R+ session is required.  \begin{Sinput} R> library(affy) ##load the affy package \end{Sinput} <<echo=F,results=hide>>= library(affy) @  \section{How-to} For each processing step, labels for the methods known to the package are stored in variables.   <<>>= normalize.AffyBatch.methods() bgcorrect.methods() pmcorrect.methods() express.summary.stat.methods() @ We would recommend the use of the method \verb+normalize.methods+ to access the list of available normalization methods (as a scheme for normalization methods that would go beyond 'affy' is thought). <<>>= library(affydata) data(Dilution) normalize.methods(Dilution) @  For each processing step, a naming convention exists between the method label and the function name in \verb+R+ (see table~\ref{table:summary.labels}). Each processing methods should be passed objects (and return objects) corresponding to the processing step (see table~\ref{table:summary.methods}).  \begin{table}  \begin{tabular}{|c|c|} \hline variable for labels & naming convention \\ \hline bgcorrect.methods & bg.correct.<label> \\ %\hline normalize.AffyBatch.methods & normalize.AffyBatch.<label> \\ %\hline pmcorrect.methods & pmcorrect.<label> \\ %\hline express.summary.stat.methods & generateExprset.methods.<label>\\ \hline  \end{tabular} \caption{\label{table:summary.labels}Summary table for the processing methods.} \end{table}  \begin{table}  %\begin{tabular}{|c|c|p{0.5\textwidth}|}  \begin{tabular}{|c|c|p{5cm}|} \hline step & argument(s) & returns \\ \hline background correction & \verb+AffyBatch+ & \verb+AffyBatch+ \\ %\hline normalization & \verb+AffyBatch+ & \verb+AffyBatch+ \\ %\hline {\it pm} correction &  \verb+ProbeSet+ & a \verb+matrix+ of corrected PM values (one probe per row, one column per chip).\\ %\hline expression value & a \verb+matrix+ of PM values & a list of two elements \verb+exprs+ and \verb+se.exprs+\\ \hline  \end{tabular} \caption{\label{table:summary.methods}Summary table for the processing methods.} \end{table}  Practically, this means that to add a method one has to  \begin{enumerate}  \item create an appropriate method with a name satisfying the convention.  \item register the method by adding the label name to the corresponding  variable. \end{enumerate}  \section{Examples} As an example we show how to add two simple new methods. The first one does {\it pm} correction. The method subtract {\it mm} values to the {\it pm} values, except when the result is negative. In this case the {\it pm} value remains unchanged.  We create a function using the label name \verb+subtractmmsometimes+. <<>>= pmcorrect.subtractmmsometimes <- function(object) {    ## subtract mm   mm.subtracted <- pm(object) - mm(object)    ## find which ones are unwanted and fix them   invalid <- which(mm.subtracted <= 0)   mm.subtracted[invalid] <- pm(object)[invalid]    return(mm.subtracted) } @  Once the method defined, we just register the \emph{label name} in the corresponding variable. <<>>= upDate.pmcorrect.methods(c(pmcorrect.methods(), ""subtractmmsometimes"")) @  The second new method intends to be an robust alternative to the summary expression value computation \verb+avgdiff+. The idea is to use the function \verb+huber+ of the package \verb+MASS+. <<>>= huber <- function (y, k = 1.5, tol = 1e-06) {     y <- y[!is.na(y)]     n <- length(y)     mu <- median(y)     s <- mad(y)     if (s == 0)          stop(""cannot estimate scale: MAD is zero for this sample"")     repeat {         yy <- pmin(pmax(mu - k * s, y), mu + k * s)         mu1 <- sum(yy)/n         if (abs(mu - mu1) < tol * s)              break         mu <- mu1     }     list(mu = mu, s = s) } @ This method returns P.J. Huber's location estimate with MAD scale. You think this is what you want to compute the summary expression value from the probe intensities. What is needed to have as a processing method is a simple wrapper: <<>>= computeExprVal.huber <- function(probes) {   res <- apply(probes, 2, huber)   mu <- unlist(lapply(res, function(x) x$mu))   s <- unlist(lapply(res, function(x) x$s))   return(list(exprs=mu, se.exprs=s)) }  upDate.generateExprSet.methods(c(generateExprSet.methods(), ""huber"")) @   From now the package is aware of the two new methods\ldots in this session.  The code for the methods included in the package can be informative if you plan to develop methods. An example that demonstrates how a normalization method can be added is given by the function \Rfunction{normalize.AffyBatch.vsn} in the package \Rpackage{vsn}; see its help file.  \end{document}",6001
"23","affy","Microarray:OneChannel:Preprocessing","% -*- mode: noweb; noweb-default-code-mode: R-mode; -*- %\VignetteIndexEntry{4. Import Methods} %\VignetteKeywords{Preprocessing, Affymetrix} %\VignetteDepends{affy} %\VignettePackage{affy} %documentclass[12pt, a4paper]{article} \documentclass[12pt]{article}  \usepackage{amsmath} \usepackage{hyperref} \usepackage[authoryear,round]{natbib}  \textwidth=6.2in \textheight=8.5in %\parskip=.3cm \oddsidemargin=.1in \evensidemargin=.1in \headheight=-.3in  \newcommand{\scscst}{\scriptscriptstyle} \newcommand{\scst}{\scriptstyle}  \author{Laurent} \begin{document} \title{affy: Import Methods (HowTo)}  \maketitle \tableofcontents \section{Introduction} This document describes briefly how to write import methods for the \verb+affy+ package. As one might know, the Affymetrix data are originally stored in files of type \verb+.CEL+ and \verb+.CDF+. The package extracts and store the information contained in \verb+R+ data structures using file parsers. This document outlines how to get the data from other sources than the current\footnote{today's date is early 2003} file formats.  As usual, loading the package in your \verb+R+ session is required.  \begin{Sinput} R> library(affy) ##load the affy package \end{Sinput} <<echo=F,results=hide>>= library(affy) @  {\bf note: this document only describes the process for .CEL files}  Knowing the slots of \verb+Cel+ and \verb+AffyBatch+ objects will probably help you to achieve your goals. You may want to refer to the respective help pages. Try \verb+help(Cel)+, \verb+help(AffyBatch)+.  \section{How-to}  \subsection{Cel objects} The functions \verb+getNrowForCEL+ and \verb+getNcolForCEL+ are assumed to return the number of rows and the number of columns in the \verb+.CEL+ file respectively  You will also need to have access to the $X$ and $Y$ position for the probes in the {\verb .CEL} file. The functions \verb+getPosXForCel+ and \verb+getPosYForCEL+ are assumed to return the $X$ and $Y$ positions respectively. The corresponding probe intensities are assumed to be returned by the function \verb+getIntensitiesForCEL+.  If you stored {\bf all} the $X$ and $Y$ values that were in the \verb+.CEL+, the functions verb+getNrowForCEL+ and \verb+getNcolForCEL+ can written: <<<>>= getNrowForCEL <- function() max(getPosXForCEL()) getNcolForCEL <- function() max(getPosYForCEL()) @   You will also need the name for the corresponding \verb+.CDF+ (although you will probably no need the \verb+.CDF+ file itself, the cdf packages available for download will probably be enough).  \begin{Sinput} import.celfile <- function(celfile, ...) {    cel.nrow <- getNrowForCEL(celfile)   cel.ncol <- getNcolForCEL(celfile)   x <- matrix(NA, nr=cel.nrow, nc=cel.ncol)    cel.intensities <- getIntensitiesForCEL(celfile)    cel.posx <- getPosXForCEL(celfile) # +1 if indexing starts at 0 (like in .CEL)   cel.posy <- getPosYForCEL(celfile) # idem    x[cbind(cel.posx, cel.posy)] <- cel.intensities    mycdfName <- whatcdf(""aCELfile.CEL"")    myCel <- new(""Cel"", exprs=x, cdfName=mycdfName)    return(myCel) } \end{Sinput}  The function \verb+import.celfile+ can now replace the function \verb+read.celfile+ in the \verb+affy+ package    \subsection{AffyBatch objects}  (scratch) the use of \verb+...+ should make you able to override the function read.celfile by a hack like: \begin{Sinput} read.celfile <- import.celfile \end{Sinput} The function \verb+read.affybatch+ should now function using your \verb+import.celfile+   \end{document}",3477
"24","snpStats","Microarray:SNP:GeneticVariability","\documentclass[12pt]{article} \usepackage{fullpage} \usepackage{graphicx}  \usepackage[pdftex, bookmarks, bookmarksopen, pdfauthor={David Clayton}, pdftitle={Data input}] {hyperref}  \title{Data input vignette\\Reading genotype data in {\tt snpStats}} \author{David Clayton} \date{\today}  \usepackage{Sweave} \SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}  \begin{document} \setkeys{Gin}{width=1.0\textwidth}  %\VignetteIndexEntry{Data input} %\VignettePackage{snpStats}  \maketitle  <<lib,echo=FALSE>>= require(snpStats) @  \section*{Memory limitations} Before we start it is important to emphasise that the {\tt SnpMatrix} objects that hold genotype data in {\tt snpStats}  are resident in memory, and limitations of the computer and of the R language impose limits on the maximum size of datasets that can be held at any one time. Each genotype reading uses only a single byte of memory so that large datasets can be read given the large memory capacity of modern computers. Originally,  R imposed a limit of  $2^{31}-1 \sim 2\times 10^9$ elements in a single array. This limit applied in both the 32-bit and 64-bit versions   of R, these versions differing only in the {\em total} memory that could   be used. For example, this would correspond to one million loci for two thousand subjects and would occupy two gigabytes of machine memory. However, version 3 of R removed the restriction on single arrays in the 64-bit version,  and this was implemented for {\tt SnpMatrix} and {\tt XSnpMatrix} objects in version 1.19.2 of {\tt snpStats}. However, experience of this code is limited and some caution is advised.   \section*{Reading pedfiles} A commonly encountered format for storing genotype data is the ``pedfile'' format, which originated some years ago  in the LINKAGE package. Pedfiles are text files containing one line per genotyped sample, with fields separated by ``white space'' ( TAB characters or SPACEs). The first six fields contain: \begin{enumerate} \item a pedigree or family identifier, unique to the family of which   this subject is a member, \item a further identifier, unique (within the family) to each family member, \item the member identifier of the father of the subject if the father   is also present in the data, otherwise an arbitrary code (usually   {\tt 0}), \item similarly, an identifier for the mother of the subject, \item the sex of the subject ({\tt 1 =} Male, {\tt 2 =} Female), and \item a binary trait indicator ({\tt 1 =} Absent, {\tt 2 =} Present). \end{enumerate} Missing values in the last two fields are usually coded as zero.   The first few rows and columns of a sample file is shown below: \begin{verbatim} IBD054	430	0	0	1	0	1  3	3  1	4  1	4  2 IBD054	412	430	431	2	2	1  3	1  3	4  1	4  2 IBD054	431	0	0	2	0	3  3	3  3	1  1	2  2 IBD058	438	0	0	1	0	3  3	3  3	1  1	2  2 IBD058	470	438	444	2	2	3  3	3  3	1  1	2  2 \end{verbatim} Thus, the subject of line~2 has a father whose data appears on line~1 and a mother whose data is on line~3. The grandparents do not appear on the file. This subject is affected by the trait, but the trait status of her parents is not known. The genotypes of this subject  at the first four loci are {\tt 1/3}, {\tt 1/3}, {\tt 4/1} and {\tt 4/2}. Note that {\tt snpStats} will only deal with diallelic data and, although alleles are coded 1 to 4 in this file, only two of these occur with in any one locus. In fact these data are from the sample dataset distributed with the HAPLOVIEW program (Barrett et al., 2005) which uses the numbers 1--4 to denote the four nucleotides:  {\tt 1 =} A, {\tt 2 =} C, {\tt 3 = } G, {\tt 4 =} . The pedfile contains data for 20 loci on 120 subjects, and is accompanied by a second file which describes the loci, the first four lines being: \begin{verbatim} IGR1118a_1	274044 IGR1119a_1	274541 IGR1143a_1	286593 IGR1144a_1	287261 \end{verbatim} (this file is rather simple, containing just the locus name and its position on a chromosome).   The (gzipped) pedfile and the locus information file are stored in the  {\tt extdata} sub-directory of the {\tt snpStats} package as, respectively, {\tt sample.ped.gz} and {\tt sample.info}. Since the precise location of these files may vary between installations, we first obtain full paths  to these files  using the {\tt system.file} function <<sysfile>>= pedfile <- system.file(""extdata/sample.ped.gz"", package=""snpStats"") pedfile infofile <- system.file(""extdata/sample.info"", package=""snpStats"") @  The data can then be read in using the {\tt read.pedfile} function <<redpedfile>>= sample <- read.pedfile(pedfile, snps=infofile) @  The result, {\tt sample}, is a list with three elements. The first is an object of class {\tt SnpMatrix} containing the genotype data. We shall show summaries for the first few loci <<sample1>>= sample$genotypes col.summary(sample$genotypes)$MAF head(col.summary(sample$genotypes)) @  The second list element is a dataframe containing the first six fields of the pedfile. We'll just display the start of this: <<sample2>>= head(sample$fam) @  Note that the zero values in the pedfile have been read as {\tt NA}; this is optional, but default, behaviour of the function. Here the pedigree-member identifiers have been used as subject identifiers, since these are not duplicated while pedigree identifiers (the first choice) were duplicated (if both sets of identifiers are duplicated, they are combined). Finally, the third list element is a dataframe containing the information read from the {\tt sample.info} file, to which have been added the two alleles found at each locus: <<sample3>>= head(sample$map) @  Here we have used the default settings of {\tt read.pedfile}. In particular, it is not mandatory to supply a locus description file and there are further arguments which allow additional flexibility. These options are described in the on-line help page. \section*{PLINK files} Binary PED (BED) files written by the PLINK toolset (Purcell et al., 2007)  may also be read as {\tt SnpMatrix} objects. Files of type {\tt .bed} are written by the {\tt plink --make-bed} command and are accompanied by two text files: a {\tt .fam} file containing the first six fields of a standard pedfile as described above, and a {\tt .bim} file which describes the loci. The package data directory also  contains {\tt .bed}, {\tt .fam} and {\tt .bim} files for the sample dataset of the last section; the following commands recover the full file paths for these files and  read the files: <<plink>>= fam <- system.file(""extdata/sample.fam"", package=""snpStats"") bim <- system.file(""extdata/sample.bim"", package=""snpStats"") bed <- system.file(""extdata/sample.bed"", package=""snpStats"") sample <- read.plink(bed, bim, fam) @  The output object is similar to that produced by {\tt read.pedfile}, a list with three elements: <<plinkout>>= sample$genotypes col.summary(sample$genotypes)$MAF head(sample$fam) head(sample$map) @  Usually the three input files have the same filename stub with {\tt .bed}, {\tt .fam} and {\tt .bim} extensions added. In this case it is sufficient to just supply the filename stub to {\tt read.plink}.  A useful feature of {\tt read.plink} is the ability to select a subset of data from a large PLINK dataset. This is demonstrated in our small example below <<plinkselect>>= subset <- read.plink(bed, bim, fam, select.snps=6:10) subset$genotypes col.summary(subset$genotypes)$MAF subset$map @  Note that, in order to select certain SNPs, the input PLINK file must be in SNP-major order {\it i.e.\,} all individuals for the first SNP, all individuals for the second SNP, and so on. This is the default mode in PLINK. However, to select certain individuals, the input PLINK file must be in individual-major order. \section*{Long format data} The least compact, but perhaps most flexible, input format is the ``long'' format in which each genotype call takes up a single line. Such data can be read using the function {\tt read.snps.long}. A simple example is provided by the small gzipped data file {\tt   sample-long.gz} provided with the package: <<longfile>>= longfile <- system.file(""extdata/sample-long.gz"", package=""snpStats"") longfile @  The first 5 lines of the file are listed as follows: <<longlist>>= cat(readLines(longfile, 5), sep=""\n"") @  The first field gives the SNP identifier ({\tt snp1} to {\tt snp18}), the second gives the sample, or subject, identifier  ({\tt subject1} to  {\tt subject100}), the third field gives the genotype call ({\tt   1=A/A}, {\tt 2=A/B}, {\tt 3=B/B}), and the last field gives a confidence measure for the call (here always {\tt 1.000}). To read in this file and inspect the data: <<readlong>>= gdata <- read.long(longfile,     fields=c(snp=1, sample=2, genotype=3, confidence=4),    gcodes=c(""1"", ""2"", ""3""),     threshold=0.95) gdata summary(gdata) @  A few remarks: \begin{enumerate} \item In our example, the entire file has been read. However, subsets   of data may be extracted by specifying the required SNP or sample   identifiers. \item Any calls for which the call confidence is less than {\tt     threshold} is set to {\tt NA} (this did not affect any calls in   this simple example). \item Here, calls were represented by a single genotype code. It is   also possible to read calls as pairs of alleles. The function then   returns a list whose first argument is the {\tt SnpMatrix} object,   and whose second object is a dataframe containing the allele   codes. This option is demonstrated below, using an alternative   coding of the same data (all SNPs are {\tt CT} SNPs): \end{enumerate} <<readlongallele>>= allelesfile <- system.file(""extdata/sample-long-alleles.gz"", package=""snpStats"") cat(readLines(allelesfile, 5), sep=""\n"") gdata <- read.long(allelesfile,     fields=c(snp=1, sample=2, allele.A=3, allele.B=4, confidence=5),    threshold=0.95) gdata gdata$genotypes gdata$alleles @ Note that the assignment of alleles depends on the order in which they were encountered.  This function has many options and the online help page needs to be read carefully. \section*{Other formats} \subsection*{Imputation} A further source of input data is programs which can {\em impute} genotype data for a set of study individuals, using genome-wide SNP-chip data for the study subjects plus HapMap or 1,000 genomes project datasets. {\tt snpStats} provides the functions {\tt   read.beagle}, {\tt read.impute}, and {\tt read.mach} to read in files produced by the leading imputation programs. For more details of such data, see the imputation and meta-analysis vignette. \subsection*{VCF format} The 1,000 genomes data are released in the VCF format. {\tt snpStats} does not yet include a function to read data files in this format,  but the {\tt GGtools} package does contain such a function ({\tt vcf2sm}). \subsection*{X, Y and mitocondrial SNPs} The {\tt SnpMatrix} class is designed for diploid SNP genotypes. SNPs which can be haploid are stored in objects of the {\tt   XSnpMatrix} class, which has an addition slot, named {\tt diploid}. Since, for the X chromosome, ploidy depends on sex and may vary from row to row, this (logical) vector has the same number of elements as the number of rows in the SNP data matrix. Most input routines do not allow for reading an {\tt XSnpMatrix} and simply read into a {\tt   SnpMatrix}, coding haploid calls as (homozygous) diploid. Such objects may then be coerced into the {\tt XSnpMatrix} class using {\tt   as(\ldots, ""XSnpMatrix"")} or {\tt new(""XSnpMatrix, \ldots,   diploid=\ldots)}. If {\tt as} is used, ploidy is inferred from homozygosity while, if {\tt new} is used, it must be supplied (if all rows have the same ploidy, this argument can be a scalar). In either case, calls presumed to be haploid but coded as heterozygous will be set to {\tt NA}. \section*{Reference} Barrett JC, Fry B, Maller J, Daly MJ.(2005)  Haploview: analysis and visualization of LD and haplotype maps. {\it Bioinformatics}, 2005 Jan 15, [PubMed ID: 15297300]\\[2mm] Purcell S, Neale B, Todd-Brown K, Thomas L, Ferreira MAR,  Bender D, Maller J, Sklar P, de Bakker PIW, Daly MJ and Sham PC (2007)  PLINK: a toolset for whole-genome association and population-based  linkage analysis. {\it American Journal of Human Genetics}, {\bf 81} \end{document} ",12238
"25","snpStats","Microarray:SNP:GeneticVariability","\documentclass[12pt]{article} \usepackage{fullpage}   \usepackage[pdftex, bookmarks, bookmarksopen, pdfauthor={David Clayton}, pdftitle={snpMatrix-differences Vignette}] {hyperref}  \title{Differences between snpStats and snpMatrix} \author{David Clayton} \date{\today}  \usepackage{Sweave} \SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}  \begin{document} \setkeys{Gin}{width=1.0\textwidth}  %\VignetteIndexEntry{snpMatrix-differences} %\VignettePackage{snpStats}  \maketitle  \section*{The {\tt snpMatrix} and {\tt snpStats} packages} The package ``{\tt snpMatrix}'' was written to provide data classes and methods to facilitate the analysis of whole genome association studies in R. In the data classes it implements, each genotype call is stored as a single byte and, at this density, data for single chromosomes derived from large studies and new high-throughput gene chip platforms can be handled in memory by modern PCs and workstations. The object--oriented programming model introduced with version 4 of the S-plus package, usually termed ``S4 methods'' was used to implement these classes.  {\tt snpStats} initially arose out of the need to store, and analyse, SNP  genotype data in which subjects cannot be assigned to the three possible genotypes with certainty. This necessitated a change in the way in which data are stored internally, although {\tt snpStats} can still handle conventionally called  genotype data stored in the original {\tt snpMatrix} storage mode. {\tt snpStats} currently lacks some facilities which were present in {\tt snpMatrix} (although, hopefully, the important  gaps will  soon be filled) but it also includes several  new facilities. This vignette simply describes differences for users converting from the old {\tt snpMatrix} package.  \section*{Classes} Function names have, for the most part, remained unchanged so that existing analysis scripts will continue to work with minimal modification.  Initially it was hoped also to maintain the old class names since the classes were (mostly) backwards-compatible. But this proved troublesome and, in versions 1.1.4 and later, the class names have been changed (see Table).  \begin{table}[h]   \centering   \begin{tabular}{ll}     \hline     {\tt snpMatrix} class & {\tt snpStats} class\\     \hline     {\tt snp.matrix} & {\tt SnpMatrix}\\     {\tt X.snp.matrix} & {\tt XSnpMatrix}\\     {\tt single.snp.tests}& {\tt SingleSnpTests}\\     {\tt single.snp.tests.score}& {\tt SingleSnpTestsScore}\\     {\tt snp.tests.glm} & {\tt GlmTests}\\     {\tt snp.tests.glm.score} & {\tt GlmTestsScore}\\     {\tt snp.estimates.glm} & {\tt GlmEstimates}\\     {\tt imputation.rules}&{\tt ImputationRules}\\     \hline   \end{tabular}   \caption{Changes in class names} \end{table} Two functions have been provided to help users convert objects of a {\tt snpMatrix} class to the corresponding {\tt   snpStats} class: \begin{itemize} \item {\tt convert.snpMatrix}:  Converts a {\tt snpMatrix} object to   the corresponding {\tt snpStats} class \item {\tt convert.snpMatrix.dir}: Converts all saved {\tt snpMatrix}   objects in a given directory \end{itemize} \section*{Differences}  A major difference is that the basic class, now {\tt SnpMatrix},  supports uncertain genotypes, as generated by imputation programs. Two classes have been removed, namely the {\tt snp} and {\tt X.snp} classes. These were originally devised to support a loss of dimension of a {\tt snp.matrix} or {\tt X.snp.matrix} due to selection of a single row or column with {\tt drop=TRUE} in force in the selection operator {\tt[]}. However these classes were never fully satisfactory and were seldom used. In {\tt snpStats} the {\tt drop=} option is no longer allowed during row and column selection; dimensions are never dropped.  A word or warning, however: in the event that {\tt drop=} does occur in the selection operator, this will force the object to be regarded as a simple matrix of type {\tt raw}; this is the class that {\tt SnpMatrix} extends and this class does allow {\tt drop=}.   There has been a cosmetic, but important, change in the {\tt   XSnpMatrix} class as compared with its forerunner. The {\tt Female} slot has been renamed as {\tt diploid} to emphasize that this class is not only used for SNPs on the X chromosome, but for any SNP genotypes which may be haploid; this includes SNPs on the Y chromosome and mitocondrial SNPs.   The functions for computing pairwise linkage disequilibrium statistics have been replaced by a rewritten single function, {\tt ld}. The large band matrix which this function generates in one usage is stored using the {\tt dsCMatrix} class defined in the {\tt Matrix} package, (which is now required).  The function {\tt read.pedfile} has been rewritten, this time entirely in R. It has different arguments from the function of the same name in {\tt snpMatrix} and may be somewhat slower, but is somewhat more flexible.   The {\tt ImputationRules} class has changed  as a result of the introduction of the new storage convention for uncertain genotypes.  In the new coding, uncertainty of calls is represented by (grouped) posterior probabilities of assignment to the three genotypes. This change was necessary because one of the imputation methods of in {\tt snpMatrix}  only produced a posterior expectation of the genotype (when coded 0, 1 or 2)  and this could not be accomodated unambiguously in the extended coding.   The {\tt GlmTests} and {\tt GlmTestsScore} classes (formerly {\tt   snp.tests.glm} and {\tt snp.tests.glm.score}) have changed slightly in order to accomodate ongoing work on methods for multinomial and multivariate phenotypes. The {\tt test.names} slot has  been renamed as {\tt   snp.names} and its function has been changed slightly (although this should only affect more complicated uses of {\tt snp.rhs.tests}). A new slot, {\tt var.names} has been added; this holds the name of the variable(s) tested against SNPs. \end{document} ",5961
"26","snpStats","Microarray:SNP:GeneticVariability","\documentclass[12pt]{article} \usepackage{fullpage} \usepackage{graphicx}  \usepackage[pdftex, bookmarks, bookmarksopen, pdfauthor={David Clayton}, pdftitle={Fst calculations}] {hyperref}   \title{Fst\\The algorithm used in {\tt snpStats}} \author{David Clayton} \date{\today}  \usepackage{Sweave} \SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}  \begin{document} \setkeys{Gin}{width=1.0\textwidth}  %\VignetteIndexEntry{Fst} %\VignettePackage{snpStats}  \maketitle  <<lib,echo=FALSE>>= require(snpStats) @  \section*{$F$ statistics for diversity of groups} There is a very large literature on this topic and the author does not claim any great expertise. The purpose of this vignette is simply to document the  method of calculation implemented in {\tt snpStats}.  We shall start by introducing some notation. Let:\\[5mm] \begin{tabular}[h]{ll}   $N_g$ &  Number of chromosomes in group $g$\\   $N_{sg}$ & Number of chromsomes in group $g$ observed for SNP $s$\\   $N_s$ & Number of chromosomes observed for SNP $s$, all groups\\   $p_{sg}$ & Allele (relative) frequency for SNP $s$ in group $g$\\   $p_s$ & Overall allele frequency for SNP $s$\\ \end{tabular}\\[5mm] and let: \begin{eqnarray*}   Y_s &=& \frac{N_s}{N_s-1}p_s(1-p_s)\\   X_{sg} &=& \frac{N_{sg}}{N_{sg}-1} p_{sg}(1-p_{sg})\\   X_s &=& \sum_g W_g X_{sg} \end{eqnarray*} where $W_g$ are group-specific weights (see below).  The value returned for the F statistic for SNP $s$ is \[ F_s = 1 - \frac{X_s}{Y_s} = \frac{Y_s - X_s}{Y_s}. \]  A natural combined value over all SNPs is obtained by summing numerators and  denominators of the SNP-specific values Denoting summation by a $+$ subsscript, \[ F = \frac{Y_+ - X_+}{Y_+}, \] which can also be written as a weighted mean of the SNP-specific values,  with $Y_s$ as weights: \[ F = \frac{1}{\sum_s Y_s} \sum_s Y_s F_s. \]  There appear to be two ways of looking at this index, leading to different  weights when group sizes are unequal.   \subsection*{Pairwise differences} One rationale suggests that the index can be written  \[ \frac{D_T - D_W }{D_T } \] where $D_T$ is the probability that two chromosomes, sampled at random from the total population, differ and $D_W$ is the probability that two chromosomes, sampled at random from the same subpopulation, differ. For a single SNP, $s$, $Y_s$ is an unbiased estimator for $D_T$ and $X_{sg}$ is an unbiased estimator for $D_W$ {\em within subgroup $g$}.  With this rationale, the weights, $\{W_g\}$ should reflect the numbers of distinct pairwise comparisons within each group: \[ W_g = \frac{N_g (N_g-1)}{\sum_g N_g(N_g-1)} \] \subsection*{The analysis of variance} Another rationale would seem to be in terms of partition of the total variance, specifically the ratio of the between-group variance to the total variance (this seems to be the thrust of a series of papers by Cockerham). $Y_s$ is then an unbiased estimate of the total variance of SNP  $s$ and $X_{sg}$ is an unbiased estimator of its variance in group $g$. But  the natural weights are then \[ W_g =  \frac{N_g }{\sum_g N_g}. \] (Cockerham also seems to have considered an unweighted analysis but that could be inefficient if group sizes differ  substantially). \section*{An example} Here we show the results of these calculations using the HapMap data discussed  in other vignettes. These data were constructed by re-sampling individuals from two groups of HapMap subjects, the CEU sample (of European origin) and the JPT$+$CHB sample (of Asian origin), these groups being  identified by the variable {\tt stratum} in the subject support data frame.  We first use the pair-wise difference weights, first calculating the  SNP-specific values, followed by the weighted average across all SNPs: <<pairwise>>= data(for.exercise) f1 <- Fst(snps.10, subject.support$stratum, pairwise=TRUE) weighted.mean(f1$Fst, f1$weight) @ We now compare this result with that obtained with the alternative (AOV)  weights:  <<aov>>= f2 <- Fst(snps.10, subject.support$stratum, pairwise=FALSE) weighted.mean(f2$Fst, f2$weight) @ Here there is little difference between the two values, since the group sizes  are nearly the same: <<groupsize>>= table(subject.support$stratum) @ In other cases the two weighting schemes could lead to different answers. In  such situations, the preference of this author is for the analysis of variance  weights and, accordingly, this has been set as the default action.  \end{document} ",4429
"27","snpStats","Microarray:SNP:GeneticVariability","%\documentclass[a4paper,12pt]{article} \documentclass[12pt]{article} \usepackage{fullpage} % \usepackage{times} %\usepackage{mathptmx} %\renewcommand{\ttdefault}{cmtt} \usepackage{graphicx}  \usepackage[pdftex, bookmarks, bookmarksopen, pdfauthor={David Clayton}, pdftitle={Imputed SNP analyses with snpStats}] {hyperref}  \title{Imputed SNP analyses and meta-analysis with snpStats} \author{David Clayton} \date{\today}  \usepackage{Sweave} \SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}  \begin{document} \setkeys{Gin}{width=1.0\textwidth} %\VignetteIndexEntry{Imputation and meta-analysis} %\VignettePackage{snpStats}  \maketitle  % R code as %<<label[,fig=TRUE]>>= % %@   \section*{Getting started} The need for imputation in SNP analysis studies occurs when we have a smaller set of samples in which a large number of SNPs have been typed, and a larger set of samples typed in   only a subset of the SNPs.  We use the smaller, complete dataset (which will be termed the {\em training dataset}) to impute the missing SNPs in the larger, incomplete dataset (the {\em target dataset}). Examples of such applications  include: \begin{itemize} \item use of HapMap data to impute association tests for a large   number of SNPs, given data from genome-wide studies using, for   example, a 500K SNP array, and \item meta-analyses which seek to combine results from two platforms   such as the Affymetrix 500K and Illumina 550K platforms. \end{itemize} Here we will not use a real example such as the above to explore the use of {\tt snpStats} for imputation,  but generate a fictitious example using the data analysed in earlier exercises. This is particularly artificial in that we have seen that these data suffer from extreme heterogeneity of  population structure.   We start by attaching the required libraries and accessing the data used in the exercises:  <<init>>= library(snpStats) library(hexbin) data(for.exercise) @   We shall sample 200 subjects in our fictitious study as the training data set, select every second SNP to be missing  in the target dataset, and split the training set into two parts accordingly:  <<select>>=  training <- sample(1000, 200) select <- seq(1, ncol(snps.10),2)  missing <- snps.10[training, select] present <- snps.10[training, -select]  missing  present  @ Thus the training dataset consists of the objects {\tt missing} and {\tt present}. The target dataset holds a subset of the SNPs  for the remaining 800 subjects.   <<target>>= target <- snps.10[-training, -select] target @  But, in order to see how successful we have been with imputation, we will also save the SNPs we have removed from the target dataset <<>>= lost <- snps.10[-training, select] lost @  We also need to know where the SNPs are on the chromosome in order to avoid having to search the entire chromosome for suitable predictors of a missing SNP:  <<positions>>= pos.miss <- snp.support$position[select] pos.pres <- snp.support$position[-select] @   \section*{Calculating the imputation rules}  The next step is to calculate a set of rules which for imputing the {\tt missing} SNPs from the {\tt   present} SNPs. This is carried out by the function    {\tt snp.imputation}\footnote{Sometimes this command generates a     warning message concerning the maximum number of EM iterations. If this      only concerns a small proportion of the SNPs to be imputed it can be      ignored.}:  <<rules>>= rules <- snp.imputation(present, missing, pos.pres, pos.miss) @   This step executes remarkably quickly  when we consider what the function has done. For each of the \Sexpr{length(select)}  SNPs in the ``missing'' set, the function has performed a forward step-wise regression on the 50 nearest SNPs in the ``present'' set, stopping each search either when the $R^2$ for prediction exceeds 0.95, or after including 4 SNPs in the regression, or until $R^2$ is not improved by  at least 0.05. The figure 50 is the default value of the {\tt try} argument  of the function, while the values 0.95, 4 and 0.05 together make up the  default value of the {\tt stopping} argument. After the predictor, or ``tag'' SNPs have been chosen, the haplotypes of the target SNP plus tags was phased  and haplotype frequencies calculated using the EM algorithm. These  frequencies were then stored in the {\tt rules} object. \footnote{For imputation from small samples, some smoothing of these   haplotype frequencies would be advantageous and some ability to do   this has been included. The {\tt use.haps} argument to {\tt     snp.imputation} controls this. But invoking this option    slows down the algorithm and it is not   advised other than for very small sample sizes.}  A short listing of the first 10 rules follows: <<rule1>>= rules[1:10] @  The rules are also selectable by SNP name for detailed examination: <<rule2>>= rules[c('rs11253563', 'rs2379080')] @ Rules are shown with a {\tt +} symbol separating predictor SNPs. (It is important to know which SNPs were used for each imputation when checking imputed test results for artifacts.)  A summary table of all the 14,251 rules is generated by   <<summary>>= summary(rules) @   Columns represent the number of tag SNPs while rows represent grouping on $R^2$. The last column (headed {\tt <NA>}) represents SNPs for which an imputation rule could  not be computed, either because they were monomorphic or because there was insufficient data (as determined by the {\tt minA} optional argument in the call to {\tt snp.imputation}). The same information may be displayed graphically by  <<ruleplot,fig=TRUE>>= plot(rules) @    \section*{Carrying out the association tests}  The association tests for imputed SNPs can be carried out using the function {\tt single.snp.tests}.   <<imptest>>= imp <- single.snp.tests(cc, stratum, data=subject.support,                         snp.data=target, rules=rules) @   Using the observed data in the matrix {\tt target} and the set of imputation rules stored in {\tt rules}, the above command  imputes each of the imputed SNPs, carries out 1- and 2-df single locus tests for association,  returns the results in the object {\tt   imp}. To see how successful imputation has been, we can carry out the same tests using the {\em true} data in {\tt missing}:  <<realtest>>=  obs <- single.snp.tests(cc, stratum, data=subject.support, snp.data=lost) @   The next commands extract the $p$-values for the 1-df tests, using both the  imputed and the true ``missing'' data, and plot one against the other (using the {\tt hexbin} plotting package for clarity):  <<compare,fig=TRUE>>= logP.imp <- -log10(p.value(imp, df=1)) logP.obs <- -log10(p.value(obs, df=1)) hb <- hexbin(logP.obs, logP.imp, xbin=50) sp <- plot(hb) hexVP.abline(sp$plot.vp, 0, 1, col=""black"") @   As might be expected, the agreement is rather better if we only compare the results for SNPs that can be computed with high $R^2$. The $R^2$ value is extracted from the {\tt rules} object, using the function {\tt imputation.r2} and used to select a subset of rules:  <<best,fig=TRUE>>= use <- imputation.r2(rules)>0.9 hb <- hexbin(logP.obs[use], logP.imp[use], xbin=50) sp <- plot(hb) hexVP.abline(sp$plot.vp, 0, 1, col=""black"") @   Similarly, the function {\tt imputation.maf} can be used to extract the minor allele frequencies of the imputed SNP from the {\tt rules} object. Note that there is a tendency for SNPs with a high minor allele frequency to be imputed rather more successfully:  <<rsqmaf,fig=TRUE>>= hb <- hexbin(imputation.maf(rules), imputation.r2(rules), xbin=50) sp <- plot(hb) @   The function {\tt snp.rhs.glm} also allows testing imputed SNPs. In its simplest form, it can be used to calculate essentially the same tests as carried out with  {\tt single.snp.tests}\footnote{There is a small discrepancy, of the    order of $(N-1):N$ .} (although, being a more flexible function, this will run   somewhat slower). The next commands recalculate the 1 df tests for   the imputed SNPs using {\tt snp.rhs.tests}, and plot the results   against those obtained when values are observed. <<imptest-rhs,fig=TRUE>>= imp2 <- snp.rhs.tests(cc~strata(stratum), family=""binomial"",                        data=subject.support, snp.data=target, rules=rules) logP.imp2 <- -log10(p.value(imp2)) hb <- hexbin(logP.obs, logP.imp2, xbin=50) sp <- plot(hb) hexVP.abline(sp$plot.vp, 0, 1, col=""black"") @   \section*{Storing imputed genotypes}  In the previous two sections we have seen how to (a) generate imputation rules and, (b) carry out tests on SNPs imputed according to these rules, but without storing the imputed genotypes. It is also possible to store imputed SNPs in an object of class {\tt SnpMatrix} (or {\tt   XSnpMatrix}). The posterior probabilities of assignment of each individual to the three possible genotypes are stored within a one byte variable, although obviously not to full accuracy.   The following command imputes the ``missing'' SNPs using the ``target''  dataset and stores the imputed values in an object of class  {\tt SnpMatrix}: <<impstore>>= imputed <- impute.snps(rules, target, as.numeric=FALSE) @  (If {\tt as.numeric} were set to {\tt TRUE}, the default, the resulting object would be a simple numeric matrix containing posterior expectations of the 0, 1, 2 genotype.) A nice graphical description of how {\tt snpStats} stores uncertain genotypes is provided by the function {\tt plotUncertainty}. This plots the frequency of the stored posterior probabilities on an equilateral triangle. The posterior probabilities are represented by the perpendicular distances from each side, the vertices of the triangle corresponding to certain assignments. Thus, the SNP {\tt rs4880568} is accurately imputed ($R^2 = 0.94$) <<uncert1,fig=TRUE>>= plotUncertainty(imputed[, ""rs4880568""]) @  while {\tt rs2050968} is rather less so ($R^2 = 0.77$ <<uncert2,fig=TRUE>>= plotUncertainty(imputed[, ""rs2050968""]) @   Tests can be carried out on these uncertainly assigned genotypes. For example <<imptest2>>= imp3 <- single.snp.tests(cc, stratum, data=subject.support,                         snp.data=imputed, uncertain=TRUE) @  The {\tt uncertain=TRUE} argument ensures that uncertaing genotypes are used in the computations. This should yield nearly the same result  as before. For the first five SNPs we have  <<imp3>>= imp3[1:5] imp[1:5] @  There are small discrepancies due to the genotype assignment probabilities not being stored to full accuracy. However these should have little effect on power of the tests and no effect on the type~1 error rate.  Note that the ability of {\tt snpStats} to store imputed genotypes in this way allows alternative  programs to be used to generate the imputed genotypes. For example, the file ``mach1.out.mlprob.gz'' (which is stored in the {\tt extdata} sub-directory of  the {\tt snpStats} package) contains imputed SNPs generated by the MACH program, using the {\tt   --mle} and {\tt --mldetails} options. In the following commands, we find the full path to this file, read it,  and inspect one the imputed SNP in column~50:    <<mach,fig=TRUE>>= path <- system.file(""extdata/mach1.out.mlprob.gz"", package=""snpStats"") mach <- read.mach(path) plotUncertainty(mach[,50]) @  \section*{Meta-analysis} As stated at the beginning of this document, one of the main reasons that we need imputation is to perform meta-analyses which bring together data from genome-wide studies which use different platforms.  The {\tt snpStats} package includes a number of tools to facilitate this. All the tests implemented in {\tt snpStats} are ``score'' tests. In the 1 df case we calculate a score defined by  the first derivative of the log likelihood function with respect to the association parameter of interest at the parameter value corresponding to the null hypothesis of no association. Denote this by $U$. We also calculate an estimate of its variance,  also under the null hypothesis --- $V$ say. Then $U^2/V$ provides the chi-squared test on 1~df. This procedure extends easily to meta-analysis; given two independent studies of the same hypothesis, we simply add together the two values of $U$ and the two values of $V$, and then calculate $U^2/V$ as before. These ideas also extend naturally to tests of several parameters (2 or more df tests).  In {\tt snpStats}, the statistical testing functions can be called with the option {\tt score=TRUE}, causing an extended object to be saved. The extended object contains the $U$ and $V$ values, thus allowing later combination of the evidence from different studies. We shall first see what sort of object we have calculated previously using {\tt   single.snp.tests} {\em without} the {\tt score=TRUE} argument. <<class-imp-obs>>= class(imp) @  This object contains the imputed SNP tests in our target set. However, these SNPs were observed in our training set, so we can test them. We will also recalculate the imputed tests. In both cases we will save the score information: <<save-scores>>= obs <- single.snp.tests(cc, stratum, data=subject.support, snp.data=missing,                          score=TRUE) imp <- single.snp.tests(cc, stratum, data=subject.support,                         snp.data=target, rules=rules, score=TRUE) @ The extended objects have been returned: <<>>= class(obs) class(imp) @  These extended objects behave in the same way as the original objects, so that the same functions can be used to extract chi-squared values, $p$-values etc., but several additional functions, or methods, are now available. Chief amongst these is {\tt pool}, which combines evidence across independent studies as described at the beginning of this section. Although {\tt obs} and {\tt imp} are {\em not} from independent studies, so that the resulting test would not be valid, we can use them to demonstrate this: <<pool>>= both <- pool(obs, imp) class(both) both[1:5] @  Note that if we wished at some later stage to combine the results in {\tt both} with a further study, we would also need to specify {\tt score=TRUE} in the call to {\tt pool}: <<pool-score>>= both <- pool(obs, imp, score=TRUE) class(both) @  Another reason to save the score statistics is that this allows us to investigate the {\em direction} of findings. These can be extracted from the extended objects using the function {\tt effect.sign}. For example, this command tabulates the signs of the associations in {\tt obs}: <<sign>>= table(effect.sign(obs)) @  In this table, -1 corresponds to tests in which  effect sizes were negative (corresponding to an odds ratio less than one), while +1 indicates positive effect sizes (odds ratio greater than one). Zero sign indicates that  the effect was {\tt NA} (for example because the SNP was monomorphic).  Reversal of sign can be the explanation of a puzzling phenomenon when two studies give significant results individually, but no significant association when pooled. Although it is not impossible that such results are genuine, a more usual explanation is that the two alleles have been coded differently in the two studies: allele~1 in the first study is allele~2 in the second study and vice versa. To allow for this, {\tt snpStats} provides the {\tt switch.alleles} function, which reverses the coding of specified SNPs. It can be applied to {\tt   SnpMatrix} objects but, because allele switches are often discovered quite late on in the analysis and recoding the original data matrices could have unforeseen consequences, the {\tt   switch.alleles} function can also be applied to the extended test output objects. This modifies the saved scores {\em as if} the allele coding had been switched in the original data. The use of this is demonstrated below. <<switch>>= effect.sign(obs)[1:6] sw.obs <- switch.alleles(obs, 1:3) class(sw.obs) effect.sign(sw.obs)[1:6] @  \end{document}",15761
"28","snpStats","Microarray:SNP:GeneticVariability","%\documentclass[a4paper,12pt]{article} \documentclass[12pt]{article} \usepackage{fullpage} % \usepackage{times} %\usepackage{mathptmx} %\renewcommand{\ttdefault}{cmtt} \usepackage{graphicx}  \usepackage[pdftex, bookmarks, bookmarksopen, pdfauthor={David Clayton}, pdftitle={TDT and snpStats Vignette}] {hyperref}  \title{LD vignette\\Measures of linkage disequilibrium} \author{David Clayton} \date{\today}  \usepackage{Sweave}  \SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}  \begin{document} \setkeys{Gin}{width=1.0\textwidth}  %\VignetteIndexEntry{LD statistics} %\VignettePackage{snpStats}  <<lib,echo=FALSE>>= require(snpStats) require(hexbin) @   \maketitle  \section*{Calculating linkage disequilibrium statistics} We shall first load some illustrative data.  <<data>>= data(ld.example) @  The data are drawn from the International HapMap Project and concern 603 SNPs  over a 1mb region of chromosome 22 in sample of Europeans ({\tt ceph.1mb}) and a sample of Africans ({\tt yri.1mb}): <<showgt>>= ceph.1mb yri.1mb @  The details of these SNP are stored  in the dataframe {\tt support.ld}: <<showsp>>= head(support.ld) @  The function for calculating measures of linkage disequilibrium (LD) in {\tt snpStats} is {\tt ld}. The following two commands call this function to calculate the D-prime and R-squared measures of LD between pairs of SNPs for the European and African samples: <<ldstats>>= ld.ceph <- ld(ceph.1mb, stats=c(""D.prime"", ""R.squared""), depth=100) ld.yri <- ld(yri.1mb, stats=c(""D.prime"", ""R.squared""), depth=100) @  The argument {\tt depth} specifies the maximum separation between pairs of SNPs to be considered, so that {\tt depth=1} would have specified calculation of LD only between immediately adjacent SNPs.  Both {\tt ld.ceph} and {\tt ld.yri} are lists with two elements each, named {\tt D.prime} and {\tt R.squared}. These elements are (upper triangular) band matrices, stored in a packed form defined in the {\tt Matrix} package. They are too large to be listed, but the {\tt Matrix} package provides an {\tt image} method, a convenient way to examine patterns in the matrices. You should look at these carefully and note any differences.  <<image1,eval=FALSE>>= image(ld.ceph$D.prime, lwd=0) @  <<image1a,echo=FALSE,fig=TRUE>>= print(image(ld.ceph$D.prime, lwd=0)) @  <<image2,eval=FALSE>>= image(ld.yri$D.prime, lwd=0) @  <<image2a,echo=FALSE,fig=TRUE>>= print(image(ld.yri$D.prime, lwd=0)) @  The important things to note are  \begin{enumerate} \item there are fairly well-defined ``blocks'' of LD, and \item LD is more pronounced in the Europeans than in the Africans. \end{enumerate} The second point is demonstrated by extracting the D-prime values from the matrices (they are to be found in a slot named {\tt x}) and calculating quartiles of their distribution: <<quartiles>>= quantile(ld.ceph$D.prime@x, na.rm=TRUE) quantile(ld.yri$D.prime@x, na.rm=TRUE) @  If preferred, {\tt image} can produce colour plots. We first create a set of 10 colours ranging from yellow (for low values) to red (for high values)  <<colors>>= spectrum <- rainbow(10, start=0, end=1/6)[10:1] @  and plot the image, with a colour key down its right hand side <<imagecol,eval=FALSE>>= image(ld.ceph$D.prime, lwd=0, cuts=9, col.regions=spectrum, colorkey=TRUE) @  <<imagecola,echo=FALSE,fig=TRUE>>= print(image(ld.ceph$D.prime, lwd=0, cuts=9, col.regions=spectrum, colorkey=TRUE)) @   The R-squared matrices provide similar pictures, although they are rather less regular. To show this clearly, we focus on the 200 SNPs starting from the 75-th, using the European data <<use>>= use <- 75:274 @  <<image3,eval=FALSE>>= image(ld.ceph$D.prime[use,use], lwd=0) @  <<image3a,echo=FALSE,fig=TRUE>>= print(image(ld.ceph$D.prime[use,use], lwd=0)) @  <<image4,eval=FALSE>>= image(ld.ceph$R.squared[use,use], lwd=0) @  <<image4a,echo=FALSE,fig=TRUE>>= print(image(ld.ceph$R.squared[use,use], lwd=0)) @  The R-squared values are smaller and there are ``holes'' in the LD blocks;  SNPs within an LD block do not necessarily have large R-squared between them. This is further demonstrated in the next section. \section*{D-prime, R-squared, and distance} To examine the relationship between LD and physical distance, we first need to construct a similar matrix holding the physical distances. This is carried out, by first calculating each off-diagonal, and then combining them into a band matrix <<distance>>= pos <- support.ld$Position diags <- vector(""list"", 100) for (i in 1:100) diags[[i]] <- pos[(i+1):603] - pos[1:(603-i)] dist <- bandSparse(603, k=1:100, diagonals=diags) @  The values in the body of the band matrix are contained in a slot named {\tt x}, so the following commands extract the physical distances and the corresponding LD statistics for the Europeans: <<values>>= distance <- dist@x D.prime <- ld.ceph$D.prime@x R.squared <- ld.ceph$R.squared@x @  These are very long vectors so we use the {\tt hexbin} package to produce abreviated plots. We first demonstrate the relationship between D-prime and R-squared <<drplot,fig=TRUE>>= plot(hexbin(D.prime^2, R.squared)) @  We see that the square of D-prime provides an upper bound for R-squared; a high D-prime indicates the {\em potential} for two SNPs to be highly correlated, but they need not be. The following commands examine the relationship between the two LD measures and physical distance <<dpplot1,fig=TRUE>>= plot(hexbin(distance, D.prime, xbin=10)) @  <<dpplot2,fig=TRUE>>= plot(hexbin(distance, R.squared, xbin=10)) @  Although the data are very noisy, the first plot is consistent with an approximately exponential decline in mean D-prime with distance, as predicted by the Malecot model.  \section*{A view of the calculations} To understand the calculations let us consider the first and fifth SNPs in the Europeans. We shall first converting these to character data for legibility, and then tabulate the two-SNP genotypes, saving the $3\times 3$ table of genotype frequencies as {\tt tab33}: <<two>>= snp1 <- as(ceph.1mb[,1], ""character"") snp5 <- as(ceph.1mb[,5], ""character"") tab33 <- table(snp1, snp5) tab33 @  These two SNPs have a moderately high D-prime, but a very low R-squared: <<twoDR>>= ld.ceph$D.prime[1,5] ld.ceph$R.squared[1,5] @  The LD measures cannot be directly calculated from the $3\times 3$ table above, but from a $2\times 2$ table of {\em haplotype frequencies}. In only eight cells around the periphery of the table we can  unambiguously count haplotypes and these give us the following table of haplotype frequencies: \begin{center}   \begin{tabular}{crr}     & \multicolumn{2}{c}{\Sexpr{rownames(support.ld)[5]}}\\       \cline{2-3}       \Sexpr{rownames(support.ld)[1]} & A & B\\       \hline       A&        \Sexpr{2*tab33[1,1] + tab33[1,2] + tab33[2,1]}&       \Sexpr{2*tab33[1,3] + tab33[1,2] + tab33[2,3]}\\       B&       \Sexpr{2*tab33[3,1] + tab33[2,1] + tab33[3,2]}&       \Sexpr{2*tab33[3,3] + tab33[3,2] + tab33[2,3]}       \\       \hline          \end{tabular} \end{center} However, in the central cell of the $3\times 3$ table ({\it i.e.\,}    {\tt tab33[2,2]}) we have \Sexpr{tab33[2,2]} doubly heterozygous subjects, whose genotype could correspond either to the pair of haplotypes {\tt A-A/B-B} or to the pair of haplotypes {\tt A-B/B-A}. These are said to have {\em unknown   phase}. The expected split between these possible phases is determined by a further measure of LD --- the odds ratio. If the odds ratio is $\theta$, we expect a proportion $\theta/(1+\theta)$ of the doubly heterozygous subjects to be {\tt   A-A/B-B}, and a proportion  $1/(1+\theta)$ to be  {\tt A-B/B-A}.   We next use {\tt ld} to obtain an estimate of this odds ratio\footnote{Here {\tt ld} is called with two arguments of class   {\tt SnpStats} and, since only the odds ratio is to be calculated, it   returns the odds ratio rather than a list.} and, using this, we partition the doubly heterozygous individuals between the two possible phases:  <<digits,echo=FALSE>>= options(digits=4) @  <<OR>>= OR <- ld(ceph.1mb[,1], ceph.1mb[,5], stats=""OR"") OR AABB <- tab33[2,2]*OR/(1+OR) ABBA <- tab33[2,2]*1/(1+OR) AABB ABBA @  <<twoxtwo,echo=FALSE>>= a <- format(2*tab33[1,1] + tab33[1,2] + tab33[2,1] + AABB, digits=4) b <- format(2*tab33[1,3] + tab33[1,2] + tab33[2,3] + ABBA, digits=4) c <- format(2*tab33[3,1] + tab33[2,1] + tab33[3,2] + ABBA, digits=4) d <- format(2*tab33[3,3] + tab33[3,2] + tab33[2,3] + AABB, digits=4) @  We are now able to construct the table of haplotype frequencies: \begin{center}   \begin{tabular}{crr}     & \multicolumn{2}{c}{\Sexpr{rownames(support.ld)[5]}}\\       \cline{2-3}       \Sexpr{rownames(support.ld)[1]} & A & B\\       \hline       A& \Sexpr{a}& \Sexpr{b}\\       B& \Sexpr{c}& \Sexpr{d}       \\       \hline   \end{tabular} \end{center} It is easy to confirm that the odds ratio in this table,  $(\Sexpr{a}\times\Sexpr{d})/(\Sexpr{b}\times\Sexpr{c})$,  corresponds closely with that given by the {\tt ld} function.  Having obtained the $2\times 2$ table of haplotype frequencies,  any LD statistic may be calculated.   Of course, there is a circularity here; we needed to know the odds ratio in order to be able to construct the $2\times 2$ table from which it is calculated! That is why these calculations are not simple. The usual method involves iterative solution using an EM algorithm:  an initial guess at the odds ratio is used, as in the calculations above, to compute a new estimate, and these calculations  are repeated until the estimate stabilizes. However, in {\tt snpStats} the estimate is calculated in one step, by solving a cubic equation. \section*{The extent of LD around a point} Often we wish to guage how far LD extends from a given point (for example, from a SNP which is associated with disease incidence). For illustrative purposes we shall consider the region surroung the 168-th SNP, rs2385786. We first calculate D-prime values for the 100 SNPs on either side of rs2385786, and their positions: <<extent>>= lr100 <- c(68:167, 169:268) D.prime <- ld(ceph.1mb[,168], ceph.1mb[,lr100], stats=""D.prime"") where <- pos[lr100] @  We now plot D.prime against position, adding a simple smoother: <<eplot,fig=TRUE>>= plot(where, D.prime) lines(where, smooth(D.prime)) @  Although the data are somewhat noisy (the sample size is small), the region of LD is fairly clearly delineated. \section*{Selecting tag SNPs} Several ways have been suggested to select a set of ``tag'' SNPs which can be used to test for associations in a given region. That described below is based upon a heirarchical cluster analysis. We shall apply it to the region of high LD identified in the previous section, which lies between positions  $1.579\times 10^7$ and $1.587\times 10^7$.   The following commands identify which SNPs lie in this region, and extracts the relevant part of the $R^2$ matrix, as a symmetric matrix rather than as an upper triangular matrix. <<ldregion>>= use <- pos>=1.579e7 & pos<=1.587e7 r2 <- forceSymmetric(ld.ceph$R.squared[use, use]) @  The next step is to convert $(1-R^2)$ into a distance matrix, stored as required for the hierachical clustering function {\tt hclust}, and to carry out a complete linkage cluster analysis  <<dist>>= D <- as.dist(1-r2) hc <- hclust(D, method=""complete"") @  To plot the dendrogram, we must first adjust the character size for legibility: <<clplot,fig=TRUE>>= par(cex=0.5) plot(hc) @  The interpretation of this dendrogram is that, if we were to draw a horizontal line at a ``height'' of 0.5, then this would divide the SNPs into clusters in such a way that the value of $(1-R^2)$ between any pair of SNPs in a cluster would be no more than 0.5 (so that $R^2$ would be at least 0.5). This can be carried out using the {\tt cutree} function, which returns the cluster membership of each SNP: <<clusters>>= clusters <- cutree(hc, h=0.5) head(clusters) table(clusters) @  It can be seen that there are \Sexpr{max(clusters)} clusters.  To have a reasonable chance of picking up an association with the SNPs in this 80kb region, we would need to type a SNP from each one of these clusters. Of these, \Sexpr{sum(table(clusters)==1)} SNPs would only tag themselves!  A threshold $R^2$ of 0.5 might seem rather low. However, this is a ``worst case'' figure and most values of $R^2$ would be substantially better than this, particularly if an effort is made to choose tag SNPs which are in the center of clusters rather than on their edges. Also, this process has only considered tagging by single SNPs; it can be that two or more tag SNPs, taken together, can provide substantially better prediction than any one of them alone. \end{document}",12640
"29","snpStats","Microarray:SNP:GeneticVariability","\documentclass[12pt]{article} \usepackage{fullpage} % \usepackage{times} %\usepackage{mathptmx} %\renewcommand{\ttdefault}{cmtt} \usepackage{graphicx}  \usepackage[pdftex, bookmarks, bookmarksopen, pdfauthor={David Clayton}, pdftitle={PCA-snpStats Vignette}] {hyperref}  \title{PCA vignette\\Principal components analysis with snpStats} \author{David Clayton} \date{\today}  \usepackage{Sweave} \SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}  \newcommand{\tr}{^{\mbox{\scriptsize T}}}  \begin{document} \setkeys{Gin}{width=1.0\textwidth}  %\VignetteIndexEntry{Principal components analysis} %\VignettePackage{snpStats}  \maketitle  Principal components analysis has been widely used in population genetics in order to study population structure in genetically heterogeneous populations. More recently, it has been proposed as a method for dealing with the problem of confounding by population structure in genome-wide association studies.  \section*{The maths} Usually, principal components analysis is carried out by calculating the eigenvalues and eigenvectors of the correlation matrix. With $N$ cases and $P$ variables, if we write $X$ for the $N\times P$ matrix which has been standardised so that columns have zero mean and unit standard deviation, we find the eigenvalues and eigenvectors of the $P\times P$ matrix $X\tr.X$ (which is $N$ or $(N-1)$ times the correlation matrix depending on which denominator was used when calculating standard deviations). The first eigenvector gives the loadings of each variable in the first principal component, the second eigenvector gives the loadings in the second component, and so on. Writing the first $C$ component loadings as columns of the $P\times C$ matrix $B$, the $N\times C$ matrix of subjects' principal component scores, $S$, is obtained by applying the factor loadings to the original data matrix, {\it i.e.\,}  $S = X.B$.  The sum of squares and products matrix, $S\tr.S = D $, is diagonal with elements equal to the first $C$   eigenvalues of the  $X\tr.X$ matrix, so that the variances of the principal components  can obtained by dividing the eigenvalues by  $N$ or $(N-1)$.  This standard method is rarely feasible for genome-wide data since $P$ is very large indeed and calculating the eigenvectors of $X\tr.X$ becomes impossibly onerous. However, the calculations can also be carried out by calculating the eigenvalues and eigenvectors of the $N\times N$ matrix $X.X\tr$.  The (non-zero) eigenvalues of this matrix are the same as those of $X\tr.X$, and its eigenvectors are proportional to the principal component scores defined above; writing the first $C$ eigenvectors of $X.X\tr$ as  the columns of the $N\times C$ matrix, $U$, then $U =  S.D^{-1/2}$. Since for many purposes we are not too concerned about the scaling of the principal components, it will often be acceptable to use the eigenvectors, $U$, in place of the more conventionally scaled principal components. However some attention should be paid to the corresponding eigenvalues since, as noted above, these are proportional to the variances of the conventional principle components. The factor loadings may be calculated by $B = X\tr.U.D^{-1/2}$.  Using this method of calculation, it is only (!) necessary to find the eigenvalues and eigenvectors of an $N\times N$ matrix. Current microarray-based genotyping studies are such that $N$ is typically a few thousands while $P$ may be in excess of one million.  \section*{An example}  In this exercise, we shall calculate principal component loadings in controls alone and then apply these loading to the whole data. This is more complicated than the simpler procedure of calculating principal components in the entire dataset but avoids component loadings which unduly reflect case/control differences; using such components to correct for population structure would seriously reduce the power to detect association since one would, to some extent, be ``correcting'' for case/control differences\footnote{An alternative approach is to   standardise the $X$ matrix so that each column has zero mean in   both cases and controls. This can be achieved by using the {\tt     strata} argument in the call to {\tt xxt}. Here, however, we have   used controls only since this reduces the size of the matrix for   the eigenvalue and vector calculations. }. We will also ``thin'' the data by taking only every tenth SNP. We do this mainly to reduce computation time but thinning is often employed to minimize the impact of linkage disequilibrium (LD), to reduce the risk that the larger  components may simply reflect unusually long stretches of LD rather than population structure. Of course, this would require a more sophisticated approach to thinning than that used in this demonstration.  In a more sophisticated approach, one might use the output of {\tt snp.imputation} to eliminate all but one of a groups of SNPs in strong LD for thinning.  We shall use the data introduced in the main vignette. We shall first load the data and extract the controls.  <<get-data>>= require(snpStats) data(for.exercise) controls <- rownames(subject.support)[subject.support$cc==0] use <- seq(1, ncol(snps.10), 10) ctl.10 <- snps.10[controls,use] @  The next step is to standardize the data to zero mean and unit standard deviation and to calculate the $X.X\tr$ matrix. These operations are carried out using the function {\tt xxt}. <<xxt-matrix>>= xxmat <- xxt(ctl.10, correct.for.missing=FALSE) @  The argument {\tt correct.for.missing=FALSE} selects a very simple missing data treatment, {\it i.e.\,} replacing missing values by their mean. This is quite adequate when the proportion of missing data is low. The default method is more sophisticated but introduces complications later so we will keep it simple.  When performing a genome-wide analysis, it will usually be the case that all the data cannot be stored    in a single {\tt SnpMatrix} object. Usually they will be organized with one matrix for each chromosome. In these cases, it is straightforward to write a script which carries out the above calculations for each chromosome in turn, saving the resultant matrix to disk each time. When all chromosomes have been processed, the $X.X\tr$ matrices are read and added together.  The next step of the calculations requires us to calculate the eigenvalues and eigenvectors of the $X.X\tr$ matrix. This can be carried out using a standard R function. We will save the first five components.  <<eigen>>= evv <- eigen(xxmat, symmetric=TRUE) pcs <- evv$vectors[,1:5] evals <- evv$values[1:5] evals @  Here, {\tt pcs} refers to the scaled principal components ({\it i.e.\,} to the columns of the matrix $U$ in our mathematical introduction) and all have the same variance. The eigenvalues give an idea of the relative magnitude of these sources of variation. The first principal component has a markedly larger eigenvalue and we might hope that this reflects population structure. In fact these data were drawn from two very different populations, as indicated by the {\tt stratum} variable in the subject support frame. The next set of commands extract this variable for the controls and plot box plots for the first two components by stratum. <<pc-one,fig=TRUE>>= pop <- subject.support[controls,""stratum""] par(mfrow=c(1,2)) boxplot(pcs[,1]~pop) boxplot(pcs[,2]~pop) @  Clearly the first component has captured the difference between the populations. Equally clearly, the second principal component has not.  The next step in the calculation is to obtain the SNP loadings in the components. This requires calculation of $B = X\tr.S.D^{-1/2}$. Here we calculate the transpose of this matrix, $B\tr = D^{-1/2}S\tr.X$, using the special function {\tt snp.pre.multiply} which pre-multiplies a {\tt   SnpMatrix} object by a matrix after first standardizing it to zero mean and unit standard deviation.  <<pre-multiply>>= btr <- snp.pre.multiply(ctl.10, diag(1/sqrt(evals)) %*% t(pcs)) @  We can now apply these loadings back to the entire dataset (cases as well as controls) to derive scores that we can use to correct for population structure. To do that we use the function {\tt snp.post.multiply} which post-multiplies a {\tt SnpMatrix} by a general matrix, after first standardizing the columns of the {\tt SnpMatrix}  to zero mean and unit standard deviation. Note that it is first necessary to select out those SNPs that we have actually used in the calculation of components. <<post-multiply>>= pcs <- snp.post.multiply(snps.10[,use], t(btr)) @   Finally we shall evaluate how successful the first principal component is in correcting for population structure effects.  ({\tt snp.rhs.tests} return {\tt glm} objects.) <<testing,fig=TRUE>>= cc <- subject.support$cc uncorrected <- single.snp.tests(cc, snp.data=snps.10) corrected <- snp.rhs.tests(cc~pcs[,1], snp.data=snps.10) par(mfrow=c(1,2),cex.sub=0.85) qq.chisq(chi.squared(uncorrected,1), df=1) qq.chisq(chi.squared(corrected), df=1) @  The use of the first principal component as a covariate has been quite successful in reducing the serious over-dispersion due to population structure. Indeed it is just as successful as stratification by the observed {\tt stratum} variable. \subsection*{Acknowledgement} Thanks to David Poznik for pointing out an error in the earlier versions of this vignette. \end{document}   ",9344
"30","snpStats","Microarray:SNP:GeneticVariability","%\documentclass[a4paper,12pt]{article} \documentclass[12pt]{article} \usepackage{fullpage} % \usepackage{times} %\usepackage{mathptmx} %\renewcommand{\ttdefault}{cmtt} \usepackage{graphicx}  \usepackage[pdftex, bookmarks, bookmarksopen, pdfauthor={David Clayton and Chris Wallace}, pdftitle={snpStats Vignette}] {hyperref}  \title{snpStats vignette\\Example of genome-wide association testing} \author{David Clayton and Chris Wallace} \date{\today}  \usepackage{Sweave} \SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}  \begin{document} \setkeys{Gin}{width=1.0\textwidth}  %\VignetteIndexEntry{snpStats introduction} %\VignettePackage{snpStats}  \maketitle  \section*{The {\tt snpMatrix} and {\tt snpStats} packages} The package ``{\tt snpMatrix}'' was written to provide data classes and methods to facilitate the analysis of whole genome association studies in R. In the data classes it implements, each genotype call is stored as a single byte and, at this density, data for single chromosomes derived from large studies and new high-throughput gene chip platforms can be handled in memory by modern PCs and workstations. The object--oriented programming model introduced with version 4 of the S-plus package, usually termed ``S4 methods'' was used to implement these classes.  {\tt snpStats} arose out of the need to store, and analyse, SNP  genotype data in which subjects cannot be assigned to the three possible genotypes with certainty. This necessitated a change in the way in which data are stored internally, although {\tt snpStats} can still handle conventionally called  genotype data stored in the original {\tt snpMatrix} storage mode. {\tt snpStats} currently lacks some facilities which were present in {\tt snpMatrix} (although, hopefully, the important  gaps will  soon be filled) but it also includes several important new facilities. This vignette currently exploits none of the new facilities; these are mainly used in the vignette which deals with imputation and meta-analysis.   For population-based studies, both quantitative and qualitative phenotypes may be analysed but, at present, rather more limited facilities are available for family--based studies.  Flexible functions are provided which can carry out single SNP tests which control for potential confounding by quantitative and qualitative covariates. Tests involving several SNPs taken together as ``tags'' are also supported.  The original {\tt snpMatrix} package was described by Clayton and Leung  (2007) {\it Human Heredity}, {\bf 64}: 45--51. Since this publication many new facilities have been introduced; some of  these are explored in further vignettes.   \section*{Getting started} We shall start by loading  the  the packages and the data to be used in the first part of this exercise, which concerns a population--based case--control study:  <<init>>= require(snpStats) require(hexbin) data(for.exercise) @   In addition to the {\tt snpStats} package, we have also loaded the {\tt hexbin} package which reduces file sizes and legibility of plots with very many data points.  The data have been created artificially from publicly available datasets. The SNPs have been selected from those genotyped by the International HapMap Project\footnote{\tt http://www.hapmap.org} to represent the typical density found on a whole genome association chip, (the Affymetrix 500K platform\footnote{\tt   http://www.affymetrix.com/support/technical/sample\_data/500k\_hapmap\_genotype\_data.affx}) for a moderately sized chromosome (chromosome 10). A (rather too) small study of 500 cases and 500 controls has been simulated allowing for recombination using beta software from Su and Marchini.  Re-sampling of cases was weighted in such a way as to simulate three ``causal'' locus on this chromosome, with multiplicative effects of 1.3, 1.4 and 1.5 for each copy of the risk allele at each locus. It should be noted that this is a somewhat optimistic scenario!  You have loaded three objects: \begin{enumerate} \item {\tt snps.10}, an object of class ``{\tt SnpMatrix}''   containing a matrix of SNP genotype calls. Rows of the matrix   correspond to subjects and columns correspond to SNPs: <<>>= show(snps.10) @   \item {\tt snp.support}, a conventional R data frame containing information about the SNPs typed. To see its contents: <<>>= summary(snp.support) @  Row names of this data frame correspond with column names of {\tt   snps.10} and comprise the (unique) SNP identifiers. \item {\tt subject.support}, another conventional R data frame   containing further   information about the subjects. The row names coincide with the row   names of {\tt snps.10} and   comprise the (unique) subject identifiers. In this simulated study   there are only two variables: <<>>= summary(subject.support) @  The variable {\tt cc} identifies cases ({\tt cc=1}) and controls ({\tt cc=0}) while {\tt stratum}, coded 1 or 2, identifies a stratification of the study population --- more on this later. \end{enumerate} In general, analysis of a whole--genome association study will require a subject support data frame, a SNP support data frame for each chromosome, and a SNP data file for each chromosome\footnote{ Support files are usually read in with general tools such as {\tt   read.table}. The {\tt snpStats} package contains a number of tools for reading SNP genotype data into an object of class ``{\tt   SnpMatrix}''.}.  A short summary of the contents of {\tt snps.10} is provided by the {\tt summary} function. This operation actually produces two ``summaries of summaries''. First, summary  statistics are calculated for each row (sample), and their results summarised. Then summary statistics are calculated for each column (SNP) and their results summarised. <<>>= summary(snps.10) @  The row-wise and column-wise summaries are calculated with the functions {\tt row.summary} and {\tt col.summary}. For example, to calculate summary statistics for each SNP (column): <<>>= snpsum <- col.summary(snps.10) summary(snpsum) @ The second command duplicates the latter part of the result of {\tt   summary(snps.10)}, and the contents of {\tt snpsum} are fairly self-explanatory.  We could look at a couple of summary statistics in more detail: <<plot-snpsum,fig=TRUE>>= par(mfrow = c(1, 2)) hist(snpsum$MAF) hist(snpsum$z.HWE) @  The latter should represent a $z$-statistic. {\it i.e.} a statistic normally distributed with mean zero and unit standard deviation under the hypothesis of Hardy--Weinberg equilibrium (HWE). Quite clearly there is extreme deviation from HWE, but this can be accounted for by the manner in which this synthetic dataset was created.  The function {\tt row.summary} is useful for detecting samples that have genotyped poorly. This calculates call rate and mean heterozygosity across all SNPs for each subject in turn: <<sample-qc>>= sample.qc <- row.summary(snps.10) summary(sample.qc) @ (note that the last command yields the same as the first part of  {\tt summary(snps.10)}). The plot of heterozygosity against call rate is useful in detecting poor quality samples: <<plot-outliners-qc,fig=TRUE>>= par(mfrow = c(1, 1)) plot(sample.qc) @  There is one clear outlier.  \section*{The analysis} We'll start by removing the `outlying' sample above (the sample with  Heterozygosity near zero): <<outliers>>= use <- sample.qc$Heterozygosity>0 snps.10 <- snps.10[use, ] subject.support <- subject.support[use, ] @  Then we'll see if there is any difference between call rates for cases and controls. First generate logical arrays for selecting out cases or controls:\footnote{ These commands assume that the subject support frame has the same number of rows as the SNP matrix and that they are in the same order. Otherwise a slightly more complicated derivation is necessary.} <<if-case-control>>= if.case <- subject.support$cc == 1 if.control <- subject.support$cc == 0 @ Now we recompute the genotype column summaries separately for cases and controls: <<sum-case-control>>= sum.cases <- col.summary(snps.10[if.case, ]) sum.controls <- col.summary(snps.10[if.control, ]) @ and plot the call rates, using hexagonal binning and  superimposing a line of slope 1 through the origin: <<plot-summaries,fig=TRUE>>= hb <- hexbin(sum.controls$Call.rate, sum.cases$Call.rate, xbin=50) sp <- plot(hb) hexVP.abline(sp$plot.vp, 0, 1, col=""black"") @ There is no obvious difference in call rates. This is not a surprise, since  no such difference was built into the simulation. In the same way we could look for differences between allele frequencies, superimposing a line of slope 1 through the origin: <<plot-freqs,fig=TRUE>>= sp <- plot(hexbin(sum.controls$MAF, sum.cases$MAF, xbin=50)) hexVP.abline(sp$plot.vp, 0, 1, col=""white"") @  This is not a very effective way to look for associations, but if the SNP calling algorithm has been run separately for cases and controls this plot can be a useful diagnostic for things going wrong ({\it e.g.} different labelling of clusters).  It should be stressed that, for real data, the plots described above would usually have many more outliers. Our simulation did not model the various biases and genotype failures that affect real studies.  The fastest tool for carrying out simple tests for association taking the SNP one at a time is {\tt single.snp.tests}. The output from this function is a data frame with one line of data for each SNP. Running this in our data and summarising the results: <<tests>>= tests <- single.snp.tests(cc, data = subject.support, snp.data = snps.10) @ Some words of explanation are required. In the call, the {\tt   snp.data=} argument is mandatory and provides the name of the matrix providing the genotype data. The {\tt data=} argument gives the name of the data frame that contains the remaining arguments --- usually the subject support data frame\footnote{This is not mandatory --- we could have made {\tt cc} available in the global environment. However we would then have to be careful that the values are in the right order; by specifying the data frame, order is forced to be correct by checking the order of the row names for the {\tt data} and {\tt   snp.data} arguments.}.  Let us now see what has been calculated: <<sum-tests>>= summary(tests) @  We have, for each SNP,  chi-squared tests on 1 and 2 degrees of freedom (df), together with $N$, the number of subjects for whom data were available. The  1 df test is the familiar Cochran-Armitage test for codominant effect  while the 2 df test is the conventional Pearsonian test for the  $3\times 2$ contingency table. The large number of {\tt NA} values  for the latter test reflects the fact that, for these SNPs, the minor  allele frequency was such that one homozygous genotype did not occur  in the data.  We will probably wish to restrict our attention to SNPs that pass certain criteria. For example <<use>>= use <- snpsum$MAF > 0.01 & snpsum$z.HWE^2 < 200 @ (The Hardy-Weinberg filter is ridiculous and reflects the strange characteristics of these simulated data. In real life you might want to use something like 16, equivalent to a 4SE cut-off). To see how many SNPs pass this filter <<sum-use>>= sum(use) @ We will now throw way the discarded test results and save the positions of the remaining SNPs <<subset-tests>>= tests <- tests[use] position <- snp.support[use, ""position""] @  We now calculate $p$-values  for the Cochran-Armitage tests and plot minus logs (base 10) of the $p$-values against position <<plot-tests,fig=TRUE>>= p1 <- p.value(tests, df=1) plot(hexbin(position, -log10(p1), xbin=50)) @  Clearly there are far too many ``significant'' results, an impression which is made even clearer by the quantile-quantile (QQ) plot: <<qqplot,fig=TRUE>>= chi2 <- chi.squared(tests, df=1) qq.chisq(chi2,  df = 1) @   The three numbers returned by this command are the number of tests considered, the number of outliers falling beyond the plot boundary, and the slope of a line fitted to the smallest 90\% of values ({\it i.e.} the multiple by which the chi-squared test statistics are over-dispersed).   The ``concentration band'' for the plot is shown in grey. This region is defined by upper and lower probability bounds for each order statistic.  The default is to use the 2.5\% and 95.7\% bounds\footnote{Note that this is not a simultaneous confidence region; the probability that the plot will stray outside the band at some point exceeds 95\%.}.  This over-dispersion of chi-squared values was built into our simulation. The data were constructed by re-sampling individuals from {\em two} groups of HapMap subjects, the CEU sample (of European origin) and the JPT$+$CHB sample (of Asian origin). The 55\% of the cases were of European ancestry as compared with  only 45\% of the controls. We can deal with this by stratification of the tests, achieved by adding the {\tt stratum} argument to the call to {\tt   single.snp.tests} (the remaining commands are as before) <<more-tests,fig=TRUE>>= tests <- single.snp.tests(cc, stratum, data = subject.support,      snp.data = snps.10) tests <- tests[use] p1 <- p.value(tests, df = 1) plot(hexbin(position, -log10(p1), xbin=50)) @ <<more-tests-qq,fig=TRUE>>= chi2 <- chi.squared(tests, df=1) qq.chisq(chi2, df = 1) @   Most of the over-dispersion of test statistics has been removed (the residual is probably due to ``cryptic relatedness'' owing to the way in which the data were simulated).  Now let us find the names and positions of the most significant 10 SNPs. The first step is to compute an array which gives the positions in which the first, second, third etc. can be found <<ord>>= ord <- order(p1) top10 <- ord[1:10] top10 @   We now list the 1~df $p$-values, the corresponding SNP names and their positions on the chromosome: <<top-10>>= names <- tests@snp.names p1[top10] names[top10] position[top10] @   The most associated SNPs lie within two small regions of the genome. To concentrate on the rightmost region (the most associated region on the left contains just one SNP), we'll first sort the names of the SNPs into position order along the chromosome and select those lying in the region approximately one mega-base either side of the second most associated SNP: <<top10-local>>= posord <- order(position) position <- position[posord] names <- names[posord] local <- names[position > 9.6e+07 & position < 9.8e+07] @ The variable {\tt posord} now contains the permutation necessary to sort SNPs into position order and {\tt names} and {\tt position} have now been reordered in this manner. The variable {\tt local} contains the names of the SNPs in the selected 2 mega-base region.  Next we shall estimate the size of the effect at the most associated SNPs for each region (rs870041, rs10882596). In the following commands, we extract each SNP from the matrix as a numerical variable (coded 0, 1, or 2) and then, using the {\tt glm} function, carry out a logistic regression of case--control status on this numerical coding of the SNP and upon stratum. The variable {\tt stratum} must be included in the regression in order to allow for the different population structure of cases and controls.  We first make copies of the {\tt cc} and {\tt stratum} variables in {\tt subject.support} in the current working environment (where the other variables reside): <<top1>>= cc <- subject.support$cc stratum <- subject.support$stratum top <- as(snps.10[, ""rs870041""], ""numeric"") glm(cc ~ top + stratum, family = ""binomial"") @ The coefficient of {\tt top} in this regression is estimated as 0.5100, equivalent to a relative risk of $\exp(.5100) =1.665$.   For the other top SNP we have: <<top2>>= top2 <- as(snps.10[, ""rs10882596""], ""numeric"") fit <- glm(cc ~ top2 + stratum, family = ""binomial"") summary(fit) @ This relative risk is $\exp(0.4575)=1.580$. Both estimates are close to the values used to simulate the data.  You might like to repeat the analysis above using the 2 df tests. The conclusion would have been much the same. A word of caution however; with real data the 2 df test is less robust against artifacts due to genotyping error. On the other hand, it is much more powerful against recessive or near-recessive variants.  The {\tt snpStats} package includes its own functions to fit generalized linear models. These are much faster than {\tt glm}, although not yet as flexible. They allow for a each of series of SNPs to be entered into a GLM, either on the left hand side ({\it i.e.} as the dependent variable) or on the right-hand side (as a predictor variable). In the latter case seveal SNPs can be entered in each model fit.  For example, to fit the same GLM as before, in which each SNP is entered in turn on the right-hand side of a logistic regression equation,  for each of the SNPs in the 2 megabase ``local'' region: <<estimates>>= localest <- snp.rhs.estimates(cc~stratum, family=""binomial"", sets=local,                               data=subject.support, snp.data=snps.10) @  This function call has computed \Sexpr{length(local)} GLM fits!  The parameter estimates for the first five, and for the second best SNP analyzed above (rs10882596) are shown by <<list-estimates>>= localest[1:5] localest[""rs10882596""] @  The parameter estimate for rs1088259 and its standard error agree closely  with the values obtained earlier, using the {\tt glm} function.   The GLM code within {\tt snpStats} allows a further speed-up which is not available in the standard {\tt glm} function. If a variable is to be included in the model as a ``factor'' taking many levels then a more efficient algorithm can be invoked  by using the {\tt strata} function in the model formula. For example, the following command fits the same model for all the \Sexpr{sum(use)}  SNPs we have decided to use in these analyses: <<fast-estimates>>= allest <- snp.rhs.estimates(cc~strata(stratum), family=""binomial"", sets=use,                               data=subject.support, snp.data=snps.10) length(allest) @  As expected, the parameter estimates and standard errors  are unchanged, for example: <<check-estimates>>= allest[""rs10882596""] @  Note that {\tt strata()} can only be used once in a model formula. \section*{Multi-locus tests} There are two other functions  for carrying out association tests ({\tt snp.lhs.tests} and {\tt snp.rhs.tests}) in the package. These are somewhat slower, but much more flexible. For example, the former function allows one to test for  differences in allele frequencies between more than two groups. An important use of the latter function is to carry out tests using {\em groups} of SNPs rather than single SNPs. We shall explore this use in the final part of the exercise.  A prerequisite to multi-locus analyses is to decide on how SNPs should be grouped in order to ``tag'' the genome rather more completely than by use of single markers. Hopefully, the {\tt   snpMatrix} package will eventually contain tools to compute such groups, for example, by using HapMap data. The function {\tt   ld.snp}, which we encountered earlier, will be an essential tool in this process. However this work is not complete and, for now, we demonstrate the testing tool by grouping the  \Sexpr{sum(use)} SNPs we have decided to use into 20kB blocks. The following commands compute such a grouping, tabulate the block size, and remove empty blocks: <<blocks>>= blocks <- split(posord, cut(position, seq(100000, 135300000, 20000))) bsize <- sapply(blocks, length) table(bsize) blocks <- blocks[bsize>0]   @ You can check that this has worked by listing the column positions  of the first 20 SNPs together with the those contained in the first five blocks <<twentyfive>>= posord[1:20] blocks[1:5] @   Note that these positions refer to the reduced set of SNPs after application of the filter on MAF and HWE. Therefore, before proceeding further we create a new matrix of SNP genotypes containing only these 27,828: <<blocks-use>>= snps.use <- snps.10[, use] remove(snps.10) @  The command to carry out the tests on these groups, controlling for the known population structure differences is <<mtests,keep.source=TRUE>>= mtests <- snp.rhs.tests(cc ~ stratum, family = ""binomial"",       data = subject.support, snp.data = snps.use, tests = blocks) summary(mtests) @   The first argument, together with the second, specifies  the model which corresponds to the null hypothesis. In this case we have allowed for the variation in ethnic origin ({\tt stratum}) between cases and controls. We complete the analysis by extracting  the $p$--values and plotting minus their logs (base 10): <<plot-mtests,fig=TRUE>>= pm <- p.value(mtests) plot(hexbin(-log10(pm), xbin=50)) @  The same associated region is picked out, albeit with a rather larger $p$-value; in this case the multiple df test cannot be powerful as the 1 df test since the simulation ensured that the ``causal'' locus was actually one of the SNPs typed on the Affymetrix platform. QQ plots are somewhat more difficult since the tests are on differing degrees of freedom. This difficulty is neatly circumvented by noting that, under the null hypothesis,  $-2\log p$ is distributed as chi-squared on 2~df: <<qqplot-mtests,fig=TRUE>>= qq.chisq(-2 * log(pm), df = 2) @ \end{document} ",21154
"31","snpStats","Microarray:SNP:GeneticVariability","%\documentclass[a4paper,12pt]{article} \documentclass[12pt]{article} \usepackage{fullpage} % \usepackage{times} %\usepackage{mathptmx} %\renewcommand{\ttdefault}{cmtt} \usepackage{graphicx}  \usepackage[pdftex, bookmarks, bookmarksopen, pdfauthor={David Clayton}, pdftitle={TDT and snpStats Vignette}] {hyperref}  \title{TDT vignette\\Use of snpStats in family--based studies} \author{David Clayton} \date{\today}  \usepackage{Sweave} \SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}  \begin{document} \setkeys{Gin}{width=1.0\textwidth}  %\VignetteIndexEntry{TDT tests} %\VignettePackage{snpStats}  \maketitle   \section*{Pedigree data}  The {\tt snpStats} package contains some tools for analysis of family-based studies. These assume that a subject support file provides the information necessary to reconstruct pedigrees in the well-known format used in the {\it LINKAGE} package. Each line of the support file  must contain an identifier of the {\em pedigree} to which the individual belongs, together with an identifier of subject within pedigree, and the within-pedigree identifiers for the subject's father and mother. Usually this information, together with phenotype data, will be contained in a dataframe with rownames which link to the rownames of the {\tt SnpMatrix} containing the genotype data. The following commands read some illustrative data on 3,017 subjects and 43 (autosomal) SNPs\footnote{These data are on a much smaller scale than   would arise in genome-wide studies, but serve to illustrate the   available tools. Note, however, that execution speeds are quite adequate for   genome-wide data.}. The data consist of a dataframe containing the subject and pedigree information ({\tt pedData}) and a {\tt   SnpMatrix} containing the genotype data ({\tt genotypes}): <<family-data>>= require(snpStats) data(families) genotypes head(pedData) @  The first family comprises four individuals: two parents and two sibling offspring. The parents are ``founders'' in the pedigree, {\it   i.e.}  there is no data for their parents, so that their {\tt father} and {\tt mother} identifiers are set to {\tt NA}. This differs from the convention in the {\it LINKAGE} package, which would code these as zero. Otherwise coding is as in {\it LINKAGE}: {\tt sex} is coded 1 for   male and 2 for female, and disease status ({\tt affected}) is coded   1 for unaffected and 2 for affected.    \section*{Checking for mis-inheritances}  The function {\tt misinherits} counts non-Mendelian inheritances in the data. It returns a logical matrix with one row for each subject who has any mis-inheritances and one column for each SNP which was ever mis-inherited.  <<mis-inheritances>>= mis <- misinherits(data=pedData, snp.data=genotypes) dim(mis) @  Thus, 114 of the subjects and 37 of the SNPs had at least one mis-inheritance. The following commands count mis-inheritances per subject and plot its frequency distribution, and similarly, for mis-inheritances per SNP:  <<per-subj-snp,fig=TRUE>>= per.subj <- apply(mis, 1, sum, na.rm=TRUE) per.snp <- apply(mis, 2, sum, na.rm=TRUE) par(mfrow = c(1, 2)) hist(per.subj,main='Histogram per Subject', xlab='Subject') hist(per.snp,main='Histogram per SNP', xlab='SNP') @   Note that mis-inheritances must be ascribed to offspring, although the error may lie with the parent data. The following commands first extract the pedigree identifiers for mis-inheriting subjects and go on to chart the numbers of mis-inheritances per family: <<per-family,fig=TRUE>>= fam <- pedData[rownames(mis), ""familyid""] per.fam <- tapply(per.subj, fam, sum) par(mfrow = c(1, 1)) hist(per.fam, main='Histogram per Family', xlab='Family') @  None of the above analyses suggest serious problems with the data, although there are clearly a few genotyping errors.  \section*{TDT tests}  At present, the package only allows testing of discrete disease phenotypes in case--parent trios --- basically the Transmission/Disequilibrium Test (TDT). This is carried out by the function {\tt tdt.snp}, which returns the same class of object as that returned by {\tt single.snp.tests}; allelic (1 df) and genotypic (2~df) tests are computed. The following commands compute the tests, display the $p$-values, and plot quantile--quantile plots of the 1~df tests chi-squared statistics: <<tdt-tests,fig=TRUE,keep.source=TRUE>>= tests <- tdt.snp(data = pedData, snp.data = genotypes) cbind(p.values.1df = p.value(tests, 1),       p.values.2df = p.value(tests, 2)) qq.chisq(chi.squared(tests, 1), df = 1) @  Since these SNPs were all in a region of known association, the overdispersion of test statistics is not surprising. Note that, because each family had two affected offspring, there were twice as many parent-offspring trios as families. In the above tests, the contribution of the two trios in each family to the test statistic have been assumed to be independent. When there is {\em linkage} between the genetic locus and disease trait, this assumption is incorrect and an alternative variance estimate can be used by specifying {\tt robust=TRUE} in the call. However, in practice, linkage is very rarely strong enough to require this correction. \end{document}",5170
